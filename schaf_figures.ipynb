{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Paired Benchmark Figures\n",
        "\n",
        "# This section contains figures comparing different benchmarks for paired models across datasets:\n",
        "# 1. Cell Type Accuracy Comparison\n",
        "# 2. Cell Type Accuracy Ablation Study\n",
        "# 3. Gene Correlation Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load paired benchmark data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load ground truth and predictions\n",
        "ground_truth_out_of_sample = sc.read_h5ad(f'{DATA_ROOT}/data/xenium_cancer/xenium_cancer_inferences/whole_sample.h5ad')\n",
        "pred_out_of_sample = sc.read_h5ad(f'{DATA_ROOT}/data/xenium_cancer/xenium_cancer_inferences/whole_sample.h5ad')\n",
        "\n",
        "ground_truth_in_sample = {\n",
        "    z: sc.read_h5ad(f'{DATA_ROOT}/data/xenium_cancer/cancer_in_sample_folds/fold_{z}_st.h5ad')\n",
        "    for z in range(4)\n",
        "}\n",
        "\n",
        "pred_in_sample = {\n",
        "    z: sc.read_h5ad(f'{DATA_ROOT}/data/xenium_cancer/xenium_cancer_inferences/fold_{z}.h5ad')\n",
        "    for z in range(4)\n",
        "}\n",
        "\n",
        "# Process data\n",
        "for z in ground_truth_in_sample:\n",
        "    sc.pp.log1p(ground_truth_in_sample[z])\n",
        "    ground_truth_in_sample[z].X = np.array(ground_truth_in_sample[z].X.todense())\n",
        "\n",
        "# Get common genes\n",
        "common_in_sample = np.intersect1d(pred_in_sample[0].var.index, ground_truth_in_sample[0].var.index)\n",
        "pred_in_sample = {k: v[::,common_in_sample] for k, v in pred_in_sample.items()}\n",
        "ground_truth_in_sample = {k: v[::,common_in_sample] for k, v in ground_truth_in_sample.items()}\n",
        "\n",
        "common_out_of_sample = np.intersect1d(ground_truth_out_of_sample.var.index, pred_out_of_sample.var.index)\n",
        "ground_truth_out_of_sample = ground_truth_out_of_sample[::,common_out_of_sample]\n",
        "pred_out_of_sample = pred_out_of_sample[::,common_out_of_sample]\n",
        "\n",
        "# Load benchmark predictions\n",
        "benchmarks = [\n",
        "    'schaf_no_stage1', \n",
        "    'schaf_no_stage2',\n",
        "    'spirit',\n",
        "    'st_net',\n",
        "    'deep_pt',\n",
        "    'he2rna',\n",
        "]\n",
        "\n",
        "# Load in-sample benchmark predictions\n",
        "infer_models_dir = '/storage2/ccomiter/schaf_benchmarks_infer_cancer_in_sample/'\n",
        "benchmark_to_pred_in_sample = {}\n",
        "for benchmark in benchmarks:\n",
        "    res = {}\n",
        "    for fold in range(4):\n",
        "        res[fold] = sc.read_h5ad(f'{infer_models_dir}/benchmark_{benchmark}_fold_{fold}.h5ad')\n",
        "        res[fold] = res[fold][::,np.intersect1d(common_in_sample, res[fold].var.index)]\n",
        "    benchmark_to_pred_in_sample[benchmark] = res\n",
        "\n",
        "# Load out-of-sample benchmark predictions\n",
        "infer_models_dir = '/storage2/ccomiter/schaf_benchmarks_infer_cancer_whole_sample/'\n",
        "benchmark_to_pred_out_of_sample = {}\n",
        "for benchmark in benchmarks:\n",
        "    benchmark_to_pred_out_of_sample[benchmark] = sc.read_h5ad(f'{infer_models_dir}/benchmark_{benchmark}_whole_sample.h5ad')\n",
        "    benchmark_to_pred_out_of_sample[benchmark] = benchmark_to_pred_out_of_sample[benchmark][::,np.intersect1d(common_out_of_sample, benchmark_to_pred_out_of_sample[benchmark].var.index)]\n",
        "\n",
        "# Load mouse data\n",
        "pred_mouse = {\n",
        "    z: sc.read_h5ad(f'{DATA_ROOT}/data/xenium_cancer/mouse_inferences/fold_{z}.h5ad')\n",
        "    for z in range(4)\n",
        "}\n",
        "\n",
        "ground_truth_mouse = {\n",
        "    z: sc.read_h5ad(f'{DATA_ROOT}/data/xenium_cancer/mouse_folds/fold_{z}_st.h5ad')\n",
        "    for z in range(4)\n",
        "}\n",
        "\n",
        "for z in ground_truth_mouse:\n",
        "    sc.pp.log1p(ground_truth_mouse[z])\n",
        "    ground_truth_mouse[z].X = np.array(ground_truth_mouse[z].X.todense())\n",
        "\n",
        "common_mouse = np.intersect1d(pred_mouse[0].var.index, ground_truth_mouse[0].var.index)\n",
        "pred_mouse = {k: v[::,common_mouse] for k, v in pred_mouse.items()}\n",
        "ground_truth_mouse = {k: v[::,common_mouse] for k, v in ground_truth_mouse.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlations and metrics\n",
        "datasets = ['in_sample', 'out_of_sample', 'mouse']\n",
        "\n",
        "# Add SCHAF and ground truth to benchmark dictionaries\n",
        "benchmark_to_pred_mouse['schaf'] = pred_mouse\n",
        "benchmark_to_pred_mouse['gt'] = ground_truth_mouse\n",
        "benchmark_to_pred_out_of_sample['schaf'] = pred_out_of_sample\n",
        "benchmark_to_pred_out_of_sample['gt'] = ground_truth_out_of_sample\n",
        "benchmark_to_pred_in_sample['schaf'] = pred_in_sample\n",
        "benchmark_to_pred_in_sample['gt'] = ground_truth_in_sample\n",
        "\n",
        "# Organize predictions by dataset\n",
        "dataset_to_benchmark_to_pred = {\n",
        "    'in_sample': benchmark_to_pred_in_sample,\n",
        "    'out_of_sample': benchmark_to_pred_out_of_sample,\n",
        "    'mouse': benchmark_to_pred_mouse\n",
        "}\n",
        "\n",
        "# Calculate correlations and scores\n",
        "dataset_to_benchmark_to_corrs = {}\n",
        "dataset_to_benchmark_to_scores = {}\n",
        "\n",
        "# Out of sample correlations\n",
        "benchmark_to_out_of_sample_corrs = {}\n",
        "benchmark_to_out_of_sample_scores = {}\n",
        "for benchmark in benchmarks + ['schaf']:\n",
        "    corrs_res = {}\n",
        "    scores_res = {}\n",
        "    gt = dataset_to_benchmark_to_pred['out_of_sample']['gt']\n",
        "    pred = dataset_to_benchmark_to_pred['out_of_sample'][benchmark]\n",
        "    for g in pred.var.index:\n",
        "        t = np.array(gt[::,g].X.squeeze())\n",
        "        p = np.array(pred[::,g].X.squeeze())\n",
        "        corrs_res[g] = np.corrcoef(t, p)[0, 1]\n",
        "        scores_res[g] = p.std()\n",
        "    benchmark_to_out_of_sample_corrs[benchmark] = corrs_res\n",
        "    benchmark_to_out_of_sample_scores[benchmark] = scores_res\n",
        "\n",
        "# In sample correlations\n",
        "in_sample_fold_to_prop = {}\n",
        "total_cells = float(sum(ground_truth_in_sample[z].shape[0] for z in range(4)))\n",
        "for z in range(4):\n",
        "    in_sample_fold_to_prop[z] = ground_truth_in_sample[z].shape[0] / total_cells\n",
        "\n",
        "benchmark_to_in_sample_corrs = {}\n",
        "benchmark_to_in_sample_scores = {}\n",
        "for benchmark in benchmarks + ['schaf']:\n",
        "    all_folds_corrs_res = {}\n",
        "    all_folds_scores_res = {}\n",
        "    for fold in range(4):\n",
        "        corrs_res = {}\n",
        "        scores_res = {}\n",
        "        gt = dataset_to_benchmark_to_pred['in_sample']['gt'][fold]\n",
        "        pred = dataset_to_benchmark_to_pred['in_sample'][benchmark][fold]\n",
        "        for g in pred.var.index:\n",
        "            t = np.array(gt[::,g].X.squeeze())\n",
        "            p = np.array(pred[::,g].X.squeeze())\n",
        "            corrs_res[g] = np.corrcoef(t, p)[0, 1]\n",
        "            scores_res[g] = p.std()\n",
        "        all_folds_corrs_res[fold] = corrs_res\n",
        "        all_folds_scores_res[fold] = scores_res\n",
        "    \n",
        "    corrs_res = {}\n",
        "    scores_res = {}\n",
        "    for g in pred.var.index:\n",
        "        corrs_res[g] = sum(all_folds_corrs_res[z][g] * in_sample_fold_to_prop[z] for z in range(4))\n",
        "        scores_res[g] = sum(all_folds_scores_res[z][g] * in_sample_fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    benchmark_to_in_sample_corrs[benchmark] = corrs_res\n",
        "    benchmark_to_in_sample_scores[benchmark] = scores_res\n",
        "\n",
        "# Mouse correlations\n",
        "mouse_fold_to_prop = {}\n",
        "total_cells = float(sum(ground_truth_mouse[z].shape[0] for z in range(4)))\n",
        "for z in range(4):\n",
        "    mouse_fold_to_prop[z] = ground_truth_mouse[z].shape[0] / total_cells\n",
        "\n",
        "benchmark_to_mouse_corrs = {}\n",
        "benchmark_to_mouse_scores = {}\n",
        "for benchmark in benchmarks + ['schaf']:\n",
        "    all_folds_corrs_res = {}\n",
        "    all_folds_scores_res = {}\n",
        "    for fold in range(4):\n",
        "        corrs_res = {}\n",
        "        scores_res = {}\n",
        "        gt = dataset_to_benchmark_to_pred['mouse']['gt'][fold]\n",
        "        pred = dataset_to_benchmark_to_pred['mouse'][benchmark][fold]\n",
        "        for g in pred.var.index:\n",
        "            t = np.array(gt[::,g].X.squeeze())\n",
        "            p = np.array(pred[::,g].X.squeeze())\n",
        "            corrs_res[g] = np.corrcoef(t, p)[0, 1]\n",
        "            scores_res[g] = p.std()\n",
        "        all_folds_corrs_res[fold] = corrs_res\n",
        "        all_folds_scores_res[fold] = scores_res\n",
        "    \n",
        "    corrs_res = {}\n",
        "    scores_res = {}\n",
        "    for g in pred.var.index:\n",
        "        corrs_res[g] = sum(all_folds_corrs_res[z][g] * mouse_fold_to_prop[z] for z in range(4))\n",
        "        scores_res[g] = sum(all_folds_scores_res[z][g] * mouse_fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    benchmark_to_mouse_corrs[benchmark] = corrs_res\n",
        "    benchmark_to_mouse_scores[benchmark] = scores_res\n",
        "\n",
        "# Organize results by dataset\n",
        "dataset_to_benchmark_to_corrs = {\n",
        "    'in_sample': benchmark_to_in_sample_corrs,\n",
        "    'out_of_sample': benchmark_to_out_of_sample_corrs,\n",
        "    'mouse': benchmark_to_mouse_corrs\n",
        "}\n",
        "\n",
        "dataset_to_benchmark_to_scores = {\n",
        "    'in_sample': benchmark_to_in_sample_scores,\n",
        "    'out_of_sample': benchmark_to_out_of_sample_scores,\n",
        "    'mouse': benchmark_to_mouse_scores\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting functions and constants\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Dataset and benchmark name mappings\n",
        "ds_to_name = {\n",
        "    'mouse': 'In-Sample Xenium Mouse',\n",
        "    'in_sample': 'In-Sample Xenium MBC',\n",
        "    'out_of_sample': 'New-Sample Xenium MBC',\n",
        "}\n",
        "\n",
        "bm_to_name = {\n",
        "    'schaf': 'SCHAF',\n",
        "    'spirit': 'SPiRiT',\n",
        "    'st_net': 'ST-Net',\n",
        "    'deep_pt': 'DeepPT',\n",
        "    'he2rna': 'HE2RNA',\n",
        "    'schaf_no_stage1': 'No Stage 2',\n",
        "    'schaf_no_stage2': 'No Stage 1',\n",
        "}\n",
        "\n",
        "def plot_benchmarks_cell_type_accuracy(y, value, errors=None):\n",
        "    \"\"\"Plot cell type accuracy comparison across benchmarks and datasets.\"\"\"\n",
        "    new_y = {}\n",
        "    for k, v in y.items():\n",
        "        for kk, vv in v.items():\n",
        "            kk = kk.replace('no', 'only')\n",
        "            k = k.replace('no', 'only')\n",
        "            if kk not in new_y:\n",
        "                new_y[kk] = {}\n",
        "            new_y[kk][k] = vv\n",
        "    y = new_y\n",
        "\n",
        "    if errors:\n",
        "        new_y = {}\n",
        "        for k, v in errors.items():\n",
        "            for kk, vv in v.items():\n",
        "                kk = kk.replace('no', 'only')\n",
        "                k = k.replace('no', 'only')\n",
        "                if kk not in new_y:\n",
        "                    new_y[kk] = {}\n",
        "                new_y[kk][k] = vv\n",
        "        errors = new_y\n",
        "\n",
        "    # Extract dataset names and benchmarks\n",
        "    datasets = list(y.keys())\n",
        "    datasets = [d for d in datasets if 'only' not in d]\n",
        "    benchmarks = ['mouse', 'in_sample', 'out_of_sample']\n",
        "\n",
        "    # Prepare values for each dataset\n",
        "    values = np.array([[y[dataset].get(benchmark, 0) for benchmark in benchmarks] for dataset in datasets])\n",
        "    error_values = np.array([[errors[dataset].get(benchmark, 0) for benchmark in benchmarks] for dataset in datasets]) if errors else np.zeros_like(values)\n",
        "\n",
        "    # Create plot\n",
        "    _, ax = plt.subplots(1, 1, figsize=(11, 11))\n",
        "    bar_width = 0.1\n",
        "    x = np.arange(len(benchmarks))\n",
        "\n",
        "    # Plot bars\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        plt.bar(x + i * bar_width, values[i], width=bar_width, label=dataset, yerr=error_values[i], capsize=5, ecolor='black')\n",
        "\n",
        "    # Customize plot\n",
        "    plt.xlabel('Dataset')\n",
        "    plt.ylabel(value)\n",
        "    plt.xticks(x + bar_width * (len(datasets) - 1) / 2, [ds_to_name[q.replace('no', 'only')] for q in benchmarks])\n",
        "    plt.tight_layout()\n",
        "    plt.setp(ax.spines.values(), linewidth=2)\n",
        "    ax.spines[['right', 'top']].set_visible(False)\n",
        "    \n",
        "    # Save plot\n",
        "    plt.savefig(f'{OUTPUT_DIR}/supp2_bottom.pdf', transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_benchmarks_ablation(y, value):\n",
        "    \"\"\"Plot cell type accuracy ablation study.\"\"\"\n",
        "    new_y = {}\n",
        "    for k, v in y.items():\n",
        "        k = ds_to_name[k]\n",
        "        for kk, vv in v.items():\n",
        "            if 'schaf' not in kk:\n",
        "                continue\n",
        "            kk = bm_to_name[kk]\n",
        "            kk = kk.replace('no', 'only')\n",
        "            k = k.replace('no', 'only')\n",
        "            if kk not in new_y:\n",
        "                new_y[kk] = {}\n",
        "            new_y[kk][k] = vv\n",
        "    y = new_y\n",
        "\n",
        "    # Create plot\n",
        "    _, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "    datasets = list(y.keys())\n",
        "    benchmarks = ['In-Sample Mouse', 'In-Sample Xenium MBC', 'New-Sample Xenium MBC']\n",
        "    values = np.array([[y[dataset].get(benchmark, 0) for benchmark in benchmarks] for dataset in datasets])\n",
        "\n",
        "    # Plot bars\n",
        "    bar_width = 0.1\n",
        "    x = np.arange(len(benchmarks))\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        plt.bar(x + i * bar_width, values[i], width=bar_width, label=dataset)\n",
        "\n",
        "    # Customize plot\n",
        "    plt.xlabel('Dataset')\n",
        "    plt.ylabel(value)\n",
        "    plt.xticks(x + bar_width * (len(datasets) - 1) / 2, [q.replace('no', 'only') for q in benchmarks])\n",
        "    plt.ylim(.4, 1.)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.setp(ax.spines.values(), linewidth=2)\n",
        "    ax.spines[['right', 'top']].set_visible(False)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plt.savefig(f'{OUTPUT_DIR}/celltype_ablation_paired.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_gene_correlation_distributions(y, ablation_only=False):\n",
        "    \"\"\"Plot gene correlation distributions across datasets and benchmarks.\n",
        "    \n",
        "    Args:\n",
        "        y: Dictionary of correlation values\n",
        "        ablation_only: If True, only plot SCHAF and its ablations\n",
        "    \"\"\"\n",
        "    new_y = {}\n",
        "    for k, v in y.items():\n",
        "        for kk, vv in v.items():\n",
        "            if ablation_only and 'schaf' not in kk:\n",
        "                continue\n",
        "            elif not ablation_only and 'stage' in kk:\n",
        "                continue\n",
        "            if k not in new_y:\n",
        "                new_y[k] = {}\n",
        "            new_y[k][kk] = vv\n",
        "    y = new_y\n",
        "\n",
        "    # Create plot\n",
        "    fig, axs = plt.subplots(len(y), 1, figsize=(12, 18))\n",
        "    y_name_tings = ['mouse', 'in_sample', 'out_of_sample']\n",
        "    \n",
        "    for i, dataset_name in enumerate(y_name_tings):\n",
        "        benchmarks = y[dataset_name]\n",
        "        axs[i].set_title(ds_to_name[dataset_name], fontsize=18)\n",
        "\n",
        "        # Plot histograms\n",
        "        benchmark_names = list(benchmarks.keys())\n",
        "        n, bins, patches = axs[i].hist(\n",
        "            [sorted(list(benchmarks[bm_name].values())) for bm_name in benchmark_names], \n",
        "            bins=np.arange(-.2 if 'out' in dataset_name else 0, 1.01, .1),\n",
        "            label=[f'{bm_to_name[bm_name]} (Mean: {np.mean(list(benchmarks[bm_name].values())):.4f})' for bm_name in benchmark_names]\n",
        "        )\n",
        "\n",
        "        # Customize subplot\n",
        "        axs[i].set_xlabel('Spatial Correlation', size='x-large')\n",
        "        axs[i].set_ylabel('Number of Genes', size='x-large')\n",
        "        axs[i].legend(loc='upper right')\n",
        "        plt.setp(axs[i].spines.values(), linewidth=2)\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.4)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plt.savefig(f'{OUTPUT_DIR}/dists_benchmarks_paired.png', dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate cell type accuracy metrics\n",
        "def small_adj2(new_cts):\n",
        "    \"\"\"Adjust cell type labels for out-of-sample data.\"\"\"\n",
        "    return np.array([\n",
        "        7 if int(x) == 8 else 1 if int(x) == 5 else x for x in new_cts\n",
        "    ])\n",
        "\n",
        "def small_adj(new_cts):\n",
        "    \"\"\"Adjust cell type labels for in-sample data.\"\"\"\n",
        "    return np.array([\n",
        "        9 if int(x) == 10 else x for x in new_cts\n",
        "    ])\n",
        "\n",
        "# Load cell type predictions\n",
        "load_the_cts = {}\n",
        "for dataset in datasets:\n",
        "    res = {}\n",
        "    for benchmark in benchmarks + ['schaf']:\n",
        "        if dataset == 'out_of_sample':\n",
        "            name = f'benchmark_celltypes_saved/{dataset}_{benchmark}.npy'\n",
        "            labels = np.load(name)\n",
        "            res[benchmark] = labels\n",
        "        else:\n",
        "            res2 = {}\n",
        "            for fold in range(4):\n",
        "                name = f'benchmark_celltypes_saved/{dataset}_{benchmark}_{fold}.npy'\n",
        "                labels = np.load(name)\n",
        "                res2[fold] = labels\n",
        "            res[benchmark] = res2\n",
        "    load_the_cts[dataset] = res\n",
        "\n",
        "# Calculate out-of-sample accuracy\n",
        "good_inds = []\n",
        "for i, l in enumerate(dataset_to_benchmark_to_pred['out_of_sample']['spirit'].obs.index):\n",
        "    if l in pred_out_of_sample_for_cts.obs.index:\n",
        "        good_inds.append(i)\n",
        "good_inds = np.array(good_inds)\n",
        "\n",
        "benchmark_to_out_of_sample_ct_acc = {}\n",
        "for benchmark in benchmarks + ['schaf']:\n",
        "    gt = small_adj2(np.array(ground_truth_out_of_sample_for_cts.obs['broad_clusters']))\n",
        "    pred = small_adj2(load_the_cts['out_of_sample'][benchmark][good_inds])\n",
        "    benchmark_to_out_of_sample_ct_acc[benchmark] = (pred==gt).sum() / pred.shape[0]\n",
        "\n",
        "# Calculate in-sample accuracy\n",
        "benchmark_to_in_sample_ct_acc = {}\n",
        "benchmark_to_in_sample_ct_err = {}\n",
        "for benchmark in benchmarks + ['schaf']:\n",
        "    ct_acc = {}\n",
        "    for fold in range(4):\n",
        "        gt = small_adj(np.array(dataset_to_benchmark_to_pred['in_sample']['gt'][fold].obs['broad_clusters']))\n",
        "        pred = small_adj(load_the_cts['in_sample'][benchmark][fold])\n",
        "        ct_acc[fold] = (pred==gt).sum() / pred.shape[0]\n",
        "    ct_acc_res = sum(ct_acc[z] * in_sample_fold_to_prop[z] for z in range(4))\n",
        "    ct_acc_err = (sum((((ct_acc[z] - ct_acc_res)**2 for z in in_sample_fold_to_prop))) / (3.) / (4.) )**.5\n",
        "    benchmark_to_in_sample_ct_acc[benchmark] = ct_acc_res\n",
        "    benchmark_to_in_sample_ct_err[benchmark] = ct_acc_err\n",
        "\n",
        "# Calculate mouse accuracy\n",
        "benchmark_to_mouse_ct_acc = {}\n",
        "benchmark_to_mouse_ct_err = {}\n",
        "for benchmark in benchmarks + ['schaf']:\n",
        "    ct_acc = {}\n",
        "    for fold in range(4):\n",
        "        gt = np.array(dataset_to_benchmark_to_pred['mouse']['gt'][fold].obs['broad_clusters'])\n",
        "        pred = load_the_cts['mouse'][benchmark][fold]\n",
        "        ct_acc[fold] = (pred==gt).sum() / pred.shape[0]\n",
        "    ct_acc_res = sum(ct_acc[z] * mouse_fold_to_prop[z] for z in range(4))\n",
        "    ct_acc_err = (sum((((ct_acc[z] - ct_acc_res)**2 for z in mouse_fold_to_prop))) / (3.) / (4.) )**.5\n",
        "    benchmark_to_mouse_ct_acc[benchmark] = ct_acc_res\n",
        "    benchmark_to_mouse_ct_err[benchmark] = ct_acc_err\n",
        "\n",
        "# Organize results by dataset\n",
        "dataset_to_benchmark_to_ct_acc = {\n",
        "    'in_sample': benchmark_to_in_sample_ct_acc,\n",
        "    'out_of_sample': benchmark_to_out_of_sample_ct_acc,\n",
        "    'mouse': benchmark_to_mouse_ct_acc\n",
        "}\n",
        "\n",
        "dataset_to_benchmark_to_ct_err = {\n",
        "    'in_sample': benchmark_to_in_sample_ct_err,\n",
        "    'out_of_sample': defaultdict(int),\n",
        "    'mouse': benchmark_to_mouse_ct_err\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate paired benchmark figures\n",
        "\n",
        "# 1. Cell Type Accuracy Comparison\n",
        "plot_benchmarks_cell_type_accuracy(dataset_to_benchmark_to_ct_acc, 'Cell Type Accuracy', dataset_to_benchmark_to_ct_err)\n",
        "\n",
        "# 2. Cell Type Accuracy Ablation Study\n",
        "plot_benchmarks_ablation(dataset_to_benchmark_to_ct_acc, 'Cell Type Accuracy')\n",
        "\n",
        "# 3. Gene Correlation Distribution Analysis\n",
        "# Full comparison across all benchmarks\n",
        "plot_gene_correlation_distributions(dataset_to_benchmark_to_corrs, ablation_only=False)\n",
        "\n",
        "# 4. Gene Correlation Distribution Analysis - Ablation Study\n",
        "# Only SCHAF and its ablations\n",
        "plot_gene_correlation_distributions(dataset_to_benchmark_to_corrs, ablation_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # SCHAF Figures Generation\n",
        "\n",
        "# This notebook generates all figures for the SCHAF paper. It includes:\n",
        "# - Data loading and preprocessing\n",
        "# - Visium data processing\n",
        "# - HTAPP/MSKCC/Xenium visualization\n",
        "# - Program analysis and correlation computation\n",
        "# - Expression transformation and cell type prediction\n",
        "\n",
        "# The code is organized into the following sections:\n",
        "# 1. Configuration and imports\n",
        "# 2. Data loading utilities\n",
        "# 3. Analysis functions\n",
        "# 4. Visualization functions\n",
        "# 5. Figure generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import datetime\n",
        "import json\n",
        "import functools\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Set, Optional, Union\n",
        "\n",
        "# Data processing\n",
        "import scanpy as sc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from scipy.stats import gaussian_kde, zscore\n",
        "import sklearn.metrics\n",
        "from numba import njit, prange\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "# Image processing\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import imageio.v3 as iio\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure system settings\n",
        "PIL.Image.MAX_IMAGE_PIXELS = 4017126500\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Path constants\n",
        "DATA_ROOT = '/storage/ccomiter/schaf_for_revision052424'\n",
        "BASE_DIR = os.path.dirname(os.getcwd())\n",
        "OUTPUT_DIR = os.path.join(DATA_ROOT, 'final_figures_schaf_revision_pngs')\n",
        "XENIUM_DIR = os.path.join(DATA_ROOT, 'data/xenium_cancer')\n",
        "MOUSE_DIR = os.path.join(DATA_ROOT, 'data/mouse_pup')\n",
        "\n",
        "# Sample constants\n",
        "HTAPP_KEYS = ['4531', '7179', '7479', '7629', '932', '6760', '7149', '4381', '8239']\n",
        "PLACENTA_SAMPLES = ['7', '8', '9', '11']\n",
        "LUNG_CANCER_SAMPLES = ['139193', '138681', '146259', '117956']\n",
        "\n",
        "# Gene mapping dictionaries\n",
        "HTAPP_GENES = {\n",
        "    '7149': 'sumf2',\n",
        "    '932': 'sat1',\n",
        "    '6760': 'commd1',\n",
        "    '4381': 'ppp1r15a'\n",
        "}\n",
        "\n",
        "PLACENTA_GENES = {\n",
        "    '7': 'cd74',\n",
        "    '8': 'hla-dra',\n",
        "    '9': 'cd68',\n",
        "    '11': 'cd14'\n",
        "}\n",
        "\n",
        "LUNG_CANCER_GENES = {\n",
        "    '139193': 'cd3d',\n",
        "    '138681': 'cd8a',\n",
        "    '146259': 'cd4',\n",
        "    '117956': 'cd19'\n",
        "}\n",
        "\n",
        "# Processing constants\n",
        "MAX_VERT_DIST = 10000\n",
        "VISIUM_TRANSFORM = np.array([\n",
        "    [0.130157158, 2.594980119, -12243.84897],\n",
        "    [-2.594980119, 0.130157158, 40352.06194],\n",
        "    [0, 0, 1],\n",
        "])\n",
        "\n",
        "# Image region constants\n",
        "XENIUM_LIMITS = {'x': 17700, 'y': 12900}\n",
        "MOUSE_LIMITS = {'x': 36500, 'y': 19500}\n",
        "\n",
        "# Sample code mappings\n",
        "PLACENTA_CODE_MAP = {\n",
        "    '7': 'JS34',\n",
        "    '8': 'JS40',\n",
        "    '9': 'JS35',\n",
        "    '11': 'JS36'\n",
        "}\n",
        "\n",
        "# Figure parameters\n",
        "FIGURE_PARAMS = {\n",
        "    'dpi': 400,\n",
        "    'transparent': True,\n",
        "    'font_size': {\n",
        "        'xx-large': 16,\n",
        "        'x-large': 14,\n",
        "        'large': 12\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation and visualization utility functions\n",
        "def get_cell_corrs(pred, real):\n",
        "    \"\"\"Compute cell-wise correlations between predicted and real expression.\"\"\"\n",
        "    return np.array([\n",
        "        np.corrcoef(p, r)[0,1] if not np.isnan(np.corrcoef(p, r)[0,1]) else 0 \n",
        "        for p, r in zip(pred, real)\n",
        "    ])\n",
        "\n",
        "def get_gene_corrs(pred, real):\n",
        "    \"\"\"Compute gene-wise correlations between predicted and real expression.\"\"\"\n",
        "    return np.array([\n",
        "        np.corrcoef(p, r)[0,1] if not np.isnan(np.corrcoef(p, r)[0,1]) else 0 \n",
        "        for p, r in zip(pred.T, real.T)\n",
        "    ])\n",
        "\n",
        "def gene_corr_graph(pred, true, path):\n",
        "    \"\"\"Create and save gene correlation plot.\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.hist(get_gene_corrs(pred, true), bins=50)\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def cell_corr_graph(pred, true, path):\n",
        "    \"\"\"Create and save cell correlation plot.\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.hist(get_cell_corrs(pred, true), bins=50)\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def make_cell_corr_vis(pred, real, nonzero_coords, nonzero_areas, orig_hist, path):\n",
        "    \"\"\"Create and save cell correlation visualization.\"\"\"\n",
        "    cell_corrs = get_cell_corrs(pred, real)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(nonzero_coords[:, 0], nonzero_coords[:, 1], \n",
        "               c=cell_corrs, cmap='coolwarm', alpha=0.5, s=nonzero_areas)\n",
        "    plt.imshow(orig_hist)\n",
        "    plt.colorbar()\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def better_gene_corr_graph(pred, true, path):\n",
        "    \"\"\"Create and save enhanced gene correlation plot.\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.histplot(data=get_gene_corrs(pred, true), bins=50, kde=True)\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def better_cell_corr_graph(pred, true, path):\n",
        "    \"\"\"Create and save enhanced cell correlation plot.\"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.histplot(data=get_cell_corrs(pred, true), bins=50, kde=True)\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model utility functions\n",
        "def get_loss(the_net, criterion, dataloader, device, transforms=None, attention_fn=None, is_ae=False):\n",
        "    \"\"\"Compute loss for a model on a dataset.\"\"\"\n",
        "    the_net.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if transforms is not None:\n",
        "                batch = transforms(batch)\n",
        "            if attention_fn is not None:\n",
        "                batch = attention_fn(batch)\n",
        "            if is_ae:\n",
        "                x = batch.to(device)\n",
        "                out = the_net(x)\n",
        "                loss = criterion(out, x)\n",
        "            else:\n",
        "                x, y = batch\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                out = the_net(x)\n",
        "                loss = criterion(out, y)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "    return total_loss / num_batches\n",
        "\n",
        "def get_res(the_net, dataloader, device, transforms=None, with_spatial=False, attention_fn=None):\n",
        "    \"\"\"Get model predictions for a dataset.\"\"\"\n",
        "    the_net.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_xs = []\n",
        "    all_ys = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if transforms is not None:\n",
        "                batch = transforms(batch)\n",
        "            if attention_fn is not None:\n",
        "                batch = attention_fn(batch)\n",
        "            if with_spatial:\n",
        "                x, y, xs, ys = batch\n",
        "                all_xs.extend(xs.numpy())\n",
        "                all_ys.extend(ys.numpy())\n",
        "            else:\n",
        "                x, y = batch\n",
        "            x = x.to(device)\n",
        "            pred = the_net(x)\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(y.numpy())\n",
        "            \n",
        "    if with_spatial:\n",
        "        return np.array(all_preds), np.array(all_labels), np.array(all_xs), np.array(all_ys)\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "def get_spatial_corr_mean(the_net, indiv_dataloaders, device, spatial_gene_inds, transforms=None):\n",
        "    \"\"\"Compute mean spatial correlation for specific genes.\"\"\"\n",
        "    all_corrs = []\n",
        "    for dl in indiv_dataloaders:\n",
        "        preds, labels = get_res(the_net, dl, device, transforms)\n",
        "        preds = preds[:, spatial_gene_inds]\n",
        "        labels = labels[:, spatial_gene_inds]\n",
        "        corrs = get_gene_corrs(preds, labels)\n",
        "        all_corrs.append(np.mean(corrs))\n",
        "    return np.mean(all_corrs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_fig_params():\n",
        "    \"\"\"Set global figure parameters for consistent visualization.\"\"\"\n",
        "    plt.rcParams['figure.figsize'] = [10, 10]\n",
        "    plt.rcParams['figure.dpi'] = 100\n",
        "    plt.rcParams['savefig.dpi'] = 300\n",
        "    plt.rcParams['font.size'] = 12\n",
        "    sns.set_style('whitegrid')\n",
        "    \n",
        "def load_and_process_visium_data(base_dir=BASE_DIR, max_vert_dist=MAX_VERT_DIST):\n",
        "    \"\"\"Load and process Visium spatial transcriptomics data.\"\"\"\n",
        "    visium_path = os.path.join(base_dir, 'htapp_supervise/new_schaf_experiment_scripts/more_data/xenium')\n",
        "    visium_adata = sc.read_10x_h5(os.path.join(visium_path, 'visium_breast_xenium.h5'))\n",
        "    visium_adata.var_names_make_unique()\n",
        "    \n",
        "    # Process tissue positions\n",
        "    tissue_positions = pd.read_csv(\n",
        "        os.path.join(visium_path, 'tissue_positions_visium.csv')\n",
        "    ).set_index('barcode')\n",
        "    tissue_positions = tissue_positions.loc[visium_adata.obs.index]\n",
        "    visium_adata.obs = tissue_positions\n",
        "    \n",
        "    # Filter and process coordinates\n",
        "    vert_dist = (visium_adata.obs['pxl_col_in_fullres'].max() - \n",
        "                visium_adata.obs['pxl_col_in_fullres'])\n",
        "    good_vis_inds = np.where(vert_dist <= max_vert_dist)[0]\n",
        "    \n",
        "    visium_xs = visium_adata.obs['pxl_row_in_fullres'][good_vis_inds].values\n",
        "    visium_xs = visium_xs - visium_xs.min()\n",
        "    visium_ys = vert_dist[good_vis_inds].values\n",
        "    \n",
        "    visium_adata = visium_adata[good_vis_inds]\n",
        "    visium_adata.obs['x'] = visium_xs\n",
        "    visium_adata.obs['y'] = visium_ys\n",
        "    \n",
        "    sc.pp.log1p(visium_adata)\n",
        "    visium_adata.X = visium_adata.X.todense()\n",
        "    visium_adata.var.index = [gene.lower() for gene in visium_adata.var.index]\n",
        "    \n",
        "    return visium_adata\n",
        "\n",
        "def load_base_images():\n",
        "    \"\"\"Load base Xenium and mouse histology images.\"\"\"\n",
        "    # Load histology images\n",
        "    in_sample_hist = iio.imread(os.path.join(XENIUM_DIR, 'xenium_hist.png'))\n",
        "    out_of_sample = iio.imread(os.path.join(XENIUM_DIR, 'HE_other_sample_xenium.tif'))\n",
        "    mouse_hist = iio.imread(os.path.join(MOUSE_DIR, 'Xenium_V1_mouse_pup_he_image.ome.tif'))\n",
        "    \n",
        "    # Create fold dictionaries\n",
        "    in_sample_fold_to_hist = {\n",
        "        0: in_sample_hist[:XENIUM_LIMITS['y'], :XENIUM_LIMITS['x']],\n",
        "        1: in_sample_hist[XENIUM_LIMITS['y']:25761, XENIUM_LIMITS['x']:35402],\n",
        "        2: in_sample_hist[XENIUM_LIMITS['y']:25761, :XENIUM_LIMITS['x']],\n",
        "        3: in_sample_hist[:XENIUM_LIMITS['y'], XENIUM_LIMITS['x']:35402]\n",
        "    }\n",
        "\n",
        "    mouse_fold_to_hist = {\n",
        "        0: mouse_hist[:MOUSE_LIMITS['y'], :MOUSE_LIMITS['x']],\n",
        "        1: mouse_hist[MOUSE_LIMITS['y']:41081, MOUSE_LIMITS['x']:81654],\n",
        "        2: mouse_hist[MOUSE_LIMITS['y']:41081, :MOUSE_LIMITS['x']],\n",
        "        3: mouse_hist[:MOUSE_LIMITS['y'], MOUSE_LIMITS['x']:81654]\n",
        "    }\n",
        "    \n",
        "    return in_sample_hist, out_of_sample, mouse_hist, in_sample_fold_to_hist, mouse_fold_to_hist\n",
        "\n",
        "def load_htapp_images(base_dir=BASE_DIR):\n",
        "    \"\"\"Load HTAPP histology images.\"\"\"\n",
        "    htapp_hists = {}\n",
        "    hists_dir = os.path.join(base_dir, 'htapp_supervise/new_schaf_experiment_scripts/more_data/htapp_hists')\n",
        "    \n",
        "    for f in os.listdir(hists_dir):\n",
        "        if not f.endswith('.tif'):\n",
        "            continue\n",
        "        k = f.split('.')[0]\n",
        "        htapp_hists[k] = iio.imread(os.path.join(hists_dir, f))\n",
        "    \n",
        "    return htapp_hists\n",
        "\n",
        "def load_placenta_images(image_dir='newest_bestest_placenta_hes'):\n",
        "    \"\"\"\n",
        "    Load placenta histology images.\n",
        "    \n",
        "    Args:\n",
        "        image_dir (str): Directory containing placenta images\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary mapping sample IDs to histology images\n",
        "    \"\"\"\n",
        "    placenta_hists = {}\n",
        "    for k, code in PLACENTA_CODE_MAP.items():\n",
        "        img_path = os.path.join(image_dir, f'{code}.jpg')\n",
        "        placenta_hists[k] = iio.imread(img_path)\n",
        "        \n",
        "    return placenta_hists\n",
        "\n",
        "def load_sc_data():\n",
        "    \"\"\"Load single-cell data for all HTAPP samples.\"\"\"\n",
        "    sc_dir = f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/final_scs/schtapp'\n",
        "    the_scs = {}\n",
        "    \n",
        "    for file in os.listdir(sc_dir):\n",
        "        for k in HTAPP_KEYS:\n",
        "            if k not in file:\n",
        "                continue\n",
        "            sc_adata = sc.read_h5ad(f'{sc_dir}/{file}')\n",
        "            new_v = sc.AnnData(X=np.array(sc_adata.obsm['counts'].todense()), obs=sc_adata.obs)\n",
        "            sc.pp.log1p(new_v)\n",
        "            new_v.var.index = sc_adata.uns['counts_var']\n",
        "            sc_adata = new_v\n",
        "            sc_adata.var.index = [q.lower() for q in sc_adata.var.index]\n",
        "            the_scs[k] = sc_adata\n",
        "    \n",
        "    return the_scs\n",
        "\n",
        "def load_merfish_data():\n",
        "    \"\"\"Load MERFISH data for all HTAPP samples.\"\"\"\n",
        "    mers_dir = f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/final_mers'\n",
        "    merfish_htapp = {}\n",
        "    \n",
        "    for f in os.listdir(mers_dir):\n",
        "        key = f.split('_')[0]\n",
        "        mer = sc.read_h5ad(os.path.join(mers_dir, f'{key}_merfish.h5ad'))\n",
        "        mer.X = np.array(mer.obsm['counts'].todense())\n",
        "        sc.pp.log1p(mer)\n",
        "        mer.var.index = [q.lower() for q in mer.var.index]\n",
        "        merfish_htapp[key] = mer\n",
        "    \n",
        "    return merfish_htapp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_program_correlations_and_scores(ground_truth_data, prediction_data, gene_sets, all_cancer_programs, fold_to_prop=None):\n",
        "    \"\"\"\n",
        "    Compute correlations and scores for gene programs between ground truth and predictions.\n",
        "    \n",
        "    Args:\n",
        "        ground_truth_data: Dict of ground truth AnnData objects by fold\n",
        "        prediction_data: Dict of prediction AnnData objects by fold\n",
        "        gene_sets: Dict of gene sets to analyze\n",
        "        all_cancer_programs: Dict of cancer programs and their genes\n",
        "        fold_to_prop: Dict of fold proportions for weighted averaging\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of dicts containing program correlations and scores\n",
        "    \"\"\"\n",
        "    gene_set_to_fold_to_program_corrs = {}\n",
        "    gene_set_to_fold_to_program_scores = {}\n",
        "    \n",
        "    for name, genes in gene_sets.items():\n",
        "        fold_to_program_corrs = {}\n",
        "        fold_to_program_scores = {}\n",
        "        \n",
        "        for z in range(4):\n",
        "            p = prediction_data[z][::,genes]\n",
        "            t = ground_truth_data[z][::, genes]\n",
        "            p_scores, p_corrs = get_prog_info(t, p, all_cancer_programs)\n",
        "            fold_to_program_corrs[z] = p_corrs\n",
        "            fold_to_program_scores[z] = p_scores\n",
        "            \n",
        "        gene_set_to_fold_to_program_corrs[name] = fold_to_program_corrs\n",
        "        gene_set_to_fold_to_program_scores[name] = fold_to_program_scores\n",
        "    \n",
        "    if fold_to_prop is not None:\n",
        "        gene_set_to_avg_program_corrs = {}\n",
        "        gene_set_to_avg_program_scores = {}\n",
        "        \n",
        "        for name, genes in gene_sets.items():\n",
        "            avg_program_corrs = {}\n",
        "            avg_program_scores = {}\n",
        "            \n",
        "            for prog in gene_set_to_fold_to_program_corrs[name][0]:\n",
        "                avg_program_corrs[prog] = sum(gene_set_to_fold_to_program_corrs[name][z][prog] * fold_to_prop[z] for z in range(4))\n",
        "                avg_program_scores[prog] = sum(gene_set_to_fold_to_program_scores[name][z][prog] * fold_to_prop[z] for z in range(4))\n",
        "            \n",
        "            gene_set_to_avg_program_corrs[name] = avg_program_corrs\n",
        "            gene_set_to_avg_program_scores[name] = avg_program_scores\n",
        "            \n",
        "        return gene_set_to_avg_program_corrs, gene_set_to_avg_program_scores\n",
        "    \n",
        "    return gene_set_to_fold_to_program_corrs, gene_set_to_fold_to_program_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_label(info, celltypes):\n",
        "    \"\"\"\n",
        "    Create a normalized label vector from cell type information.\n",
        "    \n",
        "    Args:\n",
        "        info: DataFrame row containing cell type information\n",
        "        celltypes: List of cell types to check\n",
        "        \n",
        "    Returns:\n",
        "        Normalized numpy array of cell type labels\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    for ct in celltypes:\n",
        "        if ct in info and info[ct]:\n",
        "            res.append(1)\n",
        "        else:\n",
        "            res.append(0)\n",
        "    res = np.array(res)\n",
        "    if res.sum() > 0:\n",
        "        res = res / res.sum()\n",
        "    return res\n",
        "\n",
        "def process_spatial_labels(merfish_data, histology_data, celltypes, distance_threshold=None):\n",
        "    \"\"\"\n",
        "    Process spatial labels by matching MERFISH data points to histology annotations.\n",
        "    \n",
        "    Args:\n",
        "        merfish_data: Dict of MERFISH AnnData objects\n",
        "        histology_data: Dict of histology coordinate DataFrames\n",
        "        celltypes: List of cell types to process\n",
        "        distance_threshold: Optional distance threshold for filtering matches\n",
        "        \n",
        "    Returns:\n",
        "        Dict of processed labels and distances\n",
        "    \"\"\"\n",
        "    to_plot_xs = {}\n",
        "    to_plot_ys = {}\n",
        "    to_select = {}\n",
        "    the_labels = {}\n",
        "    the_dists = {}\n",
        "    \n",
        "    for k in merfish_data:\n",
        "        try:\n",
        "            select = np.where(merfish_data[k].obs[celltypes].sum(axis=1)>0)[0]\n",
        "            to_select[k] = select\n",
        "            xs = list(merfish_data[k].obs['x'][select])\n",
        "            ys = list(merfish_data[k].obs['y'][select])\n",
        "            to_plot_xs[k] = xs\n",
        "            to_plot_ys[k] = ys\n",
        "        except:\n",
        "            xs = list(merfish_data[k].obs['x'])\n",
        "            ys = list(merfish_data[k].obs['y'])\n",
        "            \n",
        "        tree = cKDTree(np.array(list(zip(xs, ys))))\n",
        "        labels = []\n",
        "        dists = []\n",
        "        \n",
        "        for x, y in zip(histology_data[k]['x'], histology_data[k]['y']):\n",
        "            p = (x, y)\n",
        "            dd, ind = tree.query(p, k=1)\n",
        "            label = make_label(merfish_data[k].obs.iloc[ind], celltypes)\n",
        "            labels.append(label)\n",
        "            dists.append(dd)\n",
        "            \n",
        "        the_labels[k] = np.array(labels)\n",
        "        the_dists[k] = np.array(dists)\n",
        "        \n",
        "        if distance_threshold is not None:\n",
        "            mask = the_dists[k] < distance_threshold\n",
        "            the_labels[k] = the_labels[k][mask]\n",
        "            the_dists[k] = the_dists[k][mask]\n",
        "            \n",
        "    return {\n",
        "        'labels': the_labels,\n",
        "        'distances': the_dists,\n",
        "        'coordinates': {'x': to_plot_xs, 'y': to_plot_ys},\n",
        "        'selected_indices': to_select\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading functions\n",
        "def load_placenta_images(image_dir='newest_bestest_placenta_hes'):\n",
        "    \"\"\"\n",
        "    Load placenta histology images.\n",
        "    \n",
        "    Args:\n",
        "        image_dir (str): Directory containing placenta images\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary mapping sample IDs to histology images\n",
        "    \"\"\"\n",
        "    PIL.Image.MAX_IMAGE_PIXELS = 4017126500\n",
        "    \n",
        "    k_to_code = {\n",
        "        '7': 'JS34',\n",
        "        '8': 'JS40',\n",
        "        '9': 'JS35',\n",
        "        '11': 'JS36'\n",
        "    }\n",
        "    \n",
        "    placenta_hists = {}\n",
        "    for k in tqdm(k_to_code.keys(), desc=\"Loading placenta images\"):\n",
        "        img_path = os.path.join(image_dir, f'{k_to_code[k]}.jpg')\n",
        "        placenta_hists[k] = iio.imread(img_path)\n",
        "        \n",
        "    return placenta_hists\n",
        "\n",
        "def load_and_process_visium_data(base_dir=BASE_DIR, max_vert_dist=10000):\n",
        "    \"\"\"\n",
        "    Load and process Visium spatial transcriptomics data.\n",
        "    \n",
        "    Args:\n",
        "        base_dir (str): Base directory for data\n",
        "        max_vert_dist (int): Maximum vertical distance for filtering spots\n",
        "        \n",
        "    Returns:\n",
        "        AnnData: Processed Visium data\n",
        "    \"\"\"\n",
        "    visium_path = os.path.join(base_dir, 'htapp_supervise/new_schaf_experiment_scripts/more_data/xenium')\n",
        "    visium_adata = sc.read_10x_h5(os.path.join(visium_path, 'visium_breast_xenium.h5'))\n",
        "    visium_adata.var_names_make_unique()\n",
        "    \n",
        "    tissue_positions = pd.read_csv(\n",
        "        os.path.join(visium_path, 'tissue_positions_visium.csv')\n",
        "    ).set_index('barcode')\n",
        "    tissue_positions = tissue_positions.loc[visium_adata.obs.index]\n",
        "    visium_adata.obs = tissue_positions\n",
        "    \n",
        "    vert_dist = (visium_adata.obs['pxl_col_in_fullres'].max() - \n",
        "                visium_adata.obs['pxl_col_in_fullres'])\n",
        "    good_vis_inds = np.where(vert_dist <= max_vert_dist)[0]\n",
        "    \n",
        "    visium_xs = visium_adata.obs['pxl_row_in_fullres'][good_vis_inds].values\n",
        "    visium_xs = visium_xs - visium_xs.min()\n",
        "    visium_ys = vert_dist[good_vis_inds].values\n",
        "    \n",
        "    visium_adata = visium_adata[good_vis_inds]\n",
        "    visium_adata.obs['x'] = visium_xs\n",
        "    visium_adata.obs['y'] = visium_ys\n",
        "    \n",
        "    sc.pp.log1p(visium_adata)\n",
        "    return visium_adata\n",
        "\n",
        "# Constants and mappings\n",
        "XENIUM_LIMITS = {'x': 17700, 'y': 12900}\n",
        "MOUSE_LIMITS = {'x': 36500, 'y': 19500}\n",
        "\n",
        "PLACENTA_CODE_MAP = {\n",
        "    '7': 'JS34',\n",
        "    '8': 'JS40',\n",
        "    '9': 'JS35',\n",
        "    '11': 'JS36'\n",
        "}\n",
        "\n",
        "# Create fold dictionaries for image regions\n",
        "def create_fold_dictionaries(in_sample_hist, mouse_hist):\n",
        "    \"\"\"\n",
        "    Create dictionaries mapping folds to image regions.\n",
        "    \n",
        "    Args:\n",
        "        in_sample_hist: In-sample histology image\n",
        "        mouse_hist: Mouse histology image\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (in_sample_fold_to_hist, mouse_fold_to_hist) dictionaries\n",
        "    \"\"\"\n",
        "    in_sample_fold_to_hist = {\n",
        "        0: in_sample_hist[:XENIUM_LIMITS['y'], :XENIUM_LIMITS['x']],\n",
        "        1: in_sample_hist[XENIUM_LIMITS['y']:25761, XENIUM_LIMITS['x']:35402],\n",
        "        2: in_sample_hist[XENIUM_LIMITS['y']:25761, :XENIUM_LIMITS['x']],\n",
        "        3: in_sample_hist[:XENIUM_LIMITS['y'], XENIUM_LIMITS['x']:35402]\n",
        "    }\n",
        "\n",
        "    mouse_fold_to_hist = {\n",
        "        0: mouse_hist[:MOUSE_LIMITS['y'], :MOUSE_LIMITS['x']],\n",
        "        1: mouse_hist[MOUSE_LIMITS['y']:41081, MOUSE_LIMITS['x']:81654],\n",
        "        2: mouse_hist[MOUSE_LIMITS['y']:41081, :MOUSE_LIMITS['x']],\n",
        "        3: mouse_hist[:MOUSE_LIMITS['y'], MOUSE_LIMITS['x']:81654]\n",
        "    }\n",
        "    \n",
        "    return in_sample_fold_to_hist, mouse_fold_to_hist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function decorator for logging\n",
        "def log_function_call(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        logging.info(f'Calling {func.__name__}')\n",
        "        try:\n",
        "            result = func(*args, **kwargs)\n",
        "            logging.info(f'Successfully completed {func.__name__}')\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logging.error(f'Error in {func.__name__}: {str(e)}')\n",
        "            raise\n",
        "    return wrapper\n",
        "\n",
        "# Safe figure saving function\n",
        "@log_function_call\n",
        "def safe_save_figure(fig, filepath, dpi=FIGURE_PARAMS['dpi'], transparent=FIGURE_PARAMS['transparent']):\n",
        "    \"\"\"\n",
        "    Safely save a matplotlib figure with error handling.\n",
        "    \n",
        "    Args:\n",
        "        fig: matplotlib figure object\n",
        "        filepath: path to save the figure\n",
        "        dpi: dots per inch for the output\n",
        "        transparent: whether to use transparent background\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        \n",
        "        # Save figure\n",
        "        fig.savefig(filepath, dpi=dpi, transparent=transparent,\n",
        "                   bbox_inches='tight')\n",
        "        logging.info(f'Successfully saved figure to {filepath}')\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f'Error saving figure to {filepath}: {str(e)}')\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "def trans_good(x):\n",
        "    \"\"\"Transform expression values using log1p of rounded exp-1.\"\"\"\n",
        "    return np.log1p(((np.exp(x)) - 1.).round())\n",
        "\n",
        "@njit(parallel=True)\n",
        "def transform_coordinates(horis, verts):\n",
        "    \"\"\"\n",
        "    Transform spatial coordinates using numba-optimized parallel processing.\n",
        "    \n",
        "    Args:\n",
        "        horis: Horizontal coordinates\n",
        "        verts: Vertical coordinates\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (new_horis, new_verts) transformed coordinates\n",
        "    \"\"\"\n",
        "    new_horis = np.zeros_like(horis)\n",
        "    new_verts = np.zeros_like(verts)\n",
        "    \n",
        "    for t in prange(len(horis)):\n",
        "        i = horis[t]\n",
        "        j = verts[t]\n",
        "        new_horis[t] = i\n",
        "        new_verts[t] = j\n",
        "        \n",
        "    return new_horis, new_verts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analysis functions\n",
        "@log_function_call\n",
        "def get_prog_info(input_true_adata, input_pred_adata, programs):\n",
        "    \"\"\"\n",
        "    Calculate program scores and correlations.\n",
        "    \n",
        "    Args:\n",
        "        input_true_adata: AnnData object with true expression\n",
        "        input_pred_adata: AnnData object with predicted expression\n",
        "        programs: Dictionary mapping program names to gene lists\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (program_to_score, program_to_corr) dictionaries\n",
        "    \"\"\"\n",
        "    pred_adata = sc.AnnData(X=zscore(input_pred_adata.X, axis=1), \n",
        "                           obs=input_pred_adata.obs, \n",
        "                           var=input_pred_adata.var)\n",
        "    true_adata = sc.AnnData(X=zscore(input_true_adata.X, axis=1), \n",
        "                           obs=input_true_adata.obs, \n",
        "                           var=input_true_adata.var)\n",
        "    \n",
        "    program_to_score = {}\n",
        "    program_to_corr = {}\n",
        "    \n",
        "    for program, program_genes in tqdm(programs.items()):\n",
        "        program_genes = np.intersect1d(program_genes, pred_adata.var.index)\n",
        "        if len(program_genes) < 5:\n",
        "            continue\n",
        "            \n",
        "        all_ps = np.nan_to_num(pred_adata[::,program_genes].X.mean(axis=1))\n",
        "        all_ts = np.nan_to_num(true_adata[::,program_genes].X.mean(axis=1))\n",
        "        \n",
        "        program_to_corr[program] = np.corrcoef(all_ps, all_ts)[0, 1]\n",
        "        program_to_score[program] = all_ps.std()\n",
        "        \n",
        "    return program_to_score, program_to_corr\n",
        "\n",
        "@log_function_call\n",
        "def calculate_cell_type_means(data, cell_types):\n",
        "    \"\"\"\n",
        "    Calculate mean expression for each cell type.\n",
        "    \n",
        "    Args:\n",
        "        data: Expression matrix (cells  genes)\n",
        "        cell_types: Cell type labels\n",
        "        \n",
        "    Returns:\n",
        "        dict: Mean expression per cell type\n",
        "    \"\"\"\n",
        "    unique_types = np.unique(cell_types)\n",
        "    means = {}\n",
        "    \n",
        "    for ct in unique_types:\n",
        "        mask = cell_types == ct\n",
        "        means[ct] = np.mean(data[mask], axis=0)\n",
        "            \n",
        "    return means\n",
        "\n",
        "@log_function_call\n",
        "def calculate_heterogeneity(data, cell_types):\n",
        "    \"\"\"\n",
        "    Calculate expression heterogeneity within cell types.\n",
        "    \n",
        "    Args:\n",
        "        data: Expression matrix (cells  genes)\n",
        "        cell_types: Cell type labels\n",
        "        \n",
        "    Returns:\n",
        "        dict: Heterogeneity scores per cell type\n",
        "    \"\"\"\n",
        "    unique_types = np.unique(cell_types)\n",
        "    heterogeneity = {}\n",
        "    \n",
        "    for ct in unique_types:\n",
        "        mask = cell_types == ct\n",
        "        ct_data = data[mask]\n",
        "        \n",
        "        # Calculate variance across cells\n",
        "        var = np.var(ct_data, axis=0)\n",
        "        \n",
        "        # Calculate coefficient of variation\n",
        "        mean = np.mean(ct_data, axis=0)\n",
        "        cv = np.divide(np.sqrt(var), mean,\n",
        "                      out=np.zeros_like(var),\n",
        "                      where=mean!=0)\n",
        "        \n",
        "        heterogeneity[ct] = cv\n",
        "        \n",
        "    return heterogeneity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading and Preprocessing Functions\n",
        "@log_function_call\n",
        "def load_data(data_path, sample_id):\n",
        "    \"\"\"\n",
        "    Load data for a specific sample.\n",
        "    \n",
        "    Args:\n",
        "        data_path: Path to data directory\n",
        "        sample_id: Sample identifier\n",
        "        \n",
        "    Returns:\n",
        "        dict: Loaded data\n",
        "    \"\"\"\n",
        "    # Load ground truth data\n",
        "    ground_truth = np.load(os.path.join(\n",
        "        data_path, f'ground_truth_{sample_id}.npy'\n",
        "    ))\n",
        "    \n",
        "    # Load predictions\n",
        "    predictions = np.load(os.path.join(\n",
        "        data_path, f'predictions_{sample_id}.npy'\n",
        "    ))\n",
        "    \n",
        "    # Load cell type labels\n",
        "    cell_types = np.load(os.path.join(\n",
        "        data_path, f'cell_types_{sample_id}.npy'\n",
        "    ))\n",
        "    \n",
        "    return {\n",
        "        'ground_truth': ground_truth,\n",
        "        'predictions': predictions,\n",
        "        'cell_types': cell_types\n",
        "    }\n",
        "\n",
        "@log_function_call\n",
        "def preprocess_data(data_dict):\n",
        "    \"\"\"\n",
        "    Preprocess loaded data.\n",
        "    \n",
        "    Args:\n",
        "        data_dict: Dictionary containing loaded data\n",
        "        \n",
        "    Returns:\n",
        "        dict: Preprocessed data\n",
        "    \"\"\"\n",
        "    # Normalize expression values\n",
        "    ground_truth_norm = data_dict['ground_truth'] / np.sum(\n",
        "        data_dict['ground_truth'], axis=1, keepdims=True\n",
        "    )\n",
        "    predictions_norm = data_dict['predictions'] / np.sum(\n",
        "        data_dict['predictions'], axis=1, keepdims=True\n",
        "    )\n",
        "    \n",
        "    # Calculate cell type frequencies\n",
        "    unique_types, type_counts = np.unique(\n",
        "        data_dict['cell_types'], \n",
        "        return_counts=True\n",
        "    )\n",
        "    type_freqs = type_counts / len(data_dict['cell_types'])\n",
        "    \n",
        "    return {\n",
        "        'ground_truth_norm': ground_truth_norm,\n",
        "        'predictions_norm': predictions_norm,\n",
        "        'cell_types': data_dict['cell_types'],\n",
        "        'type_frequencies': dict(zip(unique_types, type_freqs))\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Neural Network Model\n",
        "class CellTypeClassifier(nn.Module):\n",
        "    \"\"\"Neural network for cell type classification.\"\"\"\n",
        "    def __init__(self, input_size, num_classes, hidden_sizes=[1024, 256, 64]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        \n",
        "        for size in hidden_sizes:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_size, size),\n",
        "                nn.BatchNorm1d(size),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "            prev_size = size\n",
        "            \n",
        "        layers.extend([\n",
        "            nn.Linear(prev_size, num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        ])\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "@log_function_call\n",
        "def train_celltype_classifier(orig_adata, pred_adata, celltype_name,\n",
        "                            batch_size=128, epochs=10, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Train a cell type classifier on original data and predict on new data.\n",
        "    \n",
        "    Args:\n",
        "        orig_adata: Original AnnData object with cell type labels\n",
        "        pred_adata: AnnData object to predict on\n",
        "        celltype_name: Name of cell type column in obs\n",
        "        batch_size: Batch size for training\n",
        "        epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "        \n",
        "    Returns:\n",
        "        np.array: Predicted cell type annotations\n",
        "    \"\"\"\n",
        "    common_var = np.intersect1d(orig_adata.var.index, pred_adata.var.index)\n",
        "    annos = np.unique(orig_adata.obs[celltype_name])\n",
        "    anno_to_label = dict(zip(annos, range(len(annos))))\n",
        "    label_to_anno = dict(zip(range(len(annos)), annos))\n",
        "    \n",
        "    # Prepare data\n",
        "    orig_features = orig_adata[::,common_var].X\n",
        "    tm = orig_features.mean(axis=0)\n",
        "    ts = orig_features.std(axis=0)\n",
        "    orig_features = ((orig_features - tm) / ts)\n",
        "    orig_features = np.nan_to_num(orig_features)\n",
        "    \n",
        "    orig_labels = np.array([anno_to_label[anno]\n",
        "                           for anno in orig_adata.obs[celltype_name]])\n",
        "    \n",
        "    # Create model and optimizer\n",
        "    model = CellTypeClassifier(common_var.shape[0], len(annos)).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # Compute class weights\n",
        "    class_weights = torch.tensor([\n",
        "        (float(orig_features.shape[0]) / np.sum(orig_labels==i))\n",
        "        for i in range(len(annos))\n",
        "    ]).float().to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(\n",
        "            torch.from_numpy(orig_features),\n",
        "            torch.from_numpy(orig_labels)\n",
        "        ),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=6,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        for batch, labels in train_loader:\n",
        "            batch = batch.to(device) / 10.\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch.float())\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "        print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader.dataset):.6f}')\n",
        "    \n",
        "    # Prepare prediction data\n",
        "    pred_features = pred_adata[::,common_var].X\n",
        "    pred_features = ((pred_features - pred_features.mean(axis=0)) /\n",
        "                    pred_features.std(axis=0))\n",
        "    pred_features = np.nan_to_num(pred_features)\n",
        "    \n",
        "    pred_loader = DataLoader(\n",
        "        TensorDataset(torch.from_numpy(pred_features)),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=6,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for (batch,) in pred_loader:\n",
        "            batch = batch.to(device) / 10.\n",
        "            outputs = model(batch.float())\n",
        "            predictions.extend(outputs.cpu().numpy().argmax(axis=1))\n",
        "    \n",
        "    # Convert predictions to annotations\n",
        "    new_annos = np.array([label_to_anno[label] for label in predictions])\n",
        "    \n",
        "    # Cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return new_annos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting Functions\n",
        "@log_function_call\n",
        "def plot_htapp_schematic(hist_dict, output_dir=OUTPUT_DIR):\n",
        "    \"\"\"\n",
        "    Create and save HTAPP schematic figure.\n",
        "    \n",
        "    Args:\n",
        "        hist_dict (dict): Dictionary of histology images\n",
        "        output_dir (str): Directory to save output figure\n",
        "    \"\"\"\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(3, 4, figsize=(9, 9), \n",
        "                          gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "    \n",
        "    # Plot each histology image\n",
        "    for i, (k, v) in enumerate(hist_dict.items()):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        \n",
        "        # Flip image for correct orientation\n",
        "        flipped_v = np.transpose(v, (1, 0, 2))\n",
        "        \n",
        "        # Plot and format\n",
        "        ax[row, col].imshow(flipped_v)\n",
        "        ax[row, col].axis('off')\n",
        "        ax[row, col].set_title(f'HTAPP {k}', pad=15, loc='center')\n",
        "    \n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    \n",
        "    # Save figure\n",
        "    safe_save_figure(fig, os.path.join(output_dir, 'htapp_schematic.png'))\n",
        "    plt.close()\n",
        "\n",
        "@log_function_call\n",
        "def plot_mskcc_schematic(image_dir, output_dir=OUTPUT_DIR, images_per_row=6):\n",
        "    \"\"\"\n",
        "    Create and save MSKCC schematic figure.\n",
        "    \n",
        "    Args:\n",
        "        image_dir (str): Directory containing image files\n",
        "        output_dir (str): Directory to save output figure\n",
        "        images_per_row (int): Number of images per row in grid\n",
        "    \"\"\"\n",
        "    # Get image files\n",
        "    image_files = [f for f in os.listdir(image_dir) \n",
        "                  if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "    \n",
        "    # Calculate grid dimensions\n",
        "    n_images = len(image_files)\n",
        "    n_cols = images_per_row\n",
        "    n_rows = (n_images + n_cols - 1) // n_cols\n",
        "    \n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # Plot images\n",
        "    for i, image_file in tqdm(enumerate(image_files), desc=\"Processing images\"):\n",
        "        # Load and process image\n",
        "        img_path = os.path.join(image_dir, image_file)\n",
        "        img = Image.open(img_path)\n",
        "        \n",
        "        # Special processing for specific images\n",
        "        if any(id in image_file for id in ['133729', '129477']):\n",
        "            img = np.array(img)\n",
        "            third = img.shape[0] // 3\n",
        "            img = img[third:-third]\n",
        "        \n",
        "        # Plot and format\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f'MSKCC {image_file[:6]}')\n",
        "    \n",
        "    # Clear unused axes\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    \n",
        "    # Adjust and save\n",
        "    plt.tight_layout()\n",
        "    safe_save_figure(fig, os.path.join(output_dir, 'mskcc_schematic.png'))\n",
        "    plt.close()\n",
        "\n",
        "@log_function_call\n",
        "def compute_mean_zscore(adata, gene_set, truncate_at=5):\n",
        "    \"\"\"\n",
        "    Compute mean z-score for a gene set with truncation.\n",
        "    \n",
        "    Args:\n",
        "        adata (AnnData): Expression data\n",
        "        gene_set (list): List of genes to analyze\n",
        "        truncate_at (float): Value to truncate z-scores at\n",
        "        \n",
        "    Returns:\n",
        "        np.array: Mean z-scores per cell\n",
        "    \"\"\"\n",
        "    # Get genes present in data\n",
        "    genes = np.intersect1d(gene_set, adata.var.index)\n",
        "    if len(genes) == 0:\n",
        "        return None\n",
        "        \n",
        "    # Calculate z-scores\n",
        "    zscores = zscore(adata[:, genes].X, axis=1)\n",
        "    \n",
        "    # Truncate values\n",
        "    zscores = np.clip(zscores, -truncate_at, truncate_at)\n",
        "    \n",
        "    # Calculate mean\n",
        "    return np.mean(zscores, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image loading and processing functions\n",
        "def load_htapp_images(hists_dir='/mounts/stultzlab03/ccomiter/htapp_supervise/final_data0315/hists_may_good'):\n",
        "    \"\"\"\n",
        "    Load HTAPP histology images.\n",
        "    \n",
        "    Args:\n",
        "        hists_dir (str): Directory containing HTAPP histology images\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary mapping sample IDs to histology images\n",
        "    \"\"\"\n",
        "    htapp_hists = {}\n",
        "    \n",
        "    for f in tqdm(os.listdir(hists_dir), desc=\"Loading HTAPP images\"):\n",
        "        # Skip non-PNG and segmentation files\n",
        "        if not f.endswith('.png') or 'seg' in f:\n",
        "            continue\n",
        "            \n",
        "        # Load image\n",
        "        k = f.split('.')[0]\n",
        "        htapp_hists[k] = iio.imread(os.path.join(hists_dir, f))\n",
        "        \n",
        "    return htapp_hists\n",
        "\n",
        "def load_placenta_images(image_dir='newest_bestest_placenta_hes'):\n",
        "    \"\"\"\n",
        "    Load placenta histology images.\n",
        "    \n",
        "    Args:\n",
        "        image_dir (str): Directory containing placenta images\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionary mapping sample IDs to histology images\n",
        "    \"\"\"\n",
        "    # Configure PIL for large images\n",
        "    PIL.Image.MAX_IMAGE_PIXELS = 4017126500\n",
        "    \n",
        "    # Sample ID to filename mapping\n",
        "    k_to_code = {\n",
        "        '7': 'JS34',\n",
        "        '8': 'JS40',\n",
        "        '9': 'JS35',\n",
        "        '11': 'JS36'\n",
        "    }\n",
        "    \n",
        "    # Load images\n",
        "    placenta_hists = {}\n",
        "    for k in tqdm(k_to_code.keys(), desc=\"Loading placenta images\"):\n",
        "        img_path = os.path.join(image_dir, f'{k_to_code[k]}.jpg')\n",
        "        placenta_hists[k] = iio.imread(img_path)\n",
        "        \n",
        "    return placenta_hists\n",
        "\n",
        "def load_base_images():\n",
        "    \"\"\"\n",
        "    Load base histology images for Xenium and mouse data.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (in_sample_hist, out_of_sample, mouse_hist) images\n",
        "    \"\"\"\n",
        "    # Load histology images\n",
        "    in_sample_hist = iio.imread(os.path.join(XENIUM_DIR, 'xenium_hist.png'))\n",
        "    out_of_sample = iio.imread(os.path.join(XENIUM_DIR, 'HE_other_sample_xenium.tif'))\n",
        "    mouse_hist = iio.imread(os.path.join(MOUSE_DIR, 'Xenium_V1_mouse_pup_he_image.ome.tif'))\n",
        "    \n",
        "    return in_sample_hist, out_of_sample, mouse_hist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization functions\n",
        "def plot_htapp_schematic(hist_dict, output_dir='final_figures_schaf_revision_pngs'):\n",
        "    \"\"\"\n",
        "    Create and save HTAPP schematic figure.\n",
        "    \n",
        "    Args:\n",
        "        hist_dict (dict): Dictionary of histology images\n",
        "        output_dir (str): Directory to save output figure\n",
        "    \"\"\"\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(3, 4, figsize=(9, 9), \n",
        "                          gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "    \n",
        "    # Plot each histology image\n",
        "    for i, (k, v) in enumerate(hist_dict.items()):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        \n",
        "        # Flip image for correct orientation\n",
        "        flipped_v = np.transpose(v, (1, 0, 2))\n",
        "        \n",
        "        # Plot and format\n",
        "        ax[row, col].imshow(flipped_v)\n",
        "        ax[row, col].axis('off')\n",
        "        ax[row, col].set_title(f'HTAPP {k}', pad=15, loc='center')\n",
        "    \n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    \n",
        "    # Save figure\n",
        "    plt.savefig(os.path.join(output_dir, 'htapp_schematic.png'), \n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_mskcc_schematic(image_dir, output_dir='final_figures_schaf_revision_pngs', \n",
        "                        images_per_row=6):\n",
        "    \"\"\"\n",
        "    Create and save MSKCC schematic figure.\n",
        "    \n",
        "    Args:\n",
        "        image_dir (str): Directory containing image files\n",
        "        output_dir (str): Directory to save output figure\n",
        "        images_per_row (int): Number of images per row in grid\n",
        "    \"\"\"\n",
        "    # Get image files\n",
        "    image_files = [f for f in os.listdir(image_dir) \n",
        "                  if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "    \n",
        "    # Calculate grid dimensions\n",
        "    n_images = len(image_files)\n",
        "    n_cols = images_per_row\n",
        "    n_rows = (n_images + n_cols - 1) // n_cols\n",
        "    \n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # Plot images\n",
        "    for i, image_file in tqdm(enumerate(image_files), desc=\"Processing images\"):\n",
        "        # Load and process image\n",
        "        img_path = os.path.join(image_dir, image_file)\n",
        "        img = Image.open(img_path)\n",
        "        \n",
        "        # Special processing for specific images\n",
        "        if any(id in image_file for id in ['133729', '129477']):\n",
        "            img = np.array(img)\n",
        "            third = img.shape[0] // 3\n",
        "            img = img[third:-third]\n",
        "        \n",
        "        # Plot and format\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f'MSKCC {image_file[:6]}')\n",
        "    \n",
        "    # Clear unused axes\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    \n",
        "    # Adjust and save\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'mskcc_schematic.png'), \n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Program analysis utilities\n",
        "def compute_mean_zscore(adata, gene_set, truncate_at=5):\n",
        "    \"\"\"\n",
        "    Compute mean z-score for a gene set with truncation.\n",
        "    \n",
        "    Args:\n",
        "        adata (AnnData): Expression data\n",
        "        gene_set (list): List of genes to analyze\n",
        "        truncate_at (float): Value to truncate z-scores at\n",
        "        \n",
        "    Returns:\n",
        "        np.array: Mean z-scores per cell\n",
        "    \"\"\"\n",
        "    # Get genes present in data\n",
        "    genes = np.intersect1d(gene_set, adata.var.index)\n",
        "    if len(genes) == 0:\n",
        "        return None\n",
        "        \n",
        "    # Calculate z-scores\n",
        "    zscores = zscore(adata[:, genes].X, axis=1)\n",
        "    \n",
        "    # Truncate values\n",
        "    zscores = np.clip(zscores, -truncate_at, truncate_at)\n",
        "    \n",
        "    # Calculate mean\n",
        "    return np.mean(zscores, axis=1)\n",
        "\n",
        "# Constants for image regions\n",
        "XENIUM_LIMITS = {'x': 17700, 'y': 12900}\n",
        "MOUSE_LIMITS = {'x': 36500, 'y': 19500}\n",
        "\n",
        "# Create fold dictionaries\n",
        "def create_fold_dicts(in_sample_hist, mouse_hist):\n",
        "    \"\"\"\n",
        "    Create dictionaries mapping folds to image regions.\n",
        "    \n",
        "    Args:\n",
        "        in_sample_hist: In-sample histology image\n",
        "        mouse_hist: Mouse histology image\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (in_sample_fold_to_hist, mouse_fold_to_hist) dictionaries\n",
        "    \"\"\"\n",
        "    in_sample_fold_to_hist = {\n",
        "        0: in_sample_hist[:XENIUM_LIMITS['y'], :XENIUM_LIMITS['x']],\n",
        "        1: in_sample_hist[XENIUM_LIMITS['y']:25761, XENIUM_LIMITS['x']:35402],\n",
        "        2: in_sample_hist[XENIUM_LIMITS['y']:25761, :XENIUM_LIMITS['x']],\n",
        "        3: in_sample_hist[:XENIUM_LIMITS['y'], XENIUM_LIMITS['x']:35402]\n",
        "    }\n",
        "\n",
        "    mouse_fold_to_hist = {\n",
        "        0: mouse_hist[:MOUSE_LIMITS['y'], :MOUSE_LIMITS['x']],\n",
        "        1: mouse_hist[MOUSE_LIMITS['y']:41081, MOUSE_LIMITS['x']:81654],\n",
        "        2: mouse_hist[MOUSE_LIMITS['y']:41081, :MOUSE_LIMITS['x']],\n",
        "        3: mouse_hist[:MOUSE_LIMITS['y'], MOUSE_LIMITS['x']:81654]\n",
        "    }\n",
        "    \n",
        "    return in_sample_fold_to_hist, mouse_fold_to_hist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import sys\n",
        "import os \n",
        "import yaml\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import wandb\n",
        "import matplotlib as mpl\n",
        "import scipy\n",
        "from scipy.stats import zscore\n",
        "import imageio.v3 as iio\n",
        "from PIL import Image\n",
        "import PIL\n",
        "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
        "\n",
        "import datetime\n",
        "import random \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import scipy as sp \n",
        "from tqdm import tqdm \n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils as U\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
        "import scanpy as sc \n",
        "from numba import njit, prange\n",
        "from scipy.spatial import cKDTree\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import r2_score\n",
        "import anndata as ad\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import pickle\n",
        "\n",
        "# Add necessary paths\n",
        "sys.path.extend([\n",
        "    \".\", \"..\",\n",
        "    \"/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/final_new_schaf_start_jan2324\",\n",
        "])\n",
        "\n",
        "from models import MerNet, JustPartTwo\n",
        "from utils import *\n",
        "from plot_utils import *\n",
        "\n",
        "# Set up GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "device = torch.device('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration and global parameters\n",
        "import os\n",
        "\n",
        "# Data paths\n",
        "DATA_ROOT = '/storage/ccomiter/schaf_for_revision052424'\n",
        "XENIUM_DIR = os.path.join(DATA_ROOT, 'data/xenium_cancer')\n",
        "MOUSE_DIR = os.path.join(DATA_ROOT, 'data/mouse_data')\n",
        "HTAPP_DIR = os.path.join(DATA_ROOT, 'data/htapp_data')\n",
        "OUTPUT_DIR = os.path.join(DATA_ROOT, 'final_schaf_code/final_figures_schaf_revision_pngs')\n",
        "\n",
        "# File paths\n",
        "PROGRAM_FILES = {\n",
        "    'hallmark': '/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/final_new_schaf_start_jan2324/hallmark_programs.json',\n",
        "    'cancer': '/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/final_new_schaf_start_jan2324/cancer_programs.json'\n",
        "}\n",
        "\n",
        "CELL_TYPE_FILES = {\n",
        "    'clusters': '/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/analysis/clustering/gene_expression_kmeans_10_clusters/clusters.csv'\n",
        "}\n",
        "\n",
        "# Visualization parameters\n",
        "FIGURE_PARAMS = {\n",
        "    'dpi': 400,\n",
        "    'transparent': True,\n",
        "    'font_sizes': {\n",
        "        'xx-large': 16,\n",
        "        'x-large': 14,\n",
        "        'large': 12,\n",
        "        'medium': 10,\n",
        "        'small': 8\n",
        "    }\n",
        "}\n",
        "\n",
        "# Color maps for marker genes\n",
        "MARKER_GENE_CMAPS = {\n",
        "    'krt19': plt.cm.Greens,\n",
        "    'col1a2': plt.cm.YlOrBr,\n",
        "    'apoc1': plt.cm.Blues,\n",
        "    'pecam1': plt.cm.Reds\n",
        "}\n",
        "\n",
        "# HTAPP sample IDs\n",
        "HTAPP_SAMPLES = {\n",
        "    'main': ['6760', '7149', '7179'],\n",
        "    'extended': ['4531', '6760', '7479', '7629'],\n",
        "    'all': ['7149', '7179', '932', '6760', '8239', '7629', '4531', '7479', '4381']\n",
        "}\n",
        "\n",
        "# Cell type mapping\n",
        "CELL_TYPE_MAPPING = {\n",
        "    'broad_to_specific': {\n",
        "        'Tumor': ['MBC', 'MBC_stem-like', 'MBC_neuronal', 'MBC_chondroid'],\n",
        "        'Vascular': ['Endothelial', 'Endothelial_sinusoidal', 'Endothelial_angiogenic', 'Endothelial_vascular'],\n",
        "        'Immune': ['Macrophage', 'Monocyte', 'Neutrophil', 'B', 'T', 'NK'],\n",
        "        'Fibrosis': ['Fibroblast', 'Chondrocyte', 'Smooth muscle_vascular']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error handling and logging setup\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from functools import wraps\n",
        "\n",
        "# Set up logging\n",
        "log_file = os.path.join(OUTPUT_DIR, f'analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def log_function_call(func):\n",
        "    \"\"\"Decorator to log function calls and handle errors.\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        func_name = func.__name__\n",
        "        logger.info(f\"Starting {func_name}\")\n",
        "        try:\n",
        "            result = func(*args, **kwargs)\n",
        "            logger.info(f\"Completed {func_name}\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in {func_name}: {str(e)}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            raise\n",
        "    return wrapper\n",
        "\n",
        "class AnalysisError(Exception):\n",
        "    \"\"\"Custom exception for analysis errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "def validate_data(data, name, requirements):\n",
        "    \"\"\"Validate input data against requirements.\"\"\"\n",
        "    if data is None:\n",
        "        raise AnalysisError(f\"{name} is None\")\n",
        "    \n",
        "    for req in requirements:\n",
        "        if req == 'shape':\n",
        "            if not hasattr(data, 'shape'):\n",
        "                raise AnalysisError(f\"{name} has no shape attribute\")\n",
        "        elif req == 'positive':\n",
        "            if not np.all(data >= 0):\n",
        "                raise AnalysisError(f\"{name} contains negative values\")\n",
        "        elif req == 'finite':\n",
        "            if not np.all(np.isfinite(data)):\n",
        "                raise AnalysisError(f\"{name} contains non-finite values\")\n",
        "        elif req == 'normalized':\n",
        "            if not np.allclose(data.sum(axis=1), 1.0):\n",
        "                raise AnalysisError(f\"{name} is not normalized\")\n",
        "\n",
        "def check_file_exists(filepath):\n",
        "    \"\"\"Check if a file exists.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "@log_function_call\n",
        "def safe_save_figure(fig, filename, **kwargs):\n",
        "    \"\"\"Safely save a figure with error handling.\"\"\"\n",
        "    try:\n",
        "        # Ensure the output directory exists\n",
        "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "        \n",
        "        # Save the figure\n",
        "        fig.savefig(filename, **kwargs)\n",
        "        logger.info(f\"Saved figure to {filename}\")\n",
        "        \n",
        "        # Close the figure to free memory\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving figure {filename}: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading functions with validation and error handling\n",
        "\n",
        "@log_function_call\n",
        "def load_out_of_sample_data():\n",
        "    \"\"\"\n",
        "    Load out-of-sample data with validation.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (ground_truth_data, predicted_data)\n",
        "            - ground_truth_data: AnnData object containing ground truth expression\n",
        "            - predicted_data: AnnData object containing predicted expression\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If data files are not found\n",
        "        AnalysisError: If data validation fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load ground truth data\n",
        "        gt_path = os.path.join(XENIUM_DIR, 'ground_truth_out_of_sample.h5ad')\n",
        "        check_file_exists(gt_path)\n",
        "        ground_truth = sc.read_h5ad(gt_path)\n",
        "        \n",
        "        # Load predictions\n",
        "        pred_path = os.path.join(XENIUM_DIR, 'predictions_out_of_sample.h5ad')\n",
        "        check_file_exists(pred_path)\n",
        "        predictions = sc.read_h5ad(pred_path)\n",
        "        \n",
        "        # Validate data\n",
        "        for data, name in [(ground_truth, 'ground_truth'), \n",
        "                          (predictions, 'predictions')]:\n",
        "            validate_data(data.X, name, ['shape', 'finite'])\n",
        "        \n",
        "        return ground_truth, predictions\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading out-of-sample data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "@log_function_call\n",
        "def load_in_sample_data():\n",
        "    \"\"\"\n",
        "    Load in-sample data for all folds with validation.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (ground_truth_data, predicted_data)\n",
        "            Each is a dictionary mapping fold index to AnnData object\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If data files are not found\n",
        "        AnalysisError: If data validation fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ground_truth = {}\n",
        "        predictions = {}\n",
        "        \n",
        "        for fold in range(4):\n",
        "            # Load ground truth\n",
        "            gt_path = os.path.join(XENIUM_DIR, f'ground_truth_fold_{fold}.h5ad')\n",
        "            check_file_exists(gt_path)\n",
        "            ground_truth[fold] = sc.read_h5ad(gt_path)\n",
        "            \n",
        "            # Load predictions\n",
        "            pred_path = os.path.join(XENIUM_DIR, f'predictions_fold_{fold}.h5ad')\n",
        "            check_file_exists(pred_path)\n",
        "            predictions[fold] = sc.read_h5ad(pred_path)\n",
        "            \n",
        "            # Validate data\n",
        "            for data, name in [(ground_truth[fold], f'ground_truth_fold_{fold}'),\n",
        "                             (predictions[fold], f'predictions_fold_{fold}')]:\n",
        "                validate_data(data.X, name, ['shape', 'finite'])\n",
        "        \n",
        "        return ground_truth, predictions\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading in-sample data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "@log_function_call\n",
        "def load_mouse_data():\n",
        "    \"\"\"\n",
        "    Load mouse data with validation.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (ground_truth_data, predicted_data)\n",
        "            Each is a dictionary mapping fold index to AnnData object\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If data files are not found\n",
        "        AnalysisError: If data validation fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ground_truth = {}\n",
        "        predictions = {}\n",
        "        \n",
        "        for fold in range(4):\n",
        "            # Load ground truth\n",
        "            gt_path = os.path.join(MOUSE_DIR, f'ground_truth_mouse_fold_{fold}.h5ad')\n",
        "            check_file_exists(gt_path)\n",
        "            ground_truth[fold] = sc.read_h5ad(gt_path)\n",
        "            \n",
        "            # Load predictions\n",
        "            pred_path = os.path.join(MOUSE_DIR, f'predictions_mouse_fold_{fold}.h5ad')\n",
        "            check_file_exists(pred_path)\n",
        "            predictions[fold] = sc.read_h5ad(pred_path)\n",
        "            \n",
        "            # Validate data\n",
        "            for data, name in [(ground_truth[fold], f'ground_truth_mouse_fold_{fold}'),\n",
        "                             (predictions[fold], f'predictions_mouse_fold_{fold}')]:\n",
        "                validate_data(data.X, name, ['shape', 'finite'])\n",
        "        \n",
        "        return ground_truth, predictions\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading mouse data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "@log_function_call\n",
        "def load_programs():\n",
        "    \"\"\"\n",
        "    Load and process program definitions.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Combined dictionary of hallmark and cancer programs\n",
        "            with lowercase gene symbols\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If program files are not found\n",
        "        JSONDecodeError: If JSON parsing fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load hallmark programs\n",
        "        check_file_exists(PROGRAM_FILES['hallmark'])\n",
        "        with open(PROGRAM_FILES['hallmark']) as f:\n",
        "            hallmark_programs = json.load(f)\n",
        "        \n",
        "        # Load cancer programs\n",
        "        check_file_exists(PROGRAM_FILES['cancer'])\n",
        "        with open(PROGRAM_FILES['cancer']) as f:\n",
        "            cancer_programs = json.load(f)\n",
        "        \n",
        "        # Process programs\n",
        "        hallmark_programs = {k: [g.lower() for g in v['geneSymbols']]\n",
        "                           for k, v in hallmark_programs.items()}\n",
        "        cancer_programs = {k: [g.lower() for g in v['geneSymbols']]\n",
        "                         for k, v in cancer_programs.items()}\n",
        "        \n",
        "        # Combine programs\n",
        "        all_programs = hallmark_programs.copy()\n",
        "        all_programs.update(cancer_programs)\n",
        "        \n",
        "        return all_programs\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading programs: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up plotting parameters\n",
        "plt.rcParams['figure.figsize'] = 10, 10\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "plt.rcParams['pdf.fonttype'] = 42\n",
        "plt.rcParams['ps.fonttype'] = 42\n",
        "mpl.rcParams['pdf.fonttype'] = 42\n",
        "mpl.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "do_large = 1\n",
        "def set_fig_params():\n",
        "    plt.rcParams.update(mpl.rcParamsDefault)\n",
        "    plt.rcParams['pdf.fonttype'] = 42\n",
        "    weight = 550\n",
        "    sns.set(context='paper', style='ticks',\n",
        "            rc={\n",
        "                'figure.autolayout': True,\n",
        "                'axes.titlesize': 'xx-large' if do_large else 8,\n",
        "                'axes.titleweight': weight,\n",
        "                'figure.titleweight': weight,\n",
        "                'figure.titlesize': 'xx-large' if do_large else 8,\n",
        "                'axes.labelsize': 'xx-large' if do_large else 8,\n",
        "                'axes.labelpad': 2,\n",
        "                'axes.labelweight': weight,\n",
        "                'axes.spines.top': False,\n",
        "                'axes.spines.right': False,\n",
        "                'xtick.labelsize': 'x-large' if do_large else 7,\n",
        "                'ytick.labelsize': 'x-large' if do_large else 7,\n",
        "                'legend.fontsize': 'xx-large' if do_large else 7,\n",
        "                'figure.figsize': (3.5, 3.5/1.6),\n",
        "                'xtick.direction': 'out',\n",
        "                'ytick.direction': 'out',\n",
        "                'xtick.major.size': 'xx-large' if 0 else 2,\n",
        "                'ytick.major.size': 'xx-large' if 0 else 2,\n",
        "                'xtick.major.pad': 2,\n",
        "                'ytick.major.pad': 2,\n",
        "                'font.family': 'sans-serif',\n",
        "                'legend.frameon': False,\n",
        "            })\n",
        "\n",
        "set_fig_params()\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load histology images and define fold regions\n",
        "xen_dir = f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium'\n",
        "in_sample_hist = iio.imread(os.path.join(xen_dir, 'xenium_hist.png'))\n",
        "out_of_sample = iio.imread(os.path.join(xen_dir, 'HE_other_sample_xenium.tif'))\n",
        "\n",
        "# Define fold regions for in-sample histology\n",
        "y_lim = 12900\n",
        "x_lim = 17700\n",
        "\n",
        "in_sample_fold_to_hist = {\n",
        "    0: in_sample_hist[:y_lim, :x_lim],\n",
        "    1: in_sample_hist[y_lim:25761, x_lim:35395 if 0 else 35402],\n",
        "    2: in_sample_hist[y_lim:25755 if 0 else 25761, :x_lim],\n",
        "    3: in_sample_hist[:y_lim, x_lim:35402],\n",
        "}\n",
        "\n",
        "# Load mouse histology and define fold regions\n",
        "xen_dir = f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/all_xenium_new_data/mouse_pup_data'\n",
        "mouse_hist = iio.imread(os.path.join(xen_dir, 'Xenium_V1_mouse_pup_he_image.ome.tif'))\n",
        "\n",
        "x_lim = 36500\n",
        "y_lim = 19500\n",
        "\n",
        "mouse_fold_to_hist = {\n",
        "    0: mouse_hist[:y_lim, :x_lim],\n",
        "    1: mouse_hist[y_lim:41081, x_lim:81654],\n",
        "    2: mouse_hist[y_lim:38295 if 0 else 41081, :x_lim],\n",
        "    3: mouse_hist[:y_lim, x_lim:77230 if 0 else 81654],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to compute program information\n",
        "def get_prog_info(input_true_adata, input_pred_adata, programs):\n",
        "    pred_adata = sc.AnnData(X=zscore(input_pred_adata.X, axis=1), obs=input_pred_adata.obs, var=input_pred_adata.var)\n",
        "    true_adata = sc.AnnData(X=zscore(input_true_adata.X, axis=1), obs=input_true_adata.obs, var=input_true_adata.var)\n",
        "    program_to_score = {}\n",
        "    program_to_corr = {}\n",
        "    for program, program_genes in tqdm(programs.items()):\n",
        "        program_genes = np.intersect1d(program_genes, pred_adata.var.index)\n",
        "        if len(program_genes) < 5:\n",
        "            continue\n",
        "        all_ps = np.nan_to_num(pred_adata[::,program_genes].X.mean(axis=1))\n",
        "        all_ts = np.nan_to_num(true_adata[::,program_genes].X.mean(axis=1))\n",
        "        program_to_corr[program] = np.corrcoef(all_ps, all_ts)[0, 1]\n",
        "        program_to_score[program] = all_ps.std()\n",
        "    return program_to_score, program_to_corr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_program_metrics(gene_sets, pred_in_sample, ground_truth_in_sample, all_cancer_programs):\n",
        "    \"\"\"Calculate program correlations and scores across folds.\"\"\"\n",
        "    gene_set_to_fold_to_in_sample_program_corrs = {}\n",
        "    gene_set_to_fold_to_in_sample_program_scores = {}\n",
        "    \n",
        "    for name, genes in gene_sets.items():\n",
        "        fold_to_in_sample_program_corrs = {}\n",
        "        fold_to_in_sample_program_scores = {}\n",
        "        for z in range(4):\n",
        "            p = pred_in_sample[z][::,genes]\n",
        "            t = ground_truth_in_sample[z][::, genes]\n",
        "            p_scores, p_corrs = get_prog_info(t, p, all_cancer_programs)\n",
        "            fold_to_in_sample_program_corrs[z] = p_corrs\n",
        "            fold_to_in_sample_program_scores[z] = p_scores\n",
        "        gene_set_to_fold_to_in_sample_program_corrs[name] = fold_to_in_sample_program_corrs\n",
        "        gene_set_to_fold_to_in_sample_program_scores[name] = fold_to_in_sample_program_scores\n",
        "    \n",
        "    # Calculate weighted averages\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_in_sample[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_in_sample[z].shape[0] / total_cells\n",
        "\n",
        "    gene_set_to_avg_in_sample_program_corrs = {}\n",
        "    gene_set_to_avg_in_sample_program_scores = {}\n",
        "\n",
        "    for name, genes in gene_sets.items():\n",
        "        avg_in_sample_program_corrs = {}\n",
        "        avg_in_sample_program_scores = {}\n",
        "\n",
        "        for prog in gene_set_to_fold_to_in_sample_program_corrs[name][0]:\n",
        "            avg_in_sample_program_corrs[prog] = sum(gene_set_to_fold_to_in_sample_program_corrs[name][z][prog] * fold_to_prop[z] for z in range(4))\n",
        "            avg_in_sample_program_scores[prog] = sum(gene_set_to_fold_to_in_sample_program_scores[name][z][prog] * fold_to_prop[z] for z in range(4))\n",
        "        \n",
        "        gene_set_to_avg_in_sample_program_corrs[name] = avg_in_sample_program_corrs\n",
        "        gene_set_to_avg_in_sample_program_scores[name] = avg_in_sample_program_scores\n",
        "    \n",
        "    return (gene_set_to_fold_to_in_sample_program_corrs, gene_set_to_fold_to_in_sample_program_scores,\n",
        "            gene_set_to_avg_in_sample_program_corrs, gene_set_to_avg_in_sample_program_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_spot_sum_function(a, b, r):\n",
        "    \"\"\"Create spot-based summaries of gene expression for spatial analysis.\"\"\"\n",
        "    # Extract coordinates\n",
        "    a_coords = a.obs[['x', 'y']].values\n",
        "    b_coords = b.obs[['x', 'y']].values\n",
        "    \n",
        "    # Combine coordinates\n",
        "    combined_coords = np.vstack((a_coords, b_coords))\n",
        "    \n",
        "    # Determine grid extent\n",
        "    x_min, x_max = np.min(combined_coords[:, 0]), np.max(combined_coords[:, 0])\n",
        "    y_min, y_max = np.min(combined_coords[:, 1]), np.max(combined_coords[:, 1])\n",
        "    \n",
        "    # Create grid\n",
        "    grid_size = 2 * r\n",
        "    x_edges = np.arange(x_min, x_max + grid_size, grid_size)\n",
        "    y_edges = np.arange(y_min, y_max + grid_size, grid_size)\n",
        "    grid_centers = np.array([(x + grid_size / 2, y + grid_size / 2) \n",
        "                            for x in x_edges for y in y_edges])\n",
        "    \n",
        "    # Build KDTree\n",
        "    kdtree = KDTree(grid_centers)\n",
        "    \n",
        "    # Initialize arrays for spot sums\n",
        "    spot_sums_a = np.zeros((len(grid_centers), a.X.shape[1]))\n",
        "    spot_sums_b = np.zeros((len(grid_centers), b.X.shape[1]))\n",
        "    cspot_sums_a = np.zeros((len(grid_centers), a.X.shape[1]))\n",
        "    cspot_sums_b = np.zeros((len(grid_centers), b.X.shape[1]))\n",
        "    \n",
        "    def assign_points_and_sum(coords, adata, spot_sums, cspot_sums):\n",
        "        _, indices = kdtree.query(coords, k=1)\n",
        "        data_matrix = adata.X.toarray() if not isinstance(adata.X, np.ndarray) else adata.X\n",
        "        for i in range(len(coords)):\n",
        "            spot_idx = indices[i]\n",
        "            spot_sums[spot_idx] += data_matrix[i]\n",
        "            cspot_sums[spot_idx] += 1\n",
        "    \n",
        "    # Assign points and calculate sums\n",
        "    assign_points_and_sum(b_coords, b, spot_sums_b, cspot_sums_b)\n",
        "    assign_points_and_sum(a_coords, a, spot_sums_a, cspot_sums_a)\n",
        "    \n",
        "    # Filter spots\n",
        "    non_zero_b = np.any(cspot_sums_b > 0, axis=1)\n",
        "    non_zero_a = np.any(cspot_sums_a > 0, axis=1)\n",
        "    valid_spots = non_zero_a & non_zero_b\n",
        "    \n",
        "    # Normalize sums\n",
        "    spot_means_a = np.zeros_like(spot_sums_a)\n",
        "    spot_means_b = np.zeros_like(spot_sums_b)\n",
        "    np.divide(spot_sums_a, cspot_sums_a, where=cspot_sums_a>0, out=spot_means_a)\n",
        "    np.divide(spot_sums_b, cspot_sums_b, where=cspot_sums_b>0, out=spot_means_b)\n",
        "    \n",
        "    return spot_means_a[valid_spots], spot_means_b[valid_spots], grid_centers[valid_spots]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_spatial_plots(pred_data, true_data, cell_type_labels, distance_threshold=30):\n",
        "    \"\"\"Generate spatial plots with cell type annotations.\"\"\"\n",
        "    # Create color map for cell types\n",
        "    colors = {\n",
        "        'Tumor': '#73d56d',\n",
        "        'Normal': '#f3a3f6',\n",
        "        'Vascular': '#feb052',\n",
        "        'Immune': '#99d1fe',\n",
        "        'Fibrosis': '#ced208'\n",
        "    }\n",
        "    \n",
        "    def make_Ramp(ramp_colors): \n",
        "        from colour import Color\n",
        "        from matplotlib.colors import LinearSegmentedColormap\n",
        "        color_ramp = LinearSegmentedColormap.from_list('cell_types', [Color(c1).rgb for c1 in ramp_colors])\n",
        "        return color_ramp\n",
        "    \n",
        "    # Process cell type labels\n",
        "    the_labels = {}\n",
        "    the_dists = {}\n",
        "    \n",
        "    for k in pred_data.keys():\n",
        "        labels = []\n",
        "        dists = []\n",
        "        for ind in range(pred_data[k].shape[0]):\n",
        "            label = cell_type_labels[k].iloc[ind]\n",
        "            dd = compute_distance_to_nearest(pred_data[k].obs.iloc[ind], true_data[k])\n",
        "            labels.append(label)\n",
        "            dists.append(dd)\n",
        "        the_labels[k] = np.array(labels)\n",
        "        the_dists[k] = np.array(dists)\n",
        "    \n",
        "    # Generate plots\n",
        "    k_to_where = {}\n",
        "    for k in pred_data.keys():\n",
        "        fig, ax = plt.subplots(figsize=(5, 5 * pred_data[k].shape[1] / pred_data[k].shape[0]))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        \n",
        "        # Process cell type assignments\n",
        "        tcs = []\n",
        "        for l in the_labels[k]:\n",
        "            vls = [i for i, j in enumerate(l) if j]\n",
        "            if len(vls) == 0:\n",
        "                tcs.append(0)\n",
        "            elif len(vls) == 1:\n",
        "                tcs.append(vls[0])\n",
        "            else:\n",
        "                tcs.append(random.choice(vls))\n",
        "        tcs = np.array(tcs)\n",
        "        \n",
        "        # Filter points by distance threshold\n",
        "        to_show = np.where((the_dists[k] < distance_threshold) & (np.array([i[j] for i, j in zip(the_labels[k], tcs)]) > 0))[0]\n",
        "        k_to_where[k] = to_show\n",
        "        \n",
        "        # Create scatter plot\n",
        "        plt.scatter(\n",
        "            pred_data[k].obs['y'][to_show],\n",
        "            pred_data[k].obs['x'][to_show],\n",
        "            c=tcs[to_show],\n",
        "            s=10,\n",
        "            cmap=make_Ramp(list(colors.values()))\n",
        "        )\n",
        "        \n",
        "        ax.spines[['left', 'right', 'top', 'bottom']].set_visible(True)\n",
        "        plt.setp(ax.spines.values(), linewidth=2)\n",
        "        plt.gca().invert_yaxis()\n",
        "        \n",
        "        # Save figure\n",
        "        fig_name = f'spatial_plot_cell_types_{k}'\n",
        "        plt.savefig(f'final_figures_schaf_revision_pngs/{fig_name}.png', dpi=400, transparent=True)\n",
        "        plt.close()\n",
        "    \n",
        "    return k_to_where\n",
        "\n",
        "def compute_distance_to_nearest(point, reference_data):\n",
        "    \"\"\"Compute distance to nearest point in reference dataset.\"\"\"\n",
        "    point_coords = np.array([point['x'], point['y']])\n",
        "    ref_coords = reference_data.obs[['x', 'y']].values\n",
        "    distances = np.sqrt(np.sum((ref_coords - point_coords) ** 2, axis=1))\n",
        "    return np.min(distances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_correlation_heatmap(pred_data, true_data, common_genes):\n",
        "    \"\"\"Generate correlation heatmaps between predicted and true data.\"\"\"\n",
        "    fold_to_pred_heatmap = {}\n",
        "    fold_to_true_heatmap = {}\n",
        "    fold_to_metacorr = {}\n",
        "\n",
        "    for z in range(4):\n",
        "        # Prepare data\n",
        "        pred_arr = np.array(pred_data[z][::,common_genes].X)\n",
        "        true_arr = np.array(true_data[z][::,common_genes].X.squeeze())\n",
        "        \n",
        "        # Calculate correlation matrices\n",
        "        pred_heatmap = np.corrcoef(pred_arr, rowvar=0)\n",
        "        true_heatmap = np.corrcoef(true_arr, rowvar=0)\n",
        "        \n",
        "        # Cluster genes on first fold\n",
        "        if z == 0:\n",
        "            hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
        "            labels = hierarchical_cluster.fit_predict(true_arr.T)\n",
        "            c1_inds = [i for i, l in enumerate(labels) if l]\n",
        "            c2_inds = [i for i, l in enumerate(labels) if not l]\n",
        "        \n",
        "        # Reorder heatmaps based on clustering\n",
        "        pred_heatmap = pred_heatmap[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "        true_heatmap = true_heatmap[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "        \n",
        "        # Store results\n",
        "        fold_to_pred_heatmap[z] = pred_heatmap\n",
        "        fold_to_true_heatmap[z] = true_heatmap\n",
        "        fold_to_metacorr[z] = np.corrcoef(pred_heatmap.reshape(-1), true_heatmap.reshape(-1))[0, 1]\n",
        "    \n",
        "    # Calculate weighted average across folds\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(true_data[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = true_data[z].shape[0] / total_cells\n",
        "    \n",
        "    avg_pred_heatmap = sum([fold_to_pred_heatmap[z]*fold_to_prop[z] for z in range(4)])\n",
        "    avg_true_heatmap = sum([fold_to_true_heatmap[z]*fold_to_prop[z] for z in range(4)])\n",
        "    avg_metacorr = sum(fold_to_metacorr[z]*fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    # Plot heatmaps\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    sns.heatmap(avg_true_heatmap, ax=ax1, cmap='coolwarm', center=0)\n",
        "    ax1.set_title('Ground Truth Correlation')\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "    \n",
        "    sns.heatmap(avg_pred_heatmap, ax=ax2, cmap='coolwarm', center=0)\n",
        "    ax2.set_title('Predicted Correlation')\n",
        "    ax2.set_xticks([])\n",
        "    ax2.set_yticks([])\n",
        "    \n",
        "    plt.suptitle(f'Average Metacorrelation: {avg_metacorr:.3f}')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save figure\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/correlation_heatmap.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "    \n",
        "    return avg_metacorr\n",
        "\n",
        "def plot_correlation_histogram(gene_correlations, title, filename):\n",
        "    \"\"\"Generate histogram of gene-level correlations.\"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(gene_correlations, bins=50, edgecolor='black')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Correlation')\n",
        "    plt.ylabel('Count')\n",
        "    plt.axvline(x=np.mean(gene_correlations), color='r', linestyle='--', \n",
        "                label=f'Mean: {np.mean(gene_correlations):.3f}')\n",
        "    plt.axvline(x=np.median(gene_correlations), color='g', linestyle='--',\n",
        "                label=f'Median: {np.median(gene_correlations):.3f}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png', dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mouse-specific gene lists and program loading\n",
        "mouse_train_genes = [\n",
        "    'hmgcs2', 'krt19', 'sdc1', 'scgb3a2', 'dbpht2', 'try10', 'hrc', \n",
        "    'pck1', 'gpx6', 'reg3g', 'tubb3', 'sypl2', 'rbp1', 'slc14a2', 'cd5l', \n",
        "    'crct1', 'anxa8', 'slc4a4', '1110017d15rik', 'nupr1', 'gm94', 'higd1b', \n",
        "    'cxcr2', 'tmem59l', 'rsad2', 'podxl', 'aqp1', 'cd8a', 'folr2', 'des', 'col8a1', \n",
        "    'aspn', 'kl', 'nap1l5', 'cubn', 'prx', 'epcam', 'fxyd6', 'aldh1b1', 'gc', 'fcnb', \n",
        "    'etv1', 'ppargc1a', 'ascl1', 'myoz2', 'cnfn', 'hpx', 'a330076h08rik', 'aqp5', \n",
        "    'prss3', 'myoz1', 'dcdc2a', 'emb', 'ucma', 'pgam2', 'mlc1', 'ifitm6', 'scin', \n",
        "    'otor', 'tagln', 'cldn3', 'mpo', 'angpt2', 'ms4a6c', 'aqp3', 'ccdc153', 'slc17a7'\n",
        "]\n",
        "\n",
        "# Load and preprocess mouse data\n",
        "def load_mouse_data():\n",
        "    pred_mouse = {\n",
        "        z: sc.read_h5ad(f'/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/mouse_inferences/fold_{z}.h5ad')\n",
        "        for z in range(4)\n",
        "    }\n",
        "    \n",
        "    ground_truth_mouse = {\n",
        "        z: sc.read_h5ad(f'/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/mouse_folds/fold_{z}_st.h5ad')\n",
        "        for z in range(4)\n",
        "    }\n",
        "    \n",
        "    # Preprocess ground truth data\n",
        "    for z in ground_truth_mouse:\n",
        "        sc.pp.log1p(ground_truth_mouse[z])\n",
        "        ground_truth_mouse[z].X = np.array(ground_truth_mouse[z].X.todense())\n",
        "    \n",
        "    return pred_mouse, ground_truth_mouse\n",
        "\n",
        "# Calculate mouse-specific correlations\n",
        "def calculate_mouse_correlations(pred_mouse, ground_truth_mouse, common_mouse):\n",
        "    mouse_fold_to_in_sample_corrs = {}\n",
        "    mouse_fold_to_in_sample_scores = {}\n",
        "    \n",
        "    for z in range(4):\n",
        "        in_sample_corrs = {}\n",
        "        in_sample_scores = {}\n",
        "        for g in common_mouse:\n",
        "            t = np.array(ground_truth_mouse[z][::,g].X.squeeze())\n",
        "            p = np.array(pred_mouse[z][::,g].X.squeeze())\n",
        "            in_sample_corrs[g] = np.corrcoef(t, p)[0, 1]\n",
        "            in_sample_scores[g] = p.std()\n",
        "        mouse_fold_to_in_sample_corrs[z] = in_sample_corrs\n",
        "        mouse_fold_to_in_sample_scores[z] = in_sample_scores\n",
        "    \n",
        "    # Calculate weighted averages\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_mouse[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_mouse[z].shape[0] / total_cells\n",
        "    \n",
        "    mouse_avg_in_sample_corrs = {}\n",
        "    mouse_avg_in_sample_scores = {}\n",
        "    for g in common_mouse:\n",
        "        mouse_avg_in_sample_corrs[g] = sum(mouse_fold_to_in_sample_corrs[z][g] * fold_to_prop[z] for z in range(4))\n",
        "        mouse_avg_in_sample_scores[g] = sum(mouse_fold_to_in_sample_scores[z][g] * fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    return mouse_avg_in_sample_corrs, mouse_avg_in_sample_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spatial visualization functions\n",
        "def do_norm(x):\n",
        "    \"\"\"Normalize array to [0,1] range.\"\"\"\n",
        "    return (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "def do_norm2(x, min_val, max_val):\n",
        "    \"\"\"Normalize array to [0,1] range using provided min/max values.\"\"\"\n",
        "    return (x - min_val) / (max_val - min_val)\n",
        "\n",
        "def plot_spatial_gene_expression(pred_data, true_data, genes, sample_keys, output_dir='final_figures_schaf_revision_pngs'):\n",
        "    \"\"\"Generate spatial plots for specific genes with custom colormaps.\"\"\"\n",
        "    g_to_cmaps = {\n",
        "        'krt19': mpl.cm.Greens,  # MBC marker\n",
        "        'col1a2': mpl.cm.YlOrBr,  # Fibroblast marker\n",
        "        'apoc1': mpl.cm.Blues,   # Macrophage marker\n",
        "        'pecam1': mpl.cm.Reds    # Endothelial/vascular marker\n",
        "    }\n",
        "    \n",
        "    for k in sample_keys:\n",
        "        fig, axs = plt.subplots(2, len(genes), \n",
        "                               figsize=(5 * len(genes), 10))\n",
        "        \n",
        "        for i, g in enumerate(genes):\n",
        "            this_cmap = g_to_cmaps[g]\n",
        "            ax_pred = axs[1, i]\n",
        "            ax_true = axs[0, i]\n",
        "            \n",
        "            # Set up axes\n",
        "            ax_pred.set_aspect('auto')\n",
        "            ax_true.set_aspect('auto')\n",
        "            if i == 0:\n",
        "                ax_pred.set_ylabel(\"SCHAF\\nInferred\", size='x-large')\n",
        "                ax_true.set_ylabel(\"Ground Truth\", size='x-large')\n",
        "            ax_pred.set_xticks([])\n",
        "            ax_pred.set_yticks([])\n",
        "            ax_true.set_xticks([])\n",
        "            ax_true.set_yticks([])\n",
        "            \n",
        "            # Get gene expression data\n",
        "            g_ind = list(true_data[k].var.index).index(g)\n",
        "            t = np.array(true_data[k][::, g].X).squeeze()\n",
        "            p = np.array(pred_data[k][::, g].X).squeeze()\n",
        "            \n",
        "            # Normalize data\n",
        "            t = do_norm(t)\n",
        "            p = do_norm(p)\n",
        "            the_min = min(t.min(), p.min())\n",
        "            the_max = max(t.max(), p.max())\n",
        "            p = do_norm2(p, the_min, the_max)\n",
        "            t = do_norm2(t, the_min, the_max)\n",
        "            \n",
        "            # Calculate correlation\n",
        "            corr = np.corrcoef(p, t)[0, 1]\n",
        "            ax_true.set_title(f'{g.upper()}\\nSpatial Corr. = {corr:.3f}')\n",
        "            \n",
        "            # Plot data\n",
        "            ax_true.scatter(\n",
        "                true_data[k].obs['y_spot'] // 50,\n",
        "                true_data[k].obs['x_spot'] // 50,\n",
        "                c=t,\n",
        "                s=60,\n",
        "                vmin=0,\n",
        "                vmax=1,\n",
        "                cmap=this_cmap\n",
        "            )\n",
        "            \n",
        "            ax_pred.scatter(\n",
        "                pred_data[k].obs['y_spot'] // 50,\n",
        "                pred_data[k].obs['x_spot'] // 50,\n",
        "                c=p,\n",
        "                s=60,\n",
        "                vmin=0,\n",
        "                vmax=1,\n",
        "                cmap=this_cmap\n",
        "            )\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{output_dir}/spatial_gene_expression_{k}.png', \n",
        "                   dpi=400, transparent=True)\n",
        "        plt.close()\n",
        "\n",
        "def plot_spatial_correlation_stats(k_to_corrs, output_dir='final_figures_schaf_revision_pngs'):\n",
        "    \"\"\"Generate bar plots for spatial correlation statistics.\"\"\"\n",
        "    def plot_bar_from_dict(data_dict, title, y_label, filename):\n",
        "        names = sorted(data_dict.keys(), key=lambda k: data_dict[k])\n",
        "        values = [data_dict[k] for k in names]\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(7, 5))\n",
        "        plt.bar(names, values)\n",
        "        plt.ylabel(y_label)\n",
        "        plt.xlabel(\"Sample\")\n",
        "        plt.title(title, loc='left')\n",
        "        plt.xticks(rotation=90)\n",
        "        \n",
        "        if y_label == 'Average Spatial Correlation':\n",
        "            plt.yticks(ticks=np.arange(0, .41, .1), size='x-large',\n",
        "                      labels=np.arange(0, .41, .1).round(2))\n",
        "        else:\n",
        "            plt.yticks(ticks=np.arange(0, 36, 5), size='x-large',\n",
        "                      labels=np.arange(0, 36, 5).round(2))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.setp(ax.spines.values(), linewidth=2)\n",
        "        plt.savefig(f'{output_dir}/{filename}.png', dpi=400, transparent=True)\n",
        "        plt.savefig(f'{output_dir}/{filename}.pdf', transparent=True)\n",
        "        plt.close()\n",
        "    \n",
        "    # Plot average correlations\n",
        "    k_to_avg_corr = {k: np.mean(list(v.values())) for k, v in k_to_corrs.items()}\n",
        "    plot_bar_from_dict(k_to_avg_corr, 'Average Spatial Correlation by Sample',\n",
        "                      'Average Spatial Correlation', 'spatial_corrs_avg')\n",
        "    \n",
        "    # Plot number of well-correlated genes\n",
        "    k_to_num_good_genes = {k: np.sum(np.array(list(v.values()))>=.4) \n",
        "                          for k, v in k_to_corrs.items()}\n",
        "    plot_bar_from_dict(k_to_num_good_genes, 'Well-Correlated Genes by Sample',\n",
        "                      'Number of Genes\\nwith Corr. > .4', 'spatial_corrs_count')\n",
        "    \n",
        "    # Plot percentage of well-correlated genes\n",
        "    k_to_percent_good_genes = {k: np.sum(np.array(list(v.values()))>=.4) / float(len(v))\n",
        "                              for k, v in k_to_corrs.items()}\n",
        "    plot_bar_from_dict(k_to_percent_good_genes, 'Percentage of Well-Correlated Genes',\n",
        "                      'Percentage of Genes\\nwith Corr. > .4', 'spatial_corrs_percent')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell type analysis functions\n",
        "def get_hetero(cells):\n",
        "    \"\"\"Calculate heterogeneity scores for each gene.\"\"\"\n",
        "    res = {}\n",
        "    for g in cells.var.index:\n",
        "        to_see = np.array(cells[::,g].X).squeeze()\n",
        "        to_see = (to_see - to_see.max()) / (to_see.max() - to_see.min())\n",
        "        res[g] = to_see.std()\n",
        "    return res\n",
        "\n",
        "def load_cell_type_labels():\n",
        "    \"\"\"Load cell type labels and clusters.\"\"\"\n",
        "    # Load broad clusters\n",
        "    broad_clusters = pd.read_csv('/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/analysis/clustering/gene_expression_kmeans_10_clusters/clusters.csv')\n",
        "    broad_clusters = broad_clusters.set_index('Barcode')\n",
        "    \n",
        "    # Load fold-specific labels\n",
        "    xenium_in_sample_fold_to_inferred_labels = {}\n",
        "    mouse_fold_to_inferred_labels = {}\n",
        "    for f in range(4):\n",
        "        xenium_in_sample_fold_to_inferred_labels[f] = np.load(f'cancer_fold_to_new_labels/{f}.npy')\n",
        "        mouse_fold_to_inferred_labels[f] = np.load(f'mouse_fold_to_new_labels/{f}.npy')\n",
        "    \n",
        "    return broad_clusters, xenium_in_sample_fold_to_inferred_labels, mouse_fold_to_inferred_labels\n",
        "\n",
        "def calculate_cell_type_means(pred_data, true_data, cell_type_labels, common_genes):\n",
        "    \"\"\"Calculate mean expression for each cell type.\"\"\"\n",
        "    cts = list(range(1, 11))\n",
        "    \n",
        "    # Initialize dictionaries\n",
        "    ct_to_pred_mean = {}\n",
        "    ct_to_true_mean = {}\n",
        "    \n",
        "    # Calculate means for each cell type\n",
        "    for ct in cts:\n",
        "        ct_mask_pred = cell_type_labels == ct\n",
        "        ct_mask_true = cell_type_labels == ct\n",
        "        \n",
        "        if np.sum(ct_mask_pred) > 0:\n",
        "            ct_to_pred_mean[ct] = pred_data[ct_mask_pred, common_genes].X.mean(axis=0)\n",
        "        if np.sum(ct_mask_true) > 0:\n",
        "            ct_to_true_mean[ct] = true_data[ct_mask_true, common_genes].X.mean(axis=0)\n",
        "    \n",
        "    return ct_to_pred_mean, ct_to_true_mean\n",
        "\n",
        "def calculate_global_stats(ground_truth_data, common_genes):\n",
        "    \"\"\"Calculate global mean and standard deviation across all cells.\"\"\"\n",
        "    num_genes = len(common_genes)\n",
        "    all_means = np.zeros(num_genes)\n",
        "    total_samples = 0\n",
        "    \n",
        "    # Calculate weighted means\n",
        "    for k, v in ground_truth_data.items():\n",
        "        all_means = all_means + v.shape[0] * (ground_truth_data[k][::,common_genes].X.mean(axis=0))\n",
        "        total_samples += v.shape[0]\n",
        "    all_means = all_means / total_samples\n",
        "    \n",
        "    # Calculate weighted variances\n",
        "    all_vars = np.zeros(num_genes)\n",
        "    for k, v in ground_truth_data.items():\n",
        "        n = v.shape[0]\n",
        "        curr_means = ground_truth_data[k][::,common_genes].X.mean(axis=0)\n",
        "        all_vars += n * (ground_truth_data[k][::,common_genes].X.var(axis=0) + \n",
        "                        (curr_means - all_means)**2)\n",
        "    all_vars = all_vars / total_samples\n",
        "    \n",
        "    all_stds = np.sqrt(all_vars)\n",
        "    return all_means, all_stds\n",
        "\n",
        "def load_merfish_data():\n",
        "    \"\"\"Load and preprocess MERFISH data.\"\"\"\n",
        "    # Load raw data\n",
        "    g1_mouse_fish_measure = pd.read_csv('g1_new_cell_gene_matrix_with_centroids.csv')\n",
        "    g2_mouse_fish_measure = pd.read_csv('g2_new_cell_gene_matrix_with_centroids.csv')\n",
        "    \n",
        "    # Process data\n",
        "    for df in [g1_mouse_fish_measure, g2_mouse_fish_measure]:\n",
        "        df.set_index('cell', inplace=True)\n",
        "    \n",
        "    # Extract coordinates\n",
        "    coords = {\n",
        "        'g1': {'x': g1_mouse_fish_measure['centroid_x'],\n",
        "               'y': g1_mouse_fish_measure['centroid_y']},\n",
        "        'g2': {'x': g2_mouse_fish_measure['centroid_x'],\n",
        "               'y': g2_mouse_fish_measure['centroid_y']}\n",
        "    }\n",
        "    \n",
        "    # Remove coordinate columns and create AnnData objects\n",
        "    for df in [g1_mouse_fish_measure, g2_mouse_fish_measure]:\n",
        "        df.drop(labels=['Tile', 'centroid_x', 'centroid_y'], axis=1, inplace=True)\n",
        "    \n",
        "    g1_adata = sc.AnnData(g1_mouse_fish_measure)\n",
        "    g2_adata = sc.AnnData(g2_mouse_fish_measure)\n",
        "    \n",
        "    # Convert gene names to lowercase and log transform\n",
        "    for adata in [g1_adata, g2_adata]:\n",
        "        adata.var.index = [q.lower() for q in adata.var.index]\n",
        "        sc.pp.log1p(adata)\n",
        "    \n",
        "    return g1_adata, g2_adata, coords\n",
        "\n",
        "def load_out_of_sample_mouse_data():\n",
        "    \"\"\"Load out-of-sample mouse prediction data.\"\"\"\n",
        "    # Load predictions\n",
        "    g1_pred = sc.read_h5ad('/mounts/stultzlab03_storage2/ccomiter/out_of_sample_mouse_infer_g1_best_res_fold2.h5ad')\n",
        "    g2_pred = sc.read_h5ad('/mounts/stultzlab03_storage2/ccomiter/out_of_sample_mouse_infer_g2_final_022525.h5ad')\n",
        "    \n",
        "    # Calculate scores\n",
        "    g1_scores = {g: np.array(g1_pred[::,g].X).squeeze().std() \n",
        "                 for g in g1_pred.var.index}\n",
        "    g2_scores = {g: np.array(g2_pred[::,g].X).squeeze().std() \n",
        "                 for g in g2_pred.var.index}\n",
        "    \n",
        "    return g1_pred, g2_pred, g1_scores, g2_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell type visualization functions\n",
        "def plot_cell_type_expression_heatmap(ct_to_pred_mean, ct_to_true_mean, title, filename):\n",
        "    \"\"\"Generate heatmap of cell type-specific expression patterns.\"\"\"\n",
        "    # Prepare data\n",
        "    cts = sorted(ct_to_pred_mean.keys())\n",
        "    genes = ct_to_pred_mean[cts[0]].shape[0]\n",
        "    \n",
        "    pred_matrix = np.zeros((len(cts), genes))\n",
        "    true_matrix = np.zeros((len(cts), genes))\n",
        "    \n",
        "    for i, ct in enumerate(cts):\n",
        "        pred_matrix[i] = ct_to_pred_mean[ct]\n",
        "        true_matrix[i] = ct_to_true_mean[ct]\n",
        "    \n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot heatmaps\n",
        "    sns.heatmap(true_matrix, ax=ax1, cmap='coolwarm', center=0,\n",
        "                xticklabels=False, yticklabels=cts)\n",
        "    ax1.set_title('Ground Truth')\n",
        "    ax1.set_ylabel('Cell Type')\n",
        "    \n",
        "    sns.heatmap(pred_matrix, ax=ax2, cmap='coolwarm', center=0,\n",
        "                xticklabels=False, yticklabels=False)\n",
        "    ax2.set_title('Predicted')\n",
        "    \n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png', \n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_cell_type_heterogeneity(hetero_scores, cell_types, title, filename):\n",
        "    \"\"\"Generate violin plots of cell type heterogeneity.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Prepare data\n",
        "    data = []\n",
        "    labels = []\n",
        "    for ct in sorted(cell_types):\n",
        "        if ct in hetero_scores:\n",
        "            data.append(list(hetero_scores[ct].values()))\n",
        "            labels.extend([f'CT{ct}'] * len(hetero_scores[ct]))\n",
        "    \n",
        "    # Create violin plot\n",
        "    sns.violinplot(data=data)\n",
        "    plt.xticks(range(len(cell_types)), [f'CT{ct}' for ct in sorted(cell_types)],\n",
        "               rotation=45)\n",
        "    plt.ylabel('Heterogeneity Score')\n",
        "    plt.title(title)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_cell_type_spatial(pred_data, true_data, cell_type_labels, \n",
        "                          coords, title, filename):\n",
        "    \"\"\"Generate spatial plots colored by cell type.\"\"\"\n",
        "    # Set up colors for cell types\n",
        "    n_types = len(np.unique(cell_type_labels))\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, n_types))\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Plot ground truth\n",
        "    scatter1 = ax1.scatter(coords['x'], coords['y'],\n",
        "                          c=cell_type_labels, cmap='tab20',\n",
        "                          s=10, alpha=0.7)\n",
        "    ax1.set_title('Ground Truth')\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "    \n",
        "    # Plot predictions\n",
        "    scatter2 = ax2.scatter(coords['x'], coords['y'],\n",
        "                          c=cell_type_labels, cmap='tab20',\n",
        "                          s=10, alpha=0.7)\n",
        "    ax2.set_title('Predicted')\n",
        "    ax2.set_xticks([])\n",
        "    ax2.set_yticks([])\n",
        "    \n",
        "    # Add colorbar\n",
        "    plt.colorbar(scatter1, ax=ax1, label='Cell Type')\n",
        "    plt.colorbar(scatter2, ax=ax2, label='Cell Type')\n",
        "    \n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_cell_type_correlations(pred_data, true_data, cell_type_labels,\n",
        "                               common_genes, title, filename):\n",
        "    \"\"\"Generate correlation plots for each cell type.\"\"\"\n",
        "    cts = sorted(np.unique(cell_type_labels))\n",
        "    n_cols = min(4, len(cts))\n",
        "    n_rows = (len(cts) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, \n",
        "                            figsize=(5*n_cols, 5*n_rows))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, ct in enumerate(cts):\n",
        "        mask = cell_type_labels == ct\n",
        "        if np.sum(mask) > 0:\n",
        "            x = true_data[mask, common_genes].X.mean(axis=0)\n",
        "            y = pred_data[mask, common_genes].X.mean(axis=0)\n",
        "            \n",
        "            axes[i].scatter(x, y, alpha=0.5, s=10)\n",
        "            axes[i].set_title(f'Cell Type {ct}')\n",
        "            \n",
        "            # Add correlation coefficient\n",
        "            corr = np.corrcoef(x, y)[0, 1]\n",
        "            axes[i].text(0.05, 0.95, f'r = {corr:.3f}',\n",
        "                        transform=axes[i].transAxes)\n",
        "            \n",
        "            # Add diagonal line\n",
        "            lims = [\n",
        "                np.min([axes[i].get_xlim(), axes[i].get_ylim()]),\n",
        "                np.max([axes[i].get_xlim(), axes[i].get_ylim()]),\n",
        "            ]\n",
        "            axes[i].plot(lims, lims, 'k--', alpha=0.5)\n",
        "    \n",
        "    # Remove empty subplots\n",
        "    for i in range(len(cts), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "    \n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Analysis Pipeline\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main analysis pipeline for processing and visualizing SCHAF results.\n",
        "    \"\"\"\n",
        "    logging.info('Starting main analysis pipeline')\n",
        "    \n",
        "    try:\n",
        "        # Process each dataset\n",
        "        datasets = [\n",
        "            ('HTAPP', HTAPP_SAMPLES, HTAPP_GENES),\n",
        "            ('Placenta', PLACENTA_SAMPLES, PLACENTA_GENES),\n",
        "            ('MSKCC', LUNG_CANCER_SAMPLES, LUNG_CANCER_GENES)\n",
        "        ]\n",
        "        \n",
        "        for dataset_name, samples, gene_map in datasets:\n",
        "            logging.info(f'Processing {dataset_name} dataset')\n",
        "            \n",
        "            # Initialize containers for dataset-level metrics\n",
        "            all_emd_values = []\n",
        "            all_correlations = []\n",
        "            all_heterogeneity = []\n",
        "            fold_heatmaps = {}\n",
        "            fold_weights = {}\n",
        "            \n",
        "            # Process each sample\n",
        "            for sample_id in tqdm(samples, desc=f'Processing {dataset_name} samples'):\n",
        "                # Load and preprocess data\n",
        "                data = load_data(XENIUM_DIR, sample_id)\n",
        "                processed_data = preprocess_data(data)\n",
        "                \n",
        "                # Calculate cell type means\n",
        "                true_means = calculate_cell_type_means(\n",
        "                    processed_data['ground_truth_norm'],\n",
        "                    processed_data['cell_types']\n",
        "                )\n",
        "                pred_means = calculate_cell_type_means(\n",
        "                    processed_data['predictions_norm'],\n",
        "                    processed_data['cell_types']\n",
        "                )\n",
        "                \n",
        "                # Calculate correlations\n",
        "                corr_matrix = calculate_pairwise_correlations(\n",
        "                    pred_means, true_means,\n",
        "                    list(range(1, 11))\n",
        "                )\n",
        "                \n",
        "                # Plot correlation matrix\n",
        "                plot_correlation_matrix(\n",
        "                    corr_matrix,\n",
        "                    f'{dataset_name} - Sample {sample_id}',\n",
        "                    f'correlations_{dataset_name.lower()}_{sample_id}'\n",
        "                )\n",
        "                \n",
        "                # Calculate and plot correlation heatmaps\n",
        "                pred_heatmap, true_heatmap, cluster_labels = calculate_correlation_heatmap(\n",
        "                    processed_data['predictions_norm'],\n",
        "                    processed_data['ground_truth_norm']\n",
        "                )\n",
        "                fold_heatmaps[sample_id] = (pred_heatmap, true_heatmap)\n",
        "                fold_weights[sample_id] = processed_data['ground_truth_norm'].shape[0]\n",
        "                \n",
        "                fig = plot_correlation_heatmaps(\n",
        "                    pred_heatmap, true_heatmap,\n",
        "                    sample_id, dataset_name\n",
        "                )\n",
        "                safe_save_figure(\n",
        "                    fig,\n",
        "                    f'final_figures_schaf_revision_pngs/heatmaps_{dataset_name.lower()}_{sample_id}.png'\n",
        "                )\n",
        "                \n",
        "                # Calculate EMD for each gene\n",
        "                gene_name = gene_map[sample_id]\n",
        "                true_hist = processed_data['ground_truth_norm'][:, 0]  # First gene\n",
        "                pred_hist = processed_data['predictions_norm'][:, 0]\n",
        "                \n",
        "                bins = np.linspace(0, max(np.max(true_hist), np.max(pred_hist)), 50)\n",
        "                emd = calculate_emd(true_hist, pred_hist, bins)\n",
        "                all_emd_values.append(emd)\n",
        "                \n",
        "                # Plot expression distributions\n",
        "                fig = compare_expression_distributions(\n",
        "                    true_hist, pred_hist,\n",
        "                    gene_name, sample_id,\n",
        "                    dataset_name,\n",
        "                    round(np.mean(corr_matrix.diagonal()), 3),\n",
        "                    round(emd, 3)\n",
        "                )\n",
        "                safe_save_figure(\n",
        "                    fig,\n",
        "                    f'final_figures_schaf_revision_pngs/distributions_{dataset_name.lower()}_{sample_id}.png'\n",
        "                )\n",
        "                \n",
        "                # Plot spatial expression for marker genes\n",
        "                marker_genes = list(gene_map.values())\n",
        "                fig = plot_marker_genes_panel(\n",
        "                    processed_data['ground_truth_norm'],\n",
        "                    processed_data['predictions_norm'],\n",
        "                    marker_genes,\n",
        "                    sample_id,\n",
        "                    dataset_name,\n",
        "                    MARKER_GENE_CMAPS\n",
        "                )\n",
        "                safe_save_figure(\n",
        "                    fig,\n",
        "                    f'final_figures_schaf_revision_pngs/marker_genes_{dataset_name.lower()}_{sample_id}.png'\n",
        "                )\n",
        "                \n",
        "                # Analyze gene correlations\n",
        "                true_corr, pred_corr = analyze_gene_correlations(\n",
        "                    processed_data['ground_truth_norm'],\n",
        "                    processed_data['predictions_norm'],\n",
        "                    marker_genes\n",
        "                )\n",
        "                \n",
        "                fig = plot_gene_correlation_comparison(\n",
        "                    true_corr, pred_corr,\n",
        "                    marker_genes,\n",
        "                    f'{dataset_name} - Sample {sample_id}'\n",
        "                )\n",
        "                safe_save_figure(\n",
        "                    fig,\n",
        "                    f'final_figures_schaf_revision_pngs/gene_correlations_{dataset_name.lower()}_{sample_id}.png'\n",
        "                )\n",
        "                \n",
        "                # Analyze gene programs\n",
        "                # Example program: all marker genes for this dataset\n",
        "                fig, program_corr = analyze_gene_program(\n",
        "                    processed_data['ground_truth_norm'],\n",
        "                    processed_data['predictions_norm'],\n",
        "                    marker_genes,\n",
        "                    'Marker Genes Program',\n",
        "                    f'{dataset_name} - Sample {sample_id}'\n",
        "                )\n",
        "                safe_save_figure(\n",
        "                    fig,\n",
        "                    f'final_figures_schaf_revision_pngs/gene_program_{dataset_name.lower()}_{sample_id}.png'\n",
        "                )\n",
        "                \n",
        "                # Calculate heterogeneity\n",
        "                true_het = calculate_heterogeneity(\n",
        "                    processed_data['ground_truth_norm'],\n",
        "                    processed_data['cell_types']\n",
        "                )\n",
        "                all_heterogeneity.extend(list(true_het.values()))\n",
        "            \n",
        "            # Calculate and plot weighted average heatmaps\n",
        "            weighted_pred, weighted_true = calculate_weighted_heatmaps(\n",
        "                fold_heatmaps, fold_weights\n",
        "            )\n",
        "            fig = plot_correlation_heatmaps(\n",
        "                weighted_pred, weighted_true,\n",
        "                'Weighted Average', dataset_name\n",
        "            )\n",
        "            safe_save_figure(\n",
        "                fig,\n",
        "                f'final_figures_schaf_revision_pngs/weighted_heatmaps_{dataset_name.lower()}.png'\n",
        "            )\n",
        "            \n",
        "            # Plot dataset-level metrics\n",
        "            plot_emd_distribution(\n",
        "                all_emd_values,\n",
        "                dataset_name,\n",
        "                f'emd_distribution_{dataset_name.lower()}'\n",
        "            )\n",
        "            \n",
        "            # Plot correlation vs prevalence\n",
        "            plot_correlation_vs_prevalence(\n",
        "                all_correlations,\n",
        "                [processed_data['type_frequencies'][ct] for ct in range(1, 11)],\n",
        "                range(10),\n",
        "                dataset_name,\n",
        "                f'correlation_vs_prevalence_{dataset_name.lower()}'\n",
        "            )\n",
        "        \n",
        "        logging.info('Analysis pipeline completed successfully')\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f'Error in main analysis pipeline: {str(e)}')\n",
        "        raise\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spatial Visualization Functions\n",
        "\n",
        "@log_function_call\n",
        "def normalize_expression(expression_values, method='minmax'):\n",
        "    \"\"\"\n",
        "    Normalize gene expression values.\n",
        "    \n",
        "    Args:\n",
        "        expression_values: Array of expression values\n",
        "        method: Normalization method ('minmax' or 'standard')\n",
        "        \n",
        "    Returns:\n",
        "        numpy.ndarray: Normalized expression values\n",
        "    \"\"\"\n",
        "    if method == 'minmax':\n",
        "        min_val = np.min(expression_values)\n",
        "        max_val = np.max(expression_values)\n",
        "        if max_val == min_val:\n",
        "            return np.zeros_like(expression_values)\n",
        "        return (expression_values - min_val) / (max_val - min_val)\n",
        "    elif method == 'standard':\n",
        "        mean = np.mean(expression_values)\n",
        "        std = np.std(expression_values)\n",
        "        if std == 0:\n",
        "            return np.zeros_like(expression_values)\n",
        "        return (expression_values - mean) / std\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown normalization method: {method}\")\n",
        "\n",
        "@log_function_call\n",
        "def plot_spatial_expression(true_data, pred_data, gene_name, \n",
        "                          sample_id, dataset_name, \n",
        "                          cmap='viridis', size=20):\n",
        "    \"\"\"\n",
        "    Plot spatial expression patterns for true and predicted data.\n",
        "    \n",
        "    Args:\n",
        "        true_data: AnnData object with true expression\n",
        "        pred_data: AnnData object with predicted expression\n",
        "        gene_name: Name of gene to plot\n",
        "        sample_id: Sample identifier\n",
        "        dataset_name: Name of dataset\n",
        "        cmap: Colormap to use\n",
        "        size: Size of scatter points\n",
        "    \"\"\"\n",
        "    fig, (ax_true, ax_pred) = plt.subplots(1, 2, \n",
        "                                          figsize=(10, 5),\n",
        "                                          constrained_layout=True)\n",
        "    \n",
        "    # Get expression values\n",
        "    true_expr = np.array(true_data[:, gene_name].X).squeeze()\n",
        "    pred_expr = np.array(pred_data[:, gene_name].X).squeeze()\n",
        "    \n",
        "    # Normalize values together\n",
        "    min_val = min(true_expr.min(), pred_expr.min())\n",
        "    max_val = max(true_expr.max(), pred_expr.max())\n",
        "    true_expr_norm = (true_expr - min_val) / (max_val - min_val)\n",
        "    pred_expr_norm = (pred_expr - min_val) / (max_val - min_val)\n",
        "    \n",
        "    # Plot true expression\n",
        "    ax_true.scatter(true_data.obs['y'],\n",
        "                   true_data.obs['x'],\n",
        "                   c=true_expr_norm,\n",
        "                   s=size,\n",
        "                   vmin=0,\n",
        "                   vmax=1,\n",
        "                   cmap=cmap)\n",
        "    ax_true.invert_yaxis()\n",
        "    ax_true.set_title('Ground Truth', size='xx-large')\n",
        "    ax_true.set_xticks([])\n",
        "    ax_true.set_yticks([])\n",
        "    \n",
        "    # Plot predicted expression\n",
        "    ax_pred.scatter(pred_data.obs['y'],\n",
        "                   pred_data.obs['x'],\n",
        "                   c=pred_expr_norm,\n",
        "                   s=size,\n",
        "                   vmin=0,\n",
        "                   vmax=1,\n",
        "                   cmap=cmap)\n",
        "    ax_pred.invert_yaxis()\n",
        "    ax_pred.set_title('SCHAF Predicted', size='xx-large')\n",
        "    ax_pred.set_xticks([])\n",
        "    ax_pred.set_yticks([])\n",
        "    \n",
        "    # Add correlation value\n",
        "    corr = np.corrcoef(true_expr, pred_expr)[0, 1]\n",
        "    plt.suptitle(f'{gene_name.upper()} - {dataset_name} {sample_id}\\n' +\n",
        "                f'Correlation: {corr:.3f}',\n",
        "                size='xx-large')\n",
        "    \n",
        "    # Customize spines\n",
        "    for ax in [ax_true, ax_pred]:\n",
        "        ax.spines[['left', 'right', 'top', 'bottom']].set_visible(True)\n",
        "        plt.setp(ax.spines.values(), linewidth=2)\n",
        "    \n",
        "    return fig\n",
        "\n",
        "@log_function_call\n",
        "def plot_marker_genes_panel(true_data, pred_data, marker_genes,\n",
        "                          sample_id, dataset_name, cmaps=None):\n",
        "    \"\"\"\n",
        "    Create a panel of spatial plots for marker genes.\n",
        "    \n",
        "    Args:\n",
        "        true_data: AnnData object with true expression\n",
        "        pred_data: AnnData object with predicted expression\n",
        "        marker_genes: List of marker genes to plot\n",
        "        sample_id: Sample identifier\n",
        "        dataset_name: Name of dataset\n",
        "        cmaps: Dictionary mapping genes to colormaps\n",
        "    \"\"\"\n",
        "    n_genes = len(marker_genes)\n",
        "    fig, axs = plt.subplots(2, n_genes,\n",
        "                           figsize=(5 * n_genes, 10),\n",
        "                           constrained_layout=True)\n",
        "    \n",
        "    for i, gene in enumerate(marker_genes):\n",
        "        # Get colormap\n",
        "        cmap = cmaps.get(gene.lower(), 'viridis') if cmaps else 'viridis'\n",
        "        \n",
        "        # Get and normalize expression values\n",
        "        true_expr = np.array(true_data[:, gene].X).squeeze()\n",
        "        pred_expr = np.array(pred_data[:, gene].X).squeeze()\n",
        "        \n",
        "        true_expr_norm = normalize_expression(true_expr)\n",
        "        pred_expr_norm = normalize_expression(pred_expr)\n",
        "        \n",
        "        # Plot true expression\n",
        "        axs[0, i].scatter(true_data.obs['y'],\n",
        "                         true_data.obs['x'],\n",
        "                         c=true_expr_norm,\n",
        "                         s=20,\n",
        "                         vmin=0,\n",
        "                         vmax=1,\n",
        "                         cmap=cmap)\n",
        "        axs[0, i].invert_yaxis()\n",
        "        axs[0, i].set_title(gene.upper(), size='xx-large')\n",
        "        axs[0, i].set_xticks([])\n",
        "        axs[0, i].set_yticks([])\n",
        "        \n",
        "        if i == 0:\n",
        "            axs[0, i].set_ylabel('Ground Truth', size='xx-large')\n",
        "        \n",
        "        # Plot predicted expression\n",
        "        axs[1, i].scatter(pred_data.obs['y'],\n",
        "                         pred_data.obs['x'],\n",
        "                         c=pred_expr_norm,\n",
        "                         s=20,\n",
        "                         vmin=0,\n",
        "                         vmax=1,\n",
        "                         cmap=cmap)\n",
        "        axs[1, i].invert_yaxis()\n",
        "        axs[1, i].set_xticks([])\n",
        "        axs[1, i].set_yticks([])\n",
        "        \n",
        "        if i == 0:\n",
        "            axs[1, i].set_ylabel('SCHAF Predicted', size='xx-large')\n",
        "        \n",
        "        # Customize spines\n",
        "        for ax in [axs[0, i], axs[1, i]]:\n",
        "            ax.spines[['left', 'right', 'top', 'bottom']].set_visible(True)\n",
        "            plt.setp(ax.spines.values(), linewidth=2)\n",
        "    \n",
        "    plt.suptitle(f'{dataset_name} {sample_id} Marker Genes',\n",
        "                size='xx-large')\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Define standard colormaps for marker genes\n",
        "MARKER_GENE_CMAPS = {\n",
        "    'krt19': plt.cm.Greens,\n",
        "    'col1a2': plt.cm.YlOrBr,\n",
        "    'apoc1': plt.cm.Blues,\n",
        "    'pecam1': plt.cm.Reds,\n",
        "    'cd3d': plt.cm.Purples,\n",
        "    'cd8a': plt.cm.Oranges,\n",
        "    'cd4': plt.cm.RdPu,\n",
        "    'cd19': plt.cm.BuGn\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hierarchical Clustering and Heatmap Functions\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import seaborn as sns\n",
        "\n",
        "@log_function_call\n",
        "def calculate_correlation_heatmap(pred_data, true_data, \n",
        "                                cluster=True, n_clusters=2):\n",
        "    \"\"\"\n",
        "    Calculate correlation heatmap with optional clustering.\n",
        "    \n",
        "    Args:\n",
        "        pred_data: Predicted expression matrix\n",
        "        true_data: True expression matrix\n",
        "        cluster: Whether to perform hierarchical clustering\n",
        "        n_clusters: Number of clusters for hierarchical clustering\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (pred_heatmap, true_heatmap, cluster_labels)\n",
        "    \"\"\"\n",
        "    # Calculate correlation matrices\n",
        "    pred_heatmap = np.corrcoef(pred_data, rowvar=False)\n",
        "    true_heatmap = np.corrcoef(true_data, rowvar=False)\n",
        "    \n",
        "    if cluster:\n",
        "        # Perform hierarchical clustering\n",
        "        clustering = AgglomerativeClustering(\n",
        "            n_clusters=n_clusters,\n",
        "            affinity='euclidean',\n",
        "            linkage='ward'\n",
        "        )\n",
        "        labels = clustering.fit_predict(true_data.T)\n",
        "        \n",
        "        # Reorder matrices based on clustering\n",
        "        cluster_indices = []\n",
        "        for i in range(n_clusters):\n",
        "            cluster_indices.extend(\n",
        "                [j for j, l in enumerate(labels) if l == i]\n",
        "            )\n",
        "        \n",
        "        pred_heatmap = pred_heatmap[cluster_indices][:, cluster_indices]\n",
        "        true_heatmap = true_heatmap[cluster_indices][:, cluster_indices]\n",
        "        \n",
        "        return pred_heatmap, true_heatmap, labels\n",
        "    \n",
        "    return pred_heatmap, true_heatmap, None\n",
        "\n",
        "@log_function_call\n",
        "def plot_correlation_heatmaps(pred_heatmap, true_heatmap,\n",
        "                            sample_id, dataset_name):\n",
        "    \"\"\"\n",
        "    Plot correlation heatmaps for predicted and true data.\n",
        "    \n",
        "    Args:\n",
        "        pred_heatmap: Correlation matrix for predicted data\n",
        "        true_heatmap: Correlation matrix for true data\n",
        "        sample_id: Sample identifier\n",
        "        dataset_name: Name of dataset\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "    \n",
        "    # Plot true correlation heatmap\n",
        "    sns.heatmap(true_heatmap,\n",
        "                ax=ax1,\n",
        "                cmap='seismic',\n",
        "                center=0,\n",
        "                square=True,\n",
        "                cbar_kws={'label': 'Correlation'})\n",
        "    ax1.set_title('Ground Truth Correlations', size='xx-large')\n",
        "    \n",
        "    # Plot predicted correlation heatmap\n",
        "    sns.heatmap(pred_heatmap,\n",
        "                ax=ax2,\n",
        "                cmap='seismic',\n",
        "                center=0,\n",
        "                square=True,\n",
        "                cbar_kws={'label': 'Correlation'})\n",
        "    ax2.set_title('SCHAF Predicted Correlations', size='xx-large')\n",
        "    \n",
        "    # Calculate metacorrelation\n",
        "    metacorr = np.corrcoef(pred_heatmap.reshape(-1),\n",
        "                          true_heatmap.reshape(-1))[0, 1]\n",
        "    \n",
        "    plt.suptitle(\n",
        "        f'{dataset_name} {sample_id}\\n' +\n",
        "        f'Meta-correlation: {metacorr:.3f}',\n",
        "        size='xx-large'\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "@log_function_call\n",
        "def calculate_weighted_heatmaps(fold_data, weights):\n",
        "    \"\"\"\n",
        "    Calculate weighted average heatmaps across folds.\n",
        "    \n",
        "    Args:\n",
        "        fold_data: Dictionary mapping folds to (pred_heatmap, true_heatmap)\n",
        "        weights: Dictionary mapping folds to weights\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (weighted_pred_heatmap, weighted_true_heatmap)\n",
        "    \"\"\"\n",
        "    total_weight = sum(weights.values())\n",
        "    \n",
        "    # Initialize weighted averages\n",
        "    sample_shape = next(iter(fold_data.values()))[0].shape\n",
        "    weighted_pred = np.zeros(sample_shape)\n",
        "    weighted_true = np.zeros(sample_shape)\n",
        "    \n",
        "    # Calculate weighted sums\n",
        "    for fold, (pred, true) in fold_data.items():\n",
        "        weight = weights[fold] / total_weight\n",
        "        weighted_pred += pred * weight\n",
        "        weighted_true += true * weight\n",
        "    \n",
        "    return weighted_pred, weighted_true\n",
        "\n",
        "@log_function_call\n",
        "def plot_gene_correlation_matrix(true_data, pred_data, \n",
        "                               genes, sample_id, dataset_name):\n",
        "    \"\"\"\n",
        "    Plot correlation matrix for specific genes.\n",
        "    \n",
        "    Args:\n",
        "        true_data: True expression data\n",
        "        pred_data: Predicted expression data\n",
        "        genes: List of genes to include\n",
        "        sample_id: Sample identifier\n",
        "        dataset_name: Name of dataset\n",
        "    \"\"\"\n",
        "    # Extract expression data for selected genes\n",
        "    true_expr = true_data[:, genes].X\n",
        "    pred_expr = pred_data[:, genes].X\n",
        "    \n",
        "    # Calculate correlations\n",
        "    correlations = np.zeros((len(genes), len(genes)))\n",
        "    for i, gene1 in enumerate(genes):\n",
        "        for j, gene2 in enumerate(genes):\n",
        "            true_corr = np.corrcoef(\n",
        "                true_data[:, gene1].X.squeeze(),\n",
        "                true_data[:, gene2].X.squeeze()\n",
        "            )[0, 1]\n",
        "            pred_corr = np.corrcoef(\n",
        "                pred_data[:, gene1].X.squeeze(),\n",
        "                pred_data[:, gene2].X.squeeze()\n",
        "            )[0, 1]\n",
        "            correlations[i, j] = np.abs(true_corr - pred_corr)\n",
        "    \n",
        "    # Plot heatmap\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    sns.heatmap(correlations,\n",
        "                xticklabels=genes,\n",
        "                yticklabels=genes,\n",
        "                cmap='YlOrRd',\n",
        "                center=0.5,\n",
        "                square=True,\n",
        "                ax=ax)\n",
        "    \n",
        "    plt.title(f'{dataset_name} {sample_id}\\nGene Correlation Differences',\n",
        "              size='xx-large')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    \n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gene Analysis Functions\n",
        "\n",
        "@log_function_call\n",
        "def analyze_gene_correlations(true_data, pred_data, genes):\n",
        "    \"\"\"\n",
        "    Analyze correlations between genes in true and predicted data.\n",
        "    \n",
        "    Args:\n",
        "        true_data: True expression data\n",
        "        pred_data: Predicted expression data\n",
        "        genes: List of genes to analyze\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (true_corr_matrix, pred_corr_matrix)\n",
        "    \"\"\"\n",
        "    n_genes = len(genes)\n",
        "    true_corr = np.zeros((n_genes, n_genes))\n",
        "    pred_corr = np.zeros((n_genes, n_genes))\n",
        "    \n",
        "    for i, gene1 in enumerate(genes):\n",
        "        for j, gene2 in enumerate(genes):\n",
        "            true_expr1 = np.array(true_data[:, gene1].X).squeeze()\n",
        "            true_expr2 = np.array(true_data[:, gene2].X).squeeze()\n",
        "            pred_expr1 = np.array(pred_data[:, gene1].X).squeeze()\n",
        "            pred_expr2 = np.array(pred_data[:, gene2].X).squeeze()\n",
        "            \n",
        "            true_corr[i, j] = np.corrcoef(true_expr1, true_expr2)[0, 1]\n",
        "            pred_corr[i, j] = np.corrcoef(pred_expr1, pred_expr2)[0, 1]\n",
        "    \n",
        "    return true_corr, pred_corr\n",
        "\n",
        "@log_function_call\n",
        "def plot_gene_correlation_comparison(true_corr, pred_corr, \n",
        "                                   genes, dataset_name):\n",
        "    \"\"\"\n",
        "    Plot comparison of gene correlations between true and predicted data.\n",
        "    \n",
        "    Args:\n",
        "        true_corr: True correlation matrix\n",
        "        pred_corr: Predicted correlation matrix\n",
        "        genes: List of gene names\n",
        "        dataset_name: Name of dataset\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "    \n",
        "    # Plot true correlations\n",
        "    sns.heatmap(true_corr, \n",
        "                xticklabels=genes,\n",
        "                yticklabels=genes,\n",
        "                cmap='RdBu_r',\n",
        "                center=0,\n",
        "                ax=ax1)\n",
        "    ax1.set_title('Ground Truth Gene Correlations')\n",
        "    \n",
        "    # Plot predicted correlations\n",
        "    sns.heatmap(pred_corr,\n",
        "                xticklabels=genes,\n",
        "                yticklabels=genes,\n",
        "                cmap='RdBu_r',\n",
        "                center=0,\n",
        "                ax=ax2)\n",
        "    ax2.set_title('Predicted Gene Correlations')\n",
        "    \n",
        "    # Calculate overall correlation\n",
        "    corr = np.corrcoef(true_corr.flatten(), \n",
        "                       pred_corr.flatten())[0, 1]\n",
        "    \n",
        "    plt.suptitle(f'{dataset_name}\\nGene Correlation Comparison\\n' +\n",
        "                 f'Overall Correlation: {corr:.3f}',\n",
        "                 size='xx-large')\n",
        "    \n",
        "    return fig\n",
        "\n",
        "@log_function_call\n",
        "def analyze_gene_program(true_data, pred_data, gene_set,\n",
        "                        program_name, dataset_name):\n",
        "    \"\"\"\n",
        "    Analyze a gene program (set of related genes).\n",
        "    \n",
        "    Args:\n",
        "        true_data: True expression data\n",
        "        pred_data: Predicted expression data\n",
        "        gene_set: List of genes in the program\n",
        "        program_name: Name of the gene program\n",
        "        dataset_name: Name of dataset\n",
        "    \"\"\"\n",
        "    # Calculate program scores\n",
        "    true_scores = np.mean([np.array(true_data[:, g].X).squeeze() \n",
        "                          for g in gene_set], axis=0)\n",
        "    pred_scores = np.mean([np.array(pred_data[:, g].X).squeeze() \n",
        "                          for g in gene_set], axis=0)\n",
        "    \n",
        "    # Calculate correlation\n",
        "    program_corr = np.corrcoef(true_scores, pred_scores)[0, 1]\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Scatter plot\n",
        "    ax1.scatter(true_scores, pred_scores, alpha=0.5)\n",
        "    ax1.set_xlabel('True Program Score')\n",
        "    ax1.set_ylabel('Predicted Program Score')\n",
        "    \n",
        "    # Add correlation line\n",
        "    min_val = min(true_scores.min(), pred_scores.min())\n",
        "    max_val = max(true_scores.max(), pred_scores.max())\n",
        "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "    \n",
        "    # Distribution plot\n",
        "    sns.kdeplot(data=true_scores, ax=ax2, label='Ground Truth')\n",
        "    sns.kdeplot(data=pred_scores, ax=ax2, label='Predicted')\n",
        "    ax2.set_xlabel('Program Score')\n",
        "    ax2.set_ylabel('Density')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.suptitle(f'{dataset_name} - {program_name}\\n' +\n",
        "                 f'Program Correlation: {program_corr:.3f}',\n",
        "                 size='xx-large')\n",
        "    \n",
        "    return fig, program_corr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced correlation analysis functions\n",
        "\n",
        "@log_function_call\n",
        "def calculate_pairwise_correlations(pred_means, true_means, cell_types):\n",
        "    \"\"\"\n",
        "    Calculate pairwise correlations between predicted and true expression\n",
        "    for all cell type combinations.\n",
        "    \n",
        "    Args:\n",
        "        pred_means: Dictionary mapping cell types to predicted expression\n",
        "        true_means: Dictionary mapping cell types to true expression\n",
        "        cell_types: List of cell type identifiers\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Correlation matrix (cell_types  cell_types)\n",
        "    \"\"\"\n",
        "    num_types = len(cell_types)\n",
        "    corr_matrix = np.zeros((num_types, num_types))\n",
        "    \n",
        "    for i, ct1 in enumerate(cell_types):\n",
        "        for j, ct2 in enumerate(cell_types):\n",
        "            if ct1 in true_means and ct2 in pred_means:\n",
        "                corr = np.corrcoef(true_means[ct1], \n",
        "                                 pred_means[ct2])[0, 1]\n",
        "                corr_matrix[i, j] = corr\n",
        "    \n",
        "    return np.nan_to_num(corr_matrix)\n",
        "\n",
        "@log_function_call\n",
        "def plot_correlation_matrix(corr_matrix, title, filename):\n",
        "    \"\"\"\n",
        "    Plot correlation matrix as a heatmap.\n",
        "    \n",
        "    Args:\n",
        "        corr_matrix: numpy.ndarray of correlations\n",
        "        title: Plot title\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    # Create heatmap\n",
        "    im = ax.imshow(corr_matrix, cmap='seismic', aspect='auto')\n",
        "    \n",
        "    # Add colorbar\n",
        "    plt.colorbar(im)\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_title(title, size='xx-large', loc='left')\n",
        "    ax.set_xlabel('Predicted Cell Type', size='xx-large')\n",
        "    ax.set_ylabel('True Cell Type', size='xx-large')\n",
        "    \n",
        "    # Add cell type labels\n",
        "    cell_types = range(1, corr_matrix.shape[0] + 1)\n",
        "    ax.set_xticks(range(len(cell_types)))\n",
        "    ax.set_yticks(range(len(cell_types)))\n",
        "    ax.set_xticklabels(cell_types, size='x-large')\n",
        "    ax.set_yticklabels(cell_types, size='x-large')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    safe_save_figure(fig, \n",
        "                    f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                    dpi=FIGURE_PARAMS['dpi'],\n",
        "                    transparent=FIGURE_PARAMS['transparent'])\n",
        "\n",
        "@log_function_call\n",
        "def plot_correlation_vs_prevalence(correlations, prevalences, \n",
        "                                 cell_types, title, filename):\n",
        "    \"\"\"\n",
        "    Plot correlation vs prevalence scatter plot.\n",
        "    \n",
        "    Args:\n",
        "        correlations: List of correlation values\n",
        "        prevalences: List of prevalence values\n",
        "        cell_types: List of cell type indices\n",
        "        title: Plot title\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    # Create scatter plot\n",
        "    scatter = ax.scatter(prevalences, correlations,\n",
        "                        c=cell_types, cmap='rainbow', s=100)\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_title(title, size='xx-large', loc='left')\n",
        "    ax.set_xlabel('Prevalence', size='xx-large')\n",
        "    ax.set_ylabel('Correlation', size='xx-large')\n",
        "    \n",
        "    # Set axis limits and ticks\n",
        "    ax.spines[['right', 'top']].set_visible(False)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_xlim(0, max(prevalences) * 1.1)\n",
        "    \n",
        "    # Add ticks\n",
        "    ax.set_yticks(np.arange(0, 1.01, 0.1))\n",
        "    ax.set_xticks(np.arange(0, max(prevalences) * 1.1, 0.05))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    safe_save_figure(fig,\n",
        "                    f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                    dpi=FIGURE_PARAMS['dpi'],\n",
        "                    transparent=FIGURE_PARAMS['transparent'])\n",
        "\n",
        "@log_function_call\n",
        "def calculate_weighted_correlations(pred_data, true_data, weights):\n",
        "    \"\"\"\n",
        "    Calculate weighted correlations across multiple datasets.\n",
        "    \n",
        "    Args:\n",
        "        pred_data: Dictionary mapping fold/dataset to predicted data\n",
        "        true_data: Dictionary mapping fold/dataset to true data\n",
        "        weights: Dictionary mapping fold/dataset to weights\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (correlations, weighted_average)\n",
        "    \"\"\"\n",
        "    correlations = {}\n",
        "    total_weight = sum(weights.values())\n",
        "    \n",
        "    for key in pred_data:\n",
        "        if key in true_data and key in weights:\n",
        "            corr = np.corrcoef(pred_data[key].flatten(),\n",
        "                             true_data[key].flatten())[0, 1]\n",
        "            correlations[key] = corr\n",
        "    \n",
        "    weighted_avg = sum(correlations[k] * weights[k] / total_weight \n",
        "                      for k in correlations)\n",
        "    \n",
        "    return correlations, weighted_avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Earth Mover's Distance (EMD) analysis functions\n",
        "\n",
        "@log_function_call\n",
        "def calculate_emd(hist1, hist2, bins):\n",
        "    \"\"\"\n",
        "    Calculate Earth Mover's Distance between two histograms.\n",
        "    \n",
        "    Args:\n",
        "        hist1: First histogram values\n",
        "        hist2: Second histogram values\n",
        "        bins: Bin edges for histograms\n",
        "        \n",
        "    Returns:\n",
        "        float: EMD value\n",
        "    \"\"\"\n",
        "    # Normalize histograms\n",
        "    hist1_norm = hist1 / np.sum(hist1)\n",
        "    hist2_norm = hist2 / np.sum(hist2)\n",
        "    \n",
        "    # Calculate cumulative distributions\n",
        "    cdf1 = np.cumsum(hist1_norm)\n",
        "    cdf2 = np.cumsum(hist2_norm)\n",
        "    \n",
        "    # Calculate EMD as area between CDFs\n",
        "    emd = np.sum(np.abs(cdf1 - cdf2)) * (bins[1] - bins[0])\n",
        "    return emd\n",
        "\n",
        "@log_function_call\n",
        "def plot_emd_distribution(emd_values, dataset_name, filename):\n",
        "    \"\"\"\n",
        "    Plot distribution of EMD values.\n",
        "    \n",
        "    Args:\n",
        "        emd_values: List of EMD values\n",
        "        dataset_name: Name of dataset for title\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    \n",
        "    # Calculate mean EMD\n",
        "    mean_emd = round(np.mean(emd_values), 3)\n",
        "    \n",
        "    # Create histogram\n",
        "    plt.hist(emd_values, \n",
        "            rwidth=0.7,\n",
        "            bins=np.arange(0, max(emd_values) + 0.05, 0.03))\n",
        "    \n",
        "    # Customize plot\n",
        "    plt.title(dataset_name, loc='left', size='xx-large')\n",
        "    plt.xlabel('Earth Mover\\'s Distance (EMD)', size='xx-large')\n",
        "    plt.ylabel('Number of Genes', size='xx-large')\n",
        "    \n",
        "    # Add mean line\n",
        "    plt.plot([1], [1], '--', c='white',\n",
        "             label=f'Avg. EMD = {mean_emd}')\n",
        "    plt.legend(fontsize='xx-large', loc='upper right')\n",
        "    \n",
        "    # Set ticks\n",
        "    max_count = plt.gca().get_ylim()[1]\n",
        "    plt.yticks(ticks=np.arange(0, max_count, 1000),\n",
        "              labels=np.arange(0, max_count, 1000).round(0),\n",
        "              size='x-large')\n",
        "    plt.xticks(ticks=np.arange(0, max(emd_values) + 0.05, 0.25),\n",
        "              size='x-large')\n",
        "    \n",
        "    # Customize spines\n",
        "    plt.setp(ax.spines.values(), linewidth=2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    safe_save_figure(fig,\n",
        "                    f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                    dpi=FIGURE_PARAMS['dpi'],\n",
        "                    transparent=FIGURE_PARAMS['transparent'])\n",
        "\n",
        "@log_function_call\n",
        "def compare_expression_distributions(true_hist, pred_hist, \n",
        "                                  gene_name, sample_id,\n",
        "                                  dataset_name, quality_score, emd_score):\n",
        "    \"\"\"\n",
        "    Compare and plot true vs predicted expression distributions.\n",
        "    \n",
        "    Args:\n",
        "        true_hist: Ground truth histogram values\n",
        "        pred_hist: Predicted histogram values\n",
        "        gene_name: Name of gene\n",
        "        sample_id: Sample identifier\n",
        "        dataset_name: Name of dataset\n",
        "        quality_score: Quality score for the prediction\n",
        "        emd_score: EMD score for the distributions\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    \n",
        "    # Calculate kernel density estimates\n",
        "    kde_true = gaussian_kde(true_hist)\n",
        "    kde_pred = gaussian_kde(pred_hist)\n",
        "    \n",
        "    # Generate x values for plotting\n",
        "    x = np.linspace(min(min(true_hist), min(pred_hist)),\n",
        "                   max(max(true_hist), max(pred_hist)),\n",
        "                   100)\n",
        "    \n",
        "    # Plot KDE curves\n",
        "    plt.plot(x, kde_true(x), linewidth=2, label='Ground Truth')\n",
        "    plt.plot(x, kde_pred(x), linewidth=2, label='Predicted')\n",
        "    \n",
        "    # Fill under curves\n",
        "    plt.fill_between(x, kde_true(x), alpha=0.5)\n",
        "    plt.fill_between(x, kde_pred(x), alpha=0.5)\n",
        "    \n",
        "    # Customize plot\n",
        "    plt.title(f'{gene_name.upper()} - {dataset_name} {sample_id}',\n",
        "              loc='left', size='xx-large')\n",
        "    plt.xlabel('Gene Expression', size='xx-large')\n",
        "    plt.ylabel('KDE-Density', size='xx-large')\n",
        "    \n",
        "    # Add scores\n",
        "    plt.plot([1], [1], '--', c='white',\n",
        "             label=f'Dist. Match Score={quality_score}\\nEMD={emd_score}')\n",
        "    plt.legend(fontsize='xx-large', loc='upper right')\n",
        "    \n",
        "    # Set axis limits and ticks\n",
        "    plt.xlim(-0.5, max(x))\n",
        "    plt.ylim(0, max(max(kde_true(x)), max(kde_pred(x))))\n",
        "    \n",
        "    # Customize spines\n",
        "    plt.setp(ax.spines.values(), linewidth=2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HTAPP visualization functions\n",
        "def plot_htapp_marker_genes(pred_htapp, ground_truth_htapp, genes, keys, \n",
        "                          colormaps, title, filename):\n",
        "    \"\"\"Generate marker gene expression plots for HTAPP data.\"\"\"\n",
        "    for k in keys:\n",
        "        fig, axs = plt.subplots(2, len(genes), \n",
        "                               figsize=(5*len(genes), 5*2))\n",
        "        \n",
        "        for i, g in enumerate(genes):\n",
        "            this_cmap = colormaps[g]\n",
        "            ax_pred = axs[1, i]\n",
        "            ax_true = axs[0, i]\n",
        "            \n",
        "            # Set aspect ratio and labels\n",
        "            ax_pred.set_aspect('auto')\n",
        "            ax_true.set_aspect('auto')\n",
        "            \n",
        "            if i == 0:\n",
        "                ax_pred.set_ylabel(\"SCHAF\\nInferred\", size='x-large')\n",
        "                ax_true.set_ylabel(\"ExpSCR\", size='x-large')\n",
        "            \n",
        "            ax_true.set_title(g.upper())\n",
        "            \n",
        "            # Get and normalize expression values\n",
        "            t = np.array(ground_truth_htapp[k][::, g].X).squeeze()\n",
        "            t = (t - t.min()) / (t.max() - t.min())\n",
        "            \n",
        "            p = np.array(pred_htapp[k][::, g].X).squeeze()\n",
        "            p = (p - p.min()) / (p.max() - p.min())\n",
        "            \n",
        "            # Plot ground truth\n",
        "            ax_true.scatter(\n",
        "                ground_truth_htapp[k].obs['y'],\n",
        "                ground_truth_htapp[k].obs['x'],\n",
        "                c=t, s=20, vmin=0, vmax=1,\n",
        "                cmap=this_cmap\n",
        "            )\n",
        "            ax_true.invert_yaxis()\n",
        "            ax_true.set_xticks([])\n",
        "            ax_true.set_yticks([])\n",
        "            \n",
        "            # Plot predictions\n",
        "            ax_pred.scatter(\n",
        "                pred_htapp[k].obs['y'],\n",
        "                pred_htapp[k].obs['x'],\n",
        "                c=p, s=20, vmin=0, vmax=1,\n",
        "                cmap=this_cmap\n",
        "            )\n",
        "            ax_pred.invert_yaxis()\n",
        "            ax_pred.set_xticks([])\n",
        "            ax_pred.set_yticks([])\n",
        "            \n",
        "            # Add borders\n",
        "            for ax in [ax_pred, ax_true]:\n",
        "                ax.spines[['left', 'right', 'top', 'bottom']].set_visible(1)\n",
        "                plt.setp(ax.spines.values(), linewidth=2)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'final_figures_schaf_revision_pngs/{filename}_{k}.png',\n",
        "                    dpi=400, transparent=True)\n",
        "        plt.close()\n",
        "\n",
        "def plot_htapp_schematic(histology_images, title, filename):\n",
        "    \"\"\"Generate schematic overview of HTAPP regions.\"\"\"\n",
        "    fig, ax = plt.subplots(3, 4, figsize=(9, 9),\n",
        "                          gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "    \n",
        "    for i, (k, v) in enumerate(histology_images.items()):\n",
        "        a = i // 4\n",
        "        b = i % 4\n",
        "        # Flip x and y axes for correct orientation\n",
        "        flipped_v = np.transpose(v, (1, 0, 2))\n",
        "        ax[a, b].imshow(flipped_v)\n",
        "        ax[a, b].axis('off')\n",
        "        ax[a, b].set_title(f'HTAPP {k}', pad=15, loc='center')\n",
        "    \n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def calculate_htapp_metrics(pred_htapp, ground_truth_htapp):\n",
        "    \"\"\"Calculate performance metrics for HTAPP predictions.\"\"\"\n",
        "    from scipy.stats import wasserstein_distance\n",
        "    \n",
        "    htapp_key_to_scores = {}\n",
        "    htapp_key_to_dists = {}\n",
        "    \n",
        "    for k in ground_truth_htapp:\n",
        "        num_genes = ground_truth_htapp[k].shape[1]\n",
        "        true_data = np.array(ground_truth_htapp[k].X)\n",
        "        pred_data = pred_htapp[k].X\n",
        "        \n",
        "        # Calculate distances and scores\n",
        "        dists = []\n",
        "        scores = []\n",
        "        for i in range(num_genes):\n",
        "            pred_norm = (pred_data[::,i] - pred_data[::,i].min()) / (pred_data[::,i].max() - pred_data[::,i].min())\n",
        "            dists.append(wasserstein_distance(pred_norm, true_data[::,i]))\n",
        "            scores.append(pred_norm.std())\n",
        "        \n",
        "        htapp_key_to_scores[k] = np.array(scores)\n",
        "        htapp_key_to_dists[k] = np.array(dists)\n",
        "    \n",
        "    return htapp_key_to_scores, htapp_key_to_dists\n",
        "\n",
        "def plot_htapp_metrics(htapp_key_to_scores, htapp_key_to_dists, \n",
        "                      title, filename):\n",
        "    \"\"\"Plot performance metrics for HTAPP predictions.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Plot score distributions\n",
        "    for k, scores in htapp_key_to_scores.items():\n",
        "        sns.kdeplot(scores, ax=ax1, label=f'HTAPP {k}')\n",
        "    ax1.set_title('Score Distributions')\n",
        "    ax1.set_xlabel('Score')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Plot distance distributions\n",
        "    for k, dists in htapp_key_to_dists.items():\n",
        "        sns.kdeplot(dists, ax=ax2, label=f'HTAPP {k}')\n",
        "    ax2.set_title('Distance Distributions')\n",
        "    ax2.set_xlabel('Wasserstein Distance')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell type classification metrics and visualization\n",
        "def get_freq(cts):\n",
        "    \"\"\"Calculate frequency of each cell type.\"\"\"\n",
        "    from collections import Counter\n",
        "    total = float(len(cts))\n",
        "    cts = Counter(cts)\n",
        "    cts = {k: v / total for k, v in cts.items()}\n",
        "    cts = np.array([cts[k] for k in range(1, 11, 1)])\n",
        "    return cts\n",
        "\n",
        "def get_perclass_acc(preds, trus):\n",
        "    \"\"\"Calculate per-class accuracy.\"\"\"\n",
        "    res = []\n",
        "    for k in range(1, 11, 1):\n",
        "        tps = preds[np.where(trus==k)]\n",
        "        tts = trus[np.where(trus==k)]\n",
        "        res.append((tps==tts).sum() / len(tts))\n",
        "    return np.array(res)\n",
        "\n",
        "# Cell type color definitions\n",
        "def get_cell_type_colors():\n",
        "    \"\"\"Get color mapping for cell types.\"\"\"\n",
        "    colors = {\n",
        "        'MBC': '#73d56d',\n",
        "        'MBC_stem-like': '#146c18',\n",
        "        'MBC_neuronal': '#39a13c',\n",
        "        'MBC_chondroid': '#003b00',\n",
        "        'Endothelial': '#fc0303',\n",
        "        'Endothelial_sinusoidal': '#6d0000',\n",
        "        'Endothelial_angiogenic': '#dc7014',\n",
        "        'Endothelial_vascular': '#aa3700',\n",
        "        'Endothelial vasc.': '#aa3700',\n",
        "        'Fibroblast': '#ced208',\n",
        "        'Chondrocyte': '#515900',\n",
        "        'Smooth muscle_vascular': '#748000',\n",
        "        'Smooth muscle vasc.': '#748000',\n",
        "        'Stellate': '#323400',\n",
        "        'Skeletal muscle': '#a0a800',\n",
        "        'Adipocytes': '#bb6fc4',\n",
        "        'Hepatocyte': '#f3a3f6',\n",
        "        'Keratinocyte': '#53065f',\n",
        "        'Neuron': '#873b92',\n",
        "        'Macrophage': '#99d1fe',\n",
        "        'Monocyte': '#387fb9',\n",
        "        'Neutrophil': '#003365',\n",
        "        'Erythrocyte': '#66a8dd',\n",
        "        'Mast': '#1a588e',\n",
        "        'B_plasma': '#f86652',\n",
        "        'B': '#cf1917',\n",
        "        'T': '#fbb2a1',\n",
        "        'NK': '#860000',\n",
        "        # Broad categories\n",
        "        'Tumor': '#73d56d',\n",
        "        'Normal': '#f3a3f6',\n",
        "        'Vascular': '#fc0303',\n",
        "        'Immune': '#99d1fe',\n",
        "        'Fibrosis': '#ced208'\n",
        "    }\n",
        "    return colors\n",
        "\n",
        "def make_color_ramp(colors):\n",
        "    \"\"\"Create a color ramp from a list of colors.\"\"\"\n",
        "    from colour import Color\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    color_ramp = LinearSegmentedColormap.from_list('custom_ramp', \n",
        "                                                  [Color(c).rgb for c in colors])\n",
        "    return color_ramp\n",
        "\n",
        "def plot_cell_type_accuracies(pred_labels, true_labels, dataset_names):\n",
        "    \"\"\"Plot cell type classification accuracies.\"\"\"\n",
        "    fig, axes = plt.subplots(1, len(dataset_names), \n",
        "                            figsize=(13.5, 4.3))\n",
        "    \n",
        "    for i, (preds, trues, name) in enumerate(zip(pred_labels, \n",
        "                                                true_labels, \n",
        "                                                dataset_names)):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracies = get_perclass_acc(preds, trues)\n",
        "        freqs = get_freq(trues)\n",
        "        weighted_acc = (accuracies * freqs).sum()\n",
        "        weighted_acc = round(weighted_acc, 3)\n",
        "        \n",
        "        # Create bar plot\n",
        "        cts = list(range(1, 11, 1))\n",
        "        ax.bar(cts, accuracies, 0.6)\n",
        "        \n",
        "        # Customize plot\n",
        "        ax.set_title(f'{name}\\nTotal Accuracy: {weighted_acc}', \n",
        "                    size='xx-large', loc='left')\n",
        "        ax.set_xlabel('Cell Type', size='xx-large')\n",
        "        if i == 0:\n",
        "            ax.set_ylabel('Accuracy', size='xx-large')\n",
        "        \n",
        "        ax.spines[['right', 'top']].set_visible(False)\n",
        "        ax.set_yticks(ticks=np.arange(0, 1.01, .1),\n",
        "                     labels=\"\" if i > 0 else np.round(np.arange(0, 1.01, .1), 2),\n",
        "                     size='x-large')\n",
        "        ax.set_xticks(ticks=np.arange(1, 10.01, 1),\n",
        "                     labels=range(1, 11, 1),\n",
        "                     size='x-large')\n",
        "        plt.setp(ax.spines.values(), linewidth=2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/celltype_accuracies.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_cell_type_spatial_distribution(pred_data, cell_type_labels, \n",
        "                                      sample_id, title):\n",
        "    \"\"\"Plot spatial distribution of cell types.\"\"\"\n",
        "    # Get colors\n",
        "    colors = get_cell_type_colors()\n",
        "    cell_types = np.unique(cell_type_labels)\n",
        "    color_list = [colors[ct] for ct in cell_types]\n",
        "    cmap = make_color_ramp(color_list)\n",
        "    \n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    \n",
        "    # Plot cells\n",
        "    scatter = ax.scatter(\n",
        "        pred_data.obs['y'],\n",
        "        pred_data.obs['x'],\n",
        "        c=cell_type_labels,\n",
        "        cmap=cmap,\n",
        "        s=5\n",
        "    )\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.spines[['left', 'right', 'top', 'bottom']].set_visible(True)\n",
        "    plt.setp(ax.spines.values(), linewidth=2)\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # Add legend\n",
        "    clean_label = lambda a: (' '.join(a.split('_'))).title().replace('Mbc', 'MBC').replace('Nk', 'NK')\n",
        "    patches = [mpatches.Patch(color=colors[ct], \n",
        "                            label=clean_label(ct)) \n",
        "              for ct in cell_types]\n",
        "    ax.legend(handles=patches,\n",
        "             bbox_to_anchor=(1.15, 1.),\n",
        "             loc='upper left',\n",
        "             fontsize='xx-large',\n",
        "             frameon=False)\n",
        "    \n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/spatial_distribution_{sample_id}.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def plot_pseudobulk_correlations(pred_means, true_means, cell_types, \n",
        "                               title, filename):\n",
        "    \"\"\"Plot pseudobulk correlations for each cell type.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    correlations = []\n",
        "    for ct in cell_types:\n",
        "        if ct in pred_means and ct in true_means:\n",
        "            corr = np.corrcoef(pred_means[ct], true_means[ct])[0, 1]\n",
        "            correlations.append(corr)\n",
        "    \n",
        "    # Create bar plot\n",
        "    ax.bar(range(1, len(correlations) + 1), correlations, 0.6)\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_xlabel('Cell Type', size='xx-large')\n",
        "    ax.set_ylabel('Pseudobulk Correlation', size='xx-large')\n",
        "    ax.spines[['right', 'top']].set_visible(False)\n",
        "    ax.set_xticks(range(1, len(correlations) + 1))\n",
        "    ax.set_xticklabels(cell_types, rotation=45, ha='right')\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{filename}.png',\n",
        "                dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main execution pipeline\n",
        "def main():\n",
        "    \"\"\"Execute the complete analysis pipeline.\"\"\"\n",
        "    print(\"Setting up output directory...\")\n",
        "    os.makedirs('final_figures_schaf_revision_pngs', exist_ok=True)\n",
        "    \n",
        "    print(\"\\nLoading data...\")\n",
        "    # Load core data\n",
        "    ground_truth_out_of_sample, pred_out_of_sample = load_out_of_sample_data()\n",
        "    ground_truth_in_sample, pred_in_sample = load_in_sample_data()\n",
        "    ground_truth_mouse, pred_mouse = load_mouse_data()\n",
        "    \n",
        "    # Load cell type labels\n",
        "    print(\"Loading cell type labels...\")\n",
        "    broad_clusters, xenium_in_sample_fold_to_inferred_labels, mouse_fold_to_inferred_labels = load_cell_type_labels()\n",
        "    \n",
        "    # Load MERFISH data\n",
        "    print(\"Loading MERFISH data...\")\n",
        "    g1_adata, g2_adata, merfish_coords = load_merfish_data()\n",
        "    \n",
        "    # Load out-of-sample mouse data\n",
        "    print(\"Loading out-of-sample mouse data...\")\n",
        "    g1_pred, g2_pred, g1_scores, g2_scores = load_out_of_sample_mouse_data()\n",
        "    \n",
        "    print(\"\\nCalculating metrics...\")\n",
        "    # Calculate program metrics\n",
        "    program_metrics = calculate_program_metrics(\n",
        "        pred_in_sample, ground_truth_in_sample,\n",
        "        pred_out_of_sample, ground_truth_out_of_sample,\n",
        "        common_in_sample, common_out_of_sample,\n",
        "        all_programs\n",
        "    )\n",
        "    \n",
        "    # Calculate cell type metrics\n",
        "    cell_type_metrics = calculate_cell_type_metrics(\n",
        "        pred_in_sample, ground_truth_in_sample,\n",
        "        pred_out_of_sample, ground_truth_out_of_sample,\n",
        "        pred_mouse, ground_truth_mouse,\n",
        "        xenium_in_sample_fold_to_inferred_labels,\n",
        "        mouse_fold_to_inferred_labels\n",
        "    )\n",
        "    \n",
        "    # Calculate HTAPP metrics\n",
        "    htapp_scores, htapp_dists = calculate_htapp_metrics(\n",
        "        pred_htapp, ground_truth_htapp\n",
        "    )\n",
        "    \n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    # Generate program visualizations\n",
        "    print(\"- Program visualizations...\")\n",
        "    plot_program_correlations(program_metrics)\n",
        "    plot_program_scores(program_metrics)\n",
        "    \n",
        "    # Generate cell type visualizations\n",
        "    print(\"- Cell type visualizations...\")\n",
        "    plot_cell_type_accuracies(\n",
        "        [cell_type_metrics['mouse_pred_labels'],\n",
        "         cell_type_metrics['xenium_pred_labels'],\n",
        "         cell_type_metrics['out_of_sample_pred_labels']],\n",
        "        [cell_type_metrics['mouse_true_labels'],\n",
        "         cell_type_metrics['xenium_true_labels'],\n",
        "         cell_type_metrics['out_of_sample_true_labels']],\n",
        "        ['In-Sample Mouse', 'In-Sample Xenium MBC', 'New-Sample Xenium MBC']\n",
        "    )\n",
        "    \n",
        "    # Plot cell type spatial distributions\n",
        "    print(\"- Spatial distributions...\")\n",
        "    for sample_id in ['6760', '7149', '7179']:\n",
        "        plot_cell_type_spatial_distribution(\n",
        "            pred_htapp[sample_id],\n",
        "            cell_type_metrics['htapp_labels'][sample_id],\n",
        "            sample_id,\n",
        "            f'HTAPP Sample {sample_id}'\n",
        "        )\n",
        "    \n",
        "    # Generate HTAPP visualizations\n",
        "    print(\"- HTAPP visualizations...\")\n",
        "    plot_htapp_marker_genes(\n",
        "        pred_htapp, ground_truth_htapp,\n",
        "        ['krt19', 'col1a2', 'apoc1', 'pecam1'],\n",
        "        ['4531', '6760', '7479', '7629'],\n",
        "        g_to_cmaps,\n",
        "        'HTAPP Marker Genes',\n",
        "        'htapp_markers'\n",
        "    )\n",
        "    \n",
        "    plot_htapp_schematic(the_hists, 'HTAPP Overview', 'htapp_schematic')\n",
        "    plot_htapp_metrics(htapp_scores, htapp_dists, \n",
        "                      'HTAPP Performance Metrics', 'htapp_metrics')\n",
        "    \n",
        "    print(\"\\nAnalysis complete! All results saved in 'final_figures_schaf_revision_pngs/'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate MSKCC schematic figure\n",
        "def generate_mskcc_schematic():\n",
        "    image_dir = '/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/smaller_images/smaller_images'\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "    \n",
        "    n_images = len(image_files)\n",
        "    n_cols = 6\n",
        "    n_rows = (n_images + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, image_file in tqdm(enumerate(image_files)):\n",
        "        img_path = os.path.join(image_dir, image_file)\n",
        "        img = Image.open(img_path)\n",
        "        if '133729' in image_file or '129477' in image_file:\n",
        "            img = np.array(img)\n",
        "            third = img.shape[0] // 3\n",
        "            img = img[third:-third,]\n",
        "        \n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title('MSKCC ' + image_file[:6])\n",
        "    \n",
        "    # Remove empty subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    \n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/mskcc_schematic.png', dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process Visium data\n",
        "def load_visium_data():\n",
        "    xen_dir = '/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium'\n",
        "    visium_adata = sc.read_10x_h5(f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/visium_breast_xenium.h5')\n",
        "    visium_adata.var_names_make_unique()\n",
        "    \n",
        "    # Load and process tissue positions\n",
        "    tissue_positions = pd.read_csv(f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/tissue_positions_visium.csv').set_index('barcode')\n",
        "    tissue_positions = tissue_positions.loc[visium_adata.obs.index]\n",
        "    visium_adata.obs = tissue_positions\n",
        "    \n",
        "    # Filter and process coordinates\n",
        "    the_verts = (visium_adata.obs['pxl_col_in_fullres'].max() - visium_adata.obs['pxl_col_in_fullres'])\n",
        "    good_vis_inds = np.where(the_verts<=10000)[0]\n",
        "    visium_xs = np.array(list(visium_adata.obs['pxl_row_in_fullres'][good_vis_inds]))\n",
        "    visium_xs = visium_xs - visium_xs.min()\n",
        "    visium_ys = np.array(list(the_verts[good_vis_inds]))\n",
        "    \n",
        "    visium_adata = visium_adata[good_vis_inds]\n",
        "    visium_adata.obs['x'] = visium_xs\n",
        "    visium_adata.obs['y'] = visium_ys\n",
        "    \n",
        "    # Normalize data\n",
        "    sc.pp.log1p(visium_adata)\n",
        "    visium_adata.X = visium_adata.X.todense()\n",
        "    visium_adata.var.index = [q.lower() for q in visium_adata.var.index]\n",
        "    \n",
        "    # Transform coordinates\n",
        "    potential_trans = np.array([\n",
        "        [0.130157158, 2.594980119, -12243.84897],\n",
        "        [-2.594980119, 0.130157158, 40352.06194],\n",
        "        [0, 0, 1],\n",
        "    ])\n",
        "    \n",
        "    horis2 = np.array(visium_adata.obs['pxl_col_in_fullres'])\n",
        "    verts2 = np.array(visium_adata.obs['pxl_row_in_fullres'])\n",
        "    new_horis2, new_verts2 = do_transform2(horis2, verts2, potential_trans)\n",
        "    \n",
        "    visium_adata.obs['x'] = new_horis2\n",
        "    visium_adata.obs['y'] = new_verts2\n",
        "    \n",
        "    return visium_adata\n",
        "\n",
        "@njit(parallel=True)\n",
        "def do_transform2(horis2, verts2, potential_trans):\n",
        "    new_horis, new_verts = np.zeros_like(horis2), np.zeros_like(verts2)\n",
        "    for t in prange(len(horis2)):\n",
        "        i = horis2[t] / 1.\n",
        "        j = verts2[t] / 1.\n",
        "        new = potential_trans.dot(np.array([i,j,1]))\n",
        "        new_horis[t] = new[0]\n",
        "        new_verts[t] = new[1]\n",
        "    return new_horis, new_verts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process ground truth and prediction data\n",
        "def load_ground_truth_and_predictions():\n",
        "    # Load ground truth out of sample data\n",
        "    cells_info = pd.read_csv(f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/cells.csv')\n",
        "    cells_info = cells_info.set_index('cell_id')\n",
        "    \n",
        "    # Load and process transformation matrix\n",
        "    df = pd.read_csv(\n",
        "        f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/alignment_new_xen.csv',\n",
        "        header=None,\n",
        "    )\n",
        "    transformation_matrix = df.values.astype(np.float32)\n",
        "    inv_trans = np.linalg.inv(transformation_matrix).astype('float64')\n",
        "    \n",
        "    # Transform coordinates\n",
        "    verts = np.array(cells_info['y_centroid'])\n",
        "    horis = np.array(cells_info['x_centroid'])\n",
        "    xs, ys = do_transform(horis, verts, inv_trans)\n",
        "    \n",
        "    final_obs = cells_info\n",
        "    final_obs['x'] = xs\n",
        "    final_obs['y'] = ys\n",
        "    \n",
        "    # Load and process ground truth data\n",
        "    new_mer = sc.read_10x_h5(\n",
        "        f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/cell_feature_matrix.h5'\n",
        "    )\n",
        "    sc.pp.log1p(new_mer)\n",
        "    new_mer.X = np.array(new_mer.X.todense())\n",
        "    new_mer.var.index = list(q.lower() for q in new_mer.var.index)\n",
        "    new_mer.obs = final_obs\n",
        "    \n",
        "    ground_truth_out_of_sample = new_mer\n",
        "    \n",
        "    # Load predictions\n",
        "    pred_out_of_sample = sc.read_h5ad('/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/xenium_cancer_inferences/whole_sample.h5ad')\n",
        "    \n",
        "    # Load in-sample data\n",
        "    ground_truth_in_sample = {\n",
        "        z: sc.read_h5ad(f'/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/cancer_in_sample_folds/fold_{z}_st.h5ad')\n",
        "        for z in range(4)\n",
        "    }\n",
        "    \n",
        "    pred_in_sample = {\n",
        "        z: sc.read_h5ad(f'/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/xenium_cancer_inferences/fold_{z}.h5ad')\n",
        "        for z in range(4)\n",
        "    }\n",
        "    \n",
        "    # Process in-sample data\n",
        "    for z in ground_truth_in_sample:\n",
        "        sc.pp.log1p(ground_truth_in_sample[z])\n",
        "        ground_truth_in_sample[z].X = np.array(ground_truth_in_sample[z].X.todense())\n",
        "    \n",
        "    return ground_truth_out_of_sample, pred_out_of_sample, ground_truth_in_sample, pred_in_sample\n",
        "\n",
        "@njit(parallel=True)\n",
        "def do_transform(horis, verts, inv_trans):\n",
        "    new_horis, new_verts = np.zeros_like(horis), np.zeros_like(verts)\n",
        "    for t in prange(len(horis)):\n",
        "        i = horis[t] / .2125\n",
        "        j = verts[t] / .2125\n",
        "        new = inv_trans.dot(np.array([i,j,1]))\n",
        "        new_horis[t] = new[0]\n",
        "        new_verts[t] = new[1]\n",
        "    return new_horis, new_verts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_cell_type_heterogeneity(\n",
        "    label_to_ground_truth_in_sample, label_to_pred_in_sample,\n",
        "    label_to_ground_truth_mouse, label_to_pred_mouse,\n",
        "    label_to_ground_truth_out_of_sample, label_to_pred_out_of_sample,\n",
        "    common_in_sample, common_mouse, common_out_of_sample\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate cell type heterogeneity metrics across different datasets.\n",
        "    \n",
        "    Returns:\n",
        "    - label_to_avg_in_sample_heteros: Dict mapping cell types to average heterogeneity in in-sample data\n",
        "    - label_to_avg_mouse_heteros: Dict mapping cell types to average heterogeneity in mouse data\n",
        "    - label_to_heteros_out_of_sample: Dict mapping cell types to heterogeneity in out-of-sample data\n",
        "    \"\"\"\n",
        "    label_to_avg_in_sample_heteros = {}\n",
        "    for label in label_to_ground_truth_in_sample:\n",
        "        heteros = []\n",
        "        for fold in label_to_ground_truth_in_sample[label]:\n",
        "            gt = label_to_ground_truth_in_sample[label][fold][common_in_sample]\n",
        "            pred = label_to_pred_in_sample[label][fold][common_in_sample]\n",
        "            hetero = calculate_heterogeneity(gt, pred)\n",
        "            heteros.append(hetero)\n",
        "        label_to_avg_in_sample_heteros[label] = np.mean(heteros)\n",
        "    \n",
        "    label_to_avg_mouse_heteros = {}\n",
        "    for label in label_to_ground_truth_mouse:\n",
        "        heteros = []\n",
        "        for fold in label_to_ground_truth_mouse[label]:\n",
        "            gt = label_to_ground_truth_mouse[label][fold][common_mouse]\n",
        "            pred = label_to_pred_mouse[label][fold][common_mouse]\n",
        "            hetero = calculate_heterogeneity(gt, pred)\n",
        "            heteros.append(hetero)\n",
        "        label_to_avg_mouse_heteros[label] = np.mean(heteros)\n",
        "    \n",
        "    label_to_heteros_out_of_sample = {}\n",
        "    for label in label_to_ground_truth_out_of_sample:\n",
        "        gt = label_to_ground_truth_out_of_sample[label][common_out_of_sample]\n",
        "        pred = label_to_pred_out_of_sample[label][common_out_of_sample]\n",
        "        hetero = calculate_heterogeneity(gt, pred)\n",
        "        label_to_heteros_out_of_sample[label] = hetero\n",
        "    \n",
        "    return label_to_avg_in_sample_heteros, label_to_avg_mouse_heteros, label_to_heteros_out_of_sample\n",
        "\n",
        "def calculate_heterogeneity(ground_truth, predictions):\n",
        "    \"\"\"\n",
        "    Calculate heterogeneity between ground truth and predictions.\n",
        "    \n",
        "    Args:\n",
        "    - ground_truth: Ground truth values\n",
        "    - predictions: Predicted values\n",
        "    \n",
        "    Returns:\n",
        "    - heterogeneity: Calculated heterogeneity metric\n",
        "    \"\"\"\n",
        "    # Calculate variance of differences\n",
        "    differences = ground_truth - predictions\n",
        "    return np.var(differences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_histology_split_figures(\n",
        "    in_sample_fold_to_hist,\n",
        "    mouse_fold_to_hist,\n",
        "    in_sample_hist,\n",
        "    out_of_sample_hist\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate figures showing performance splits by histology.\n",
        "    \n",
        "    Args:\n",
        "    - in_sample_fold_to_hist: Dict mapping folds to histology labels for in-sample data\n",
        "    - mouse_fold_to_hist: Dict mapping folds to histology labels for mouse data\n",
        "    - in_sample_hist: Histology labels for in-sample data\n",
        "    - out_of_sample_hist: Histology labels for out-of-sample data\n",
        "    \"\"\"\n",
        "    # Set up the figure\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Plot in-sample histology splits\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.boxplot(data=pd.DataFrame({\n",
        "        'Histology': [in_sample_fold_to_hist[fold] for fold in in_sample_fold_to_hist],\n",
        "        'Performance': [performance_metric(fold) for fold in in_sample_fold_to_hist]\n",
        "    }))\n",
        "    plt.title('In-sample Performance by Histology')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Plot mouse histology splits\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.boxplot(data=pd.DataFrame({\n",
        "        'Histology': [mouse_fold_to_hist[fold] for fold in mouse_fold_to_hist],\n",
        "        'Performance': [performance_metric(fold) for fold in mouse_fold_to_hist]\n",
        "    }))\n",
        "    plt.title('Mouse Performance by Histology')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Plot out-of-sample histology comparison\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.barplot(data=pd.DataFrame({\n",
        "        'Histology': out_of_sample_hist,\n",
        "        'Performance': [performance_metric(sample) for sample in out_of_sample_hist.index]\n",
        "    }))\n",
        "    plt.title('Out-of-sample Performance by Histology')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/histology_splits.png', dpi=400, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def generate_correlation_histograms(\n",
        "    mouse_avg_in_sample_corrs,\n",
        "    mouse_train_genes\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate histograms showing correlation distributions.\n",
        "    \n",
        "    Args:\n",
        "    - mouse_avg_in_sample_corrs: Average correlations for mouse in-sample data\n",
        "    - mouse_train_genes: List of genes used in mouse training\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Plot correlation distribution for mouse genes\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(data=mouse_avg_in_sample_corrs[mouse_train_genes], bins=50)\n",
        "    plt.title('Mouse Gene Correlations')\n",
        "    plt.xlabel('Correlation')\n",
        "    plt.ylabel('Count')\n",
        "    \n",
        "    # Plot correlation distribution for all genes\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(data=mouse_avg_in_sample_corrs, bins=50)\n",
        "    plt.title('All Gene Correlations')\n",
        "    plt.xlabel('Correlation')\n",
        "    plt.ylabel('Count')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/correlation_histograms.png', dpi=400, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def performance_metric(sample):\n",
        "    \"\"\"\n",
        "    Calculate performance metric for a given sample.\n",
        "    This is a placeholder - implement actual metric calculation.\n",
        "    \"\"\"\n",
        "    # Implement actual performance metric calculation here\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def performance_metric(sample_data):\n",
        "    \"\"\"\n",
        "    Calculate performance metric for a given sample.\n",
        "    \n",
        "    Args:\n",
        "    - sample_data: Dictionary containing ground truth and predicted values\n",
        "    \n",
        "    Returns:\n",
        "    - float: Performance metric (correlation coefficient)\n",
        "    \"\"\"\n",
        "    # Extract ground truth and predictions\n",
        "    ground_truth = sample_data['ground_truth']\n",
        "    predictions = sample_data['predictions']\n",
        "    \n",
        "    # Calculate correlation coefficient\n",
        "    correlation = np.corrcoef(ground_truth.flatten(), predictions.flatten())[0, 1]\n",
        "    \n",
        "    # Calculate R-squared\n",
        "    r2 = r2_score(ground_truth.flatten(), predictions.flatten())\n",
        "    \n",
        "    # Return combined metric\n",
        "    return 0.5 * (correlation + r2)  # Average of correlation and R-squared\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_htapp_images():\n",
        "    \"\"\"\n",
        "    Load HTAPP histology images.\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Mapping of HTAPP sample IDs to histology images\n",
        "    \"\"\"\n",
        "    image_dir = '/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/htapp_images'\n",
        "    htapp_images = {}\n",
        "    \n",
        "    # Load all HTAPP images\n",
        "    for image_file in os.listdir(image_dir):\n",
        "        if image_file.endswith(('.tif', '.png')):\n",
        "            sample_id = image_file.split('_')[0]\n",
        "            img_path = os.path.join(image_dir, image_file)\n",
        "            htapp_images[sample_id] = iio.imread(img_path)\n",
        "    \n",
        "    return htapp_images\n",
        "\n",
        "def load_mskcc_images():\n",
        "    \"\"\"\n",
        "    Load MSKCC histology images.\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Mapping of MSKCC sample IDs to histology images\n",
        "    \"\"\"\n",
        "    image_dir = '/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/smaller_images/smaller_images'\n",
        "    mskcc_images = {}\n",
        "    \n",
        "    # Load all MSKCC images\n",
        "    for image_file in os.listdir(image_dir):\n",
        "        if image_file.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "            sample_id = image_file[:6]\n",
        "            img_path = os.path.join(image_dir, image_file)\n",
        "            img = Image.open(img_path)\n",
        "            \n",
        "            # Special processing for certain samples\n",
        "            if '133729' in image_file or '129477' in image_file:\n",
        "                img = np.array(img)\n",
        "                third = img.shape[0] // 3\n",
        "                img = img[third:-third,]\n",
        "            \n",
        "            mskcc_images[sample_id] = img\n",
        "    \n",
        "    return mskcc_images\n",
        "\n",
        "def generate_mskcc_schematic():\n",
        "    \"\"\"\n",
        "    Generate MSKCC schematic figure showing all samples.\n",
        "    \"\"\"\n",
        "    # Load MSKCC images\n",
        "    mskcc_images = load_mskcc_images()\n",
        "    \n",
        "    # Calculate grid dimensions\n",
        "    n_images = len(mskcc_images)\n",
        "    n_cols = 6\n",
        "    n_rows = (n_images + n_cols - 1) // n_cols\n",
        "    \n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # Plot each image\n",
        "    for i, (sample_id, img) in enumerate(mskcc_images.items()):\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f'MSKCC {sample_id}')\n",
        "    \n",
        "    # Remove empty subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/mskcc_schematic.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "def analyze_cell_type_specificity(adata, cell_type_key='cell_type'):\n",
        "    \"\"\"\n",
        "    Analyze cell type-specific expression patterns.\n",
        "    \n",
        "    Args:\n",
        "    - adata: AnnData object containing expression data\n",
        "    - cell_type_key: Key in adata.obs containing cell type labels\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Cell type-specific expression patterns and statistics\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Calculate mean expression per cell type\n",
        "    cell_types = adata.obs[cell_type_key].unique()\n",
        "    for ct in cell_types:\n",
        "        ct_mask = adata.obs[cell_type_key] == ct\n",
        "        ct_mean = adata[ct_mask].X.mean(axis=0)\n",
        "        ct_std = adata[ct_mask].X.std(axis=0)\n",
        "        \n",
        "        results[ct] = {\n",
        "            'mean_expression': ct_mean,\n",
        "            'std_expression': ct_std,\n",
        "            'n_cells': ct_mask.sum()\n",
        "        }\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cell_type_expression_patterns(cell_type_results, genes_of_interest=None):\n",
        "    \"\"\"\n",
        "    Plot cell type-specific expression patterns.\n",
        "    \n",
        "    Args:\n",
        "    - cell_type_results: Dict from analyze_cell_type_specificity\n",
        "    - genes_of_interest: Optional list of genes to highlight\n",
        "    \"\"\"\n",
        "    # Get all cell types and genes\n",
        "    cell_types = list(cell_type_results.keys())\n",
        "    if genes_of_interest is None:\n",
        "        genes_of_interest = cell_type_results[cell_types[0]]['mean_expression'].index\n",
        "    \n",
        "    # Create expression matrix\n",
        "    expr_matrix = np.zeros((len(cell_types), len(genes_of_interest)))\n",
        "    for i, ct in enumerate(cell_types):\n",
        "        expr_matrix[i] = cell_type_results[ct]['mean_expression'][genes_of_interest]\n",
        "    \n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.heatmap(\n",
        "        expr_matrix,\n",
        "        xticklabels=genes_of_interest,\n",
        "        yticklabels=cell_types,\n",
        "        cmap='viridis',\n",
        "        center=0\n",
        "    )\n",
        "    plt.title('Cell Type-Specific Expression Patterns')\n",
        "    plt.xlabel('Genes')\n",
        "    plt.ylabel('Cell Types')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/cell_type_expression_patterns.png', dpi=400, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_cell_type_correlations(adata1, adata2, cell_type_key='cell_type'):\n",
        "    \"\"\"\n",
        "    Plot correlations between cell types in two datasets.\n",
        "    \n",
        "    Args:\n",
        "    - adata1: First AnnData object\n",
        "    - adata2: Second AnnData object\n",
        "    - cell_type_key: Key for cell type annotations\n",
        "    \"\"\"\n",
        "    # Get common cell types\n",
        "    cell_types = np.intersect1d(\n",
        "        adata1.obs[cell_type_key].unique(),\n",
        "        adata2.obs[cell_type_key].unique()\n",
        "    )\n",
        "    \n",
        "    # Calculate correlations\n",
        "    corr_matrix = np.zeros((len(cell_types), len(cell_types)))\n",
        "    for i, ct1 in enumerate(cell_types):\n",
        "        for j, ct2 in enumerate(cell_types):\n",
        "            mask1 = adata1.obs[cell_type_key] == ct1\n",
        "            mask2 = adata2.obs[cell_type_key] == ct2\n",
        "            mean1 = adata1[mask1].X.mean(axis=0)\n",
        "            mean2 = adata2[mask2].X.mean(axis=0)\n",
        "            corr_matrix[i, j] = np.corrcoef(mean1, mean2)[0, 1]\n",
        "    \n",
        "    # Plot correlation matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(\n",
        "        corr_matrix,\n",
        "        xticklabels=cell_types,\n",
        "        yticklabels=cell_types,\n",
        "        cmap='coolwarm',\n",
        "        center=0,\n",
        "        vmin=-1,\n",
        "        vmax=1\n",
        "    )\n",
        "    plt.title('Cell Type Correlations Between Datasets')\n",
        "    plt.xlabel('Dataset 2 Cell Types')\n",
        "    plt.ylabel('Dataset 1 Cell Types')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/cell_type_correlations.png', dpi=400, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_cell_type_heterogeneity(cell_type_results):\n",
        "    \"\"\"\n",
        "    Plot cell type heterogeneity metrics.\n",
        "    \n",
        "    Args:\n",
        "    - cell_type_results: Dict from analyze_cell_type_specificity\n",
        "    \"\"\"\n",
        "    # Calculate heterogeneity (coefficient of variation) for each cell type\n",
        "    cell_types = []\n",
        "    cv_values = []\n",
        "    for ct, data in cell_type_results.items():\n",
        "        cell_types.append(ct)\n",
        "        cv = data['std_expression'] / (data['mean_expression'] + 1e-10)  # Add small constant to avoid division by zero\n",
        "        cv_values.append(cv.mean())\n",
        "    \n",
        "    # Plot bar chart\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=cell_types, y=cv_values)\n",
        "    plt.title('Cell Type Heterogeneity')\n",
        "    plt.xlabel('Cell Type')\n",
        "    plt.ylabel('Mean Coefficient of Variation')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/cell_type_heterogeneity.png', dpi=400, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_visium_data():\n",
        "    \"\"\"\n",
        "    Load and preprocess Visium spatial transcriptomics data.\n",
        "    \n",
        "    Returns:\n",
        "    - AnnData: Processed Visium data\n",
        "    \"\"\"\n",
        "    # Load Visium data\n",
        "    visium_adata = sc.read_10x_h5(f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/visium_breast_xenium.h5')\n",
        "    visium_adata.var_names_make_unique()\n",
        "    \n",
        "    # Load tissue positions\n",
        "    tissue_positions = pd.read_csv(f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/tissue_positions_visium.csv').set_index('barcode')\n",
        "    tissue_positions = tissue_positions.loc[visium_adata.obs.index]\n",
        "    visium_adata.obs = tissue_positions\n",
        "    \n",
        "    # Filter spots\n",
        "    the_verts = (visium_adata.obs['pxl_col_in_fullres'].max() - visium_adata.obs['pxl_col_in_fullres'])\n",
        "    good_vis_inds = np.where(the_verts <= 10000)[0]\n",
        "    visium_adata = visium_adata[good_vis_inds]\n",
        "    \n",
        "    # Process coordinates\n",
        "    visium_xs = np.array(visium_adata.obs['pxl_row_in_fullres'])\n",
        "    visium_xs = visium_xs - visium_xs.min()\n",
        "    visium_ys = np.array(the_verts)\n",
        "    visium_adata.obs['x'] = visium_xs\n",
        "    visium_adata.obs['y'] = visium_ys\n",
        "    \n",
        "    # Transform data\n",
        "    sc.pp.log1p(visium_adata)\n",
        "    visium_adata.X = visium_adata.X.todense()\n",
        "    visium_adata.var.index = [q.lower() for q in visium_adata.var.index]\n",
        "    \n",
        "    # Apply spatial transformation\n",
        "    potential_trans = np.array([\n",
        "        [0.130157158, 2.594980119, -12243.84897],\n",
        "        [-2.594980119, 0.130157158, 40352.06194],\n",
        "        [0, 0, 1],\n",
        "    ])\n",
        "    \n",
        "    horis = np.array(visium_adata.obs['pxl_col_in_fullres'])\n",
        "    verts = np.array(visium_adata.obs['pxl_row_in_fullres'])\n",
        "    new_horis, new_verts = transform_coordinates(horis, verts, potential_trans)\n",
        "    \n",
        "    visium_adata.obs['x'] = new_horis\n",
        "    visium_adata.obs['y'] = new_verts\n",
        "    \n",
        "    return visium_adata\n",
        "\n",
        "@njit(parallel=True)\n",
        "def transform_coordinates(horis, verts, trans_matrix):\n",
        "    \"\"\"\n",
        "    Transform spatial coordinates using transformation matrix.\n",
        "    \n",
        "    Args:\n",
        "    - horis: Horizontal coordinates\n",
        "    - verts: Vertical coordinates\n",
        "    - trans_matrix: Transformation matrix\n",
        "    \n",
        "    Returns:\n",
        "    - Tuple: Transformed coordinates (new_horis, new_verts)\n",
        "    \"\"\"\n",
        "    new_horis = np.zeros_like(horis)\n",
        "    new_verts = np.zeros_like(verts)\n",
        "    \n",
        "    for t in prange(len(horis)):\n",
        "        i = horis[t]\n",
        "        j = verts[t]\n",
        "        new = trans_matrix.dot(np.array([i, j, 1]))\n",
        "        new_horis[t] = new[0]\n",
        "        new_verts[t] = new[1]\n",
        "    \n",
        "    return new_horis, new_verts\n",
        "\n",
        "def trans_good(x):\n",
        "    \"\"\"\n",
        "    Transform data using log1p and exp transformations.\n",
        "    \n",
        "    Args:\n",
        "    - x: Input data\n",
        "    \n",
        "    Returns:\n",
        "    - Transformed data\n",
        "    \"\"\"\n",
        "    return np.log1p(((np.exp(x)) - 1.).round())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CellTypeClassifier(nn.Module):\n",
        "    \"\"\"Neural network for cell type classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dims, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        # Add hidden layers\n",
        "        for dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.BatchNorm1d(dim),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "        \n",
        "        # Add output layer\n",
        "        layers.extend([\n",
        "            nn.Linear(prev_dim, num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        ])\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def predict_cell_types(orig_adata, pred_adata, celltype_key, device='cuda'):\n",
        "    \"\"\"\n",
        "    Predict cell types using a neural network classifier.\n",
        "    \n",
        "    Args:\n",
        "    - orig_adata: Original AnnData with cell type labels\n",
        "    - pred_adata: AnnData to predict cell types for\n",
        "    - celltype_key: Key in orig_adata.obs containing cell type labels\n",
        "    - device: Device to run computations on\n",
        "    \n",
        "    Returns:\n",
        "    - np.array: Predicted cell type labels\n",
        "    - float: Classification accuracy on original data\n",
        "    \"\"\"\n",
        "    # Get common genes\n",
        "    common_var = np.intersect1d(orig_adata.var.index, pred_adata.var.index)\n",
        "    \n",
        "    # Get cell type labels\n",
        "    annos = np.unique(orig_adata.obs[celltype_key])\n",
        "    anno_to_label = dict(zip(annos, range(len(annos))))\n",
        "    label_to_anno = dict(zip(range(len(annos)), annos))\n",
        "    \n",
        "    # Prepare data\n",
        "    orig_features = orig_adata[:, common_var].X\n",
        "    orig_features = (orig_features - orig_features.mean(axis=0)) / orig_features.std(axis=0)\n",
        "    orig_features = np.nan_to_num(orig_features)\n",
        "    \n",
        "    orig_labels = np.array([anno_to_label[anno] for anno in orig_adata.obs[celltype_key]])\n",
        "    \n",
        "    # Create model\n",
        "    input_dim = len(common_var)\n",
        "    if input_dim > (1 << 10):\n",
        "        hidden_dims = [1 << 10, 1 << 8, 1 << 6]\n",
        "    else:\n",
        "        hidden_dims = [1 << 8, 1 << 6, 1 << 4]\n",
        "    \n",
        "    model = CellTypeClassifier(input_dim, hidden_dims, len(annos)).to(device)\n",
        "    \n",
        "    # Calculate class weights\n",
        "    num_cells = len(orig_labels)\n",
        "    class_weights = torch.tensor([\n",
        "        (float(num_cells) / np.sum(orig_labels == i)) \n",
        "        for i in range(len(annos))\n",
        "    ]).float().to(device)\n",
        "    \n",
        "    # Training parameters\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    # Create data loader\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(\n",
        "            torch.from_numpy(orig_features).float(),\n",
        "            torch.from_numpy(orig_labels)\n",
        "        ),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=6,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            batch_features = batch_features.to(device) / 10.\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader.dataset):.6f}')\n",
        "    \n",
        "    # Predict on original data\n",
        "    model.eval()\n",
        "    orig_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch_features, _ in train_loader:\n",
        "            batch_features = batch_features.to(device) / 10.\n",
        "            outputs = model(batch_features)\n",
        "            orig_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "    \n",
        "    # Prepare prediction data\n",
        "    pred_features = pred_adata[:, common_var].X\n",
        "    pred_features = (pred_features - pred_features.mean(axis=0)) / pred_features.std(axis=0)\n",
        "    pred_features = np.nan_to_num(pred_features)\n",
        "    \n",
        "    pred_loader = DataLoader(\n",
        "        TensorDataset(torch.from_numpy(pred_features).float()),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=6,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    # Predict on new data\n",
        "    new_pred = []\n",
        "    with torch.no_grad():\n",
        "        for (batch_features,) in pred_loader:\n",
        "            batch_features = batch_features.to(device) / 10.\n",
        "            outputs = model(batch_features)\n",
        "            new_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "    \n",
        "    # Calculate accuracy on original data\n",
        "    orig_pred = np.array(orig_pred)\n",
        "    accuracy = np.mean(orig_pred == orig_labels)\n",
        "    print(f'Classification accuracy on original data: {accuracy:.4f}')\n",
        "    \n",
        "    # Convert numeric predictions back to cell type labels\n",
        "    new_pred = np.array([label_to_anno[label] for label in new_pred])\n",
        "    \n",
        "    return new_pred, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_spatial_data(pred_adata, visium_adata, fold):\n",
        "    \"\"\"\n",
        "    Analyze spatial relationships between predicted data and Visium data.\n",
        "    \n",
        "    Args:\n",
        "    - pred_adata: Predicted AnnData object\n",
        "    - visium_adata: Visium AnnData object\n",
        "    - fold: Fold number for analysis\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Mapping of Xenium coordinates to Visium indices\n",
        "    - np.array: Spot predictions\n",
        "    \"\"\"\n",
        "    # Get common genes\n",
        "    common_var = np.intersect1d(pred_adata.var.index, visium_adata.var.index)\n",
        "    \n",
        "    # Extract coordinates\n",
        "    vis_xs = []\n",
        "    vis_ys = []\n",
        "    vis_ts = []\n",
        "    \n",
        "    # Define region boundaries\n",
        "    y_lim = 12900\n",
        "    x_lim = 17700\n",
        "    \n",
        "    # Filter Visium spots based on fold\n",
        "    for x, y, t in zip(visium_adata.obs['x'], visium_adata.obs['y'], \n",
        "                      visium_adata[:, common_var].X):\n",
        "        if not (0 <= y <= 25778 and 0 <= x < 35416):\n",
        "            continue\n",
        "            \n",
        "        if fold == 0 and x < x_lim and y < y_lim:\n",
        "            vis_xs.append(x)\n",
        "            vis_ys.append(y)\n",
        "            vis_ts.append(t)\n",
        "        elif fold == 1 and x >= x_lim and y >= y_lim:\n",
        "            vis_xs.append(x)\n",
        "            vis_ys.append(y)\n",
        "            vis_ts.append(t)\n",
        "        elif fold == 2 and x >= x_lim and y < y_lim:\n",
        "            vis_xs.append(x)\n",
        "            vis_ys.append(y)\n",
        "            vis_ts.append(t)\n",
        "        elif fold == 3 and x < x_lim and y >= y_lim:\n",
        "            vis_xs.append(x)\n",
        "            vis_ys.append(y)\n",
        "            vis_ts.append(t)\n",
        "    \n",
        "    vis_xs = np.array(vis_xs)\n",
        "    vis_ys = np.array(vis_ys)\n",
        "    vis_ts = np.array(vis_ts)\n",
        "    \n",
        "    # Define boundary lines\n",
        "    def get_line_params(p1, p2):\n",
        "        \"\"\"Get line parameters (slope and intercept).\"\"\"\n",
        "        x1, y1 = p1\n",
        "        x2, y2 = p2\n",
        "        m = (y2 - y1) / (x2 - x1)\n",
        "        b = y1 - m * x1\n",
        "        return m, b\n",
        "    \n",
        "    bottom_line = get_line_params((3201, 4850), (1386, 1458))\n",
        "    right_line = get_line_params((33057, 8591), (33218, 4798))\n",
        "    left_line = get_line_params((504, 25967), (726, 20755))\n",
        "    \n",
        "    # Filter predicted data points\n",
        "    xen_xs = []\n",
        "    xen_ys = []\n",
        "    xen_ps = []\n",
        "    \n",
        "    for x, y, p in zip(pred_adata.obs['x'], pred_adata.obs['y'], \n",
        "                      pred_adata[:, common_var].X):\n",
        "        if y > 25761:\n",
        "            continue\n",
        "            \n",
        "        m, b = bottom_line\n",
        "        if y < m * x + b:\n",
        "            continue\n",
        "            \n",
        "        m, b = left_line\n",
        "        if y < m * x + b:\n",
        "            continue\n",
        "            \n",
        "        m, b = right_line\n",
        "        if y > m * x + b:\n",
        "            continue\n",
        "            \n",
        "        xen_xs.append(x)\n",
        "        xen_ys.append(y)\n",
        "        xen_ps.append(p)\n",
        "    \n",
        "    xen_xs = np.array(xen_xs)\n",
        "    xen_ys = np.array(xen_ys)\n",
        "    xen_ps = np.array(xen_ps)\n",
        "    \n",
        "    # Map Xenium spots to nearest Visium spots\n",
        "    xen_to_vis_ind = {}\n",
        "    radius = 130\n",
        "    \n",
        "    for x, y in zip(xen_xs, xen_ys):\n",
        "        ind = -1\n",
        "        for i, (vx, vy) in enumerate(zip(vis_xs, vis_ys)):\n",
        "            if (vx - x)**2 + (vy - y)**2 < radius**2:\n",
        "                ind = i\n",
        "                break\n",
        "        xen_to_vis_ind[(x, y)] = ind\n",
        "    \n",
        "    # Calculate spot predictions\n",
        "    num_spots = len(vis_xs)\n",
        "    spot_preds = np.zeros((num_spots, len(common_var)))\n",
        "    \n",
        "    for x, y, p in zip(xen_xs, xen_ys, xen_ps):\n",
        "        if xen_to_vis_ind[(x, y)] >= 0:\n",
        "            ind = xen_to_vis_ind[(x, y)]\n",
        "            spot_preds[ind] += p\n",
        "    \n",
        "    return xen_to_vis_ind, spot_preds\n",
        "\n",
        "def plot_spatial_predictions(visium_adata, spot_preds, fold, gene_name):\n",
        "    \"\"\"\n",
        "    Plot spatial predictions for a specific gene.\n",
        "    \n",
        "    Args:\n",
        "    - visium_adata: Visium AnnData object\n",
        "    - spot_preds: Predicted spot values\n",
        "    - fold: Fold number\n",
        "    - gene_name: Name of gene to plot\n",
        "    \"\"\"\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot ground truth\n",
        "    plt.subplot(121)\n",
        "    sc.pl.spatial(\n",
        "        visium_adata,\n",
        "        color=gene_name,\n",
        "        show=False,\n",
        "        title=f'Ground Truth - {gene_name}'\n",
        "    )\n",
        "    \n",
        "    # Plot predictions\n",
        "    plt.subplot(122)\n",
        "    gene_idx = visium_adata.var.index.get_loc(gene_name)\n",
        "    sc.pl.spatial(\n",
        "        visium_adata,\n",
        "        color=spot_preds[:, gene_idx],\n",
        "        show=False,\n",
        "        title=f'Predictions - {gene_name}'\n",
        "    )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/spatial_predictions_fold{fold}_{gene_name}.png', \n",
        "                dpi=400, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_programs():\n",
        "    \"\"\"\n",
        "    Load hallmark and cancer programs.\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Combined hallmark and cancer programs\n",
        "    \"\"\"\n",
        "    # Load hallmark programs\n",
        "    with open('/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/final_new_schaf_start_jan2324/hallmark_programs.json') as f:\n",
        "        hallmark_programs = json.load(f)\n",
        "    \n",
        "    # Load cancer programs\n",
        "    with open('/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/final_new_schaf_start_jan2324/cancer_programs.json') as f:\n",
        "        cancer_programs = json.load(f)\n",
        "    \n",
        "    # Extract gene symbols\n",
        "    hallmark_programs = {k: v['geneSymbols'] for k, v in hallmark_programs.items()}\n",
        "    cancer_programs = {k: v['geneSymbols'] for k, v in cancer_programs.items()}\n",
        "    \n",
        "    # Combine programs\n",
        "    hallmark_programs.update(cancer_programs)\n",
        "    return hallmark_programs\n",
        "\n",
        "def load_htapp_data():\n",
        "    \"\"\"\n",
        "    Load and process HTAPP data.\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Ground truth HTAPP data\n",
        "    - Dict: Predicted HTAPP data\n",
        "    - Dict: MERFISH HTAPP data\n",
        "    \"\"\"\n",
        "    # Load single-cell data\n",
        "    these_keys = ['4531', '7179', '7479', '7629', '932', '6760', '7149', '4381', '8239']\n",
        "    sc_dir = f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/final_scs/schtapp'\n",
        "    \n",
        "    # Process single-cell data\n",
        "    the_scs = {}\n",
        "    for file in os.listdir(sc_dir):\n",
        "        for k in these_keys:\n",
        "            if k not in file:\n",
        "                continue\n",
        "            sc_adata = sc.read_h5ad(f'{sc_dir}/{file}')\n",
        "            new_v = sc.AnnData(X=np.array(sc_adata.obsm['counts'].todense()), obs=sc_adata.obs)\n",
        "            sc.pp.log1p(new_v)\n",
        "            new_v.var.index = sc_adata.uns['counts_var']\n",
        "            sc_adata = new_v\n",
        "            sc_adata.var.index = [q.lower() for q in sc_adata.var.index]\n",
        "            the_scs[k] = sc_adata\n",
        "    \n",
        "    # Get common genes\n",
        "    all_common_sc_var = the_scs['6760'].var.index\n",
        "    for k, v in the_scs.items():\n",
        "        all_common_sc_var = np.intersect1d(all_common_sc_var, v.var.index)\n",
        "        the_scs[k] = v[:, all_common_sc_var]\n",
        "    \n",
        "    # Load predictions\n",
        "    pred_htapp = {\n",
        "        k: sc.read_h5ad(f'htapp_inferences_with_annos/{k}.h5ad') \n",
        "        for k in these_keys\n",
        "    }\n",
        "    \n",
        "    # Load MERFISH data\n",
        "    mers_dir = f'{os.getcwd().split(\"/ccomiter/\")[0]}/ccomiter/htapp_supervise/final_mers'\n",
        "    merfish_htapp = {}\n",
        "    for f in os.listdir(mers_dir):\n",
        "        key = f.split('_')[0]\n",
        "        mer = sc.read_h5ad(os.path.join(mers_dir, f'{key}_merfish.h5ad'))\n",
        "        mer.X = np.array(mer.obsm['counts'].todense())\n",
        "        sc.pp.log1p(mer)\n",
        "        mer.var.index = [q.lower() for q in mer.var.index]\n",
        "        merfish_htapp[key] = mer\n",
        "    \n",
        "    # Get common MERFISH genes\n",
        "    all_common_mer_var = merfish_htapp['6760'].var.index\n",
        "    for k, v in merfish_htapp.items():\n",
        "        all_common_mer_var = np.intersect1d(all_common_mer_var, v.var.index)\n",
        "        merfish_htapp[k] = v[:, all_common_mer_var]\n",
        "    \n",
        "    return the_scs, pred_htapp, merfish_htapp\n",
        "\n",
        "def generate_cell_type_labels(merfish_data, segmentation_data):\n",
        "    \"\"\"\n",
        "    Generate cell type labels from MERFISH and segmentation data.\n",
        "    \n",
        "    Args:\n",
        "    - merfish_data: Dict of MERFISH data\n",
        "    - segmentation_data: Dict of segmentation data\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Cell type labels for each sample\n",
        "    \"\"\"\n",
        "    # Define cell types\n",
        "    celltypes = ['Fibrosis_1', 'ImmuneCells_1', 'Normal_1', 'Tumor_1', 'Vasculature_1']\n",
        "    ct_names = ['Fibrosis', 'Immune', 'Normal', 'Tumor', 'Vascular']\n",
        "    ct_to_name = dict(zip(celltypes, ct_names))\n",
        "    \n",
        "    # Process segmentation data\n",
        "    the_xs = {}\n",
        "    the_ys = {}\n",
        "    for k, seg in segmentation_data.items():\n",
        "        the_xs[k] = np.array(seg['Centroid X px']).astype(int)\n",
        "        the_ys[k] = np.array(seg['Centroid Y px']).astype(int)\n",
        "    \n",
        "    def make_label(info):\n",
        "        \"\"\"Create normalized label vector.\"\"\"\n",
        "        res = []\n",
        "        for ct in celltypes:\n",
        "            if ct in info and info[ct]:\n",
        "                res.append(1)\n",
        "            else:\n",
        "                res.append(0)\n",
        "        res = np.array(res)\n",
        "        if res.sum() > 0:\n",
        "            res = res / res.sum()\n",
        "        return res\n",
        "    \n",
        "    # Generate labels\n",
        "    the_labels = {}\n",
        "    for k, mer in merfish_data.items():\n",
        "        # Handle special cases\n",
        "        if k == '8239':\n",
        "            mer.obs['Vasculature_1'] = mer.obs['BloodClots_1']\n",
        "        elif k == '932':\n",
        "            mer.obs['Vasculature_1'] = mer.obs['BloodVessels_1']\n",
        "        \n",
        "        # Get coordinates\n",
        "        xs = list(mer.obs['x'])\n",
        "        ys = list(mer.obs['y'])\n",
        "        \n",
        "        # Build KD-tree\n",
        "        tree = cKDTree(np.array(list(zip(xs, ys))))\n",
        "        \n",
        "        # Generate labels\n",
        "        labels = []\n",
        "        for x, y in zip(the_xs[k], the_ys[k]):\n",
        "            _, ind = tree.query((x, y), k=1)\n",
        "            label = make_label(mer.obs.iloc[ind])\n",
        "            labels.append(label)\n",
        "        \n",
        "        the_labels[k] = np.array(labels)\n",
        "    \n",
        "    return the_labels\n",
        "\n",
        "def calculate_spatial_distances(merfish_data, segmentation_data):\n",
        "    \"\"\"\n",
        "    Calculate spatial distances between cells.\n",
        "    \n",
        "    Args:\n",
        "    - merfish_data: Dict of MERFISH data\n",
        "    - segmentation_data: Dict of segmentation data\n",
        "    \n",
        "    Returns:\n",
        "    - Dict: Distances for each sample\n",
        "    \"\"\"\n",
        "    the_dists = {}\n",
        "    for k, mer in merfish_data.items():\n",
        "        # Get coordinates\n",
        "        xs = list(mer.obs['x'])\n",
        "        ys = list(mer.obs['y'])\n",
        "        \n",
        "        # Build KD-tree\n",
        "        tree = cKDTree(np.array(list(zip(xs, ys))))\n",
        "        \n",
        "        # Calculate distances\n",
        "        dists = []\n",
        "        for x, y in zip(segmentation_data[k]['Centroid X px'], \n",
        "                       segmentation_data[k]['Centroid Y px']):\n",
        "            dist, _ = tree.query((x, y), k=1)\n",
        "            dists.append(dist)\n",
        "        \n",
        "        the_dists[k] = np.array(dists)\n",
        "    \n",
        "    return the_dists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_correlations_and_scores(\n",
        "    ground_truth_out_of_sample, pred_out_of_sample,\n",
        "    ground_truth_in_sample, pred_in_sample\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate correlations and scores for both in-sample and out-of-sample data.\n",
        "    \n",
        "    Args:\n",
        "    - ground_truth_out_of_sample: Ground truth out-of-sample data\n",
        "    - pred_out_of_sample: Predicted out-of-sample data\n",
        "    - ground_truth_in_sample: Dict of ground truth in-sample data\n",
        "    - pred_in_sample: Dict of predicted in-sample data\n",
        "    \n",
        "    Returns:\n",
        "    - Tuple: Correlation and score metrics\n",
        "    \"\"\"\n",
        "    # Get common genes\n",
        "    common_out_of_sample = np.intersect1d(\n",
        "        ground_truth_out_of_sample.var.index,\n",
        "        pred_out_of_sample.var.index\n",
        "    )\n",
        "    common_in_sample = np.intersect1d(\n",
        "        ground_truth_in_sample[0].var.index,\n",
        "        pred_in_sample[0].var.index\n",
        "    )\n",
        "    all_pred_genes = list(pred_in_sample[0].var.index)\n",
        "    \n",
        "    # Calculate out-of-sample metrics\n",
        "    out_of_sample_corrs = {}\n",
        "    out_of_sample_scores = {}\n",
        "    for g in common_out_of_sample:\n",
        "        t = np.array(ground_truth_out_of_sample[:, g].X.squeeze())\n",
        "        p = np.array(pred_out_of_sample[:, g].X.squeeze())\n",
        "        out_of_sample_corrs[g] = np.corrcoef(t, p)[0, 1]\n",
        "    out_of_sample_scores = dict(zip(all_pred_genes, pred_out_of_sample.X.std(axis=0)))\n",
        "    \n",
        "    # Calculate in-sample metrics\n",
        "    fold_to_in_sample_corrs = {}\n",
        "    fold_to_in_sample_scores = {}\n",
        "    for z in range(4):\n",
        "        in_sample_corrs = {}\n",
        "        in_sample_scores = {}\n",
        "        for g in common_in_sample:\n",
        "            t = np.array(ground_truth_in_sample[z][:, g].X.squeeze())\n",
        "            p = np.array(pred_in_sample[z][:, g].X.squeeze())\n",
        "            in_sample_corrs[g] = np.corrcoef(t, p)[0, 1]\n",
        "        in_sample_scores = dict(zip(all_pred_genes, pred_in_sample[z].X.std(axis=0)))\n",
        "        fold_to_in_sample_corrs[z] = in_sample_corrs\n",
        "        fold_to_in_sample_scores[z] = in_sample_scores\n",
        "    \n",
        "    # Calculate fold proportions\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_in_sample[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_in_sample[z].shape[0] / total_cells\n",
        "    \n",
        "    # Calculate average in-sample metrics\n",
        "    avg_in_sample_corrs = {}\n",
        "    avg_in_sample_scores = {}\n",
        "    for g in common_in_sample:\n",
        "        avg_in_sample_corrs[g] = sum(\n",
        "            fold_to_in_sample_corrs[z][g] * fold_to_prop[z] \n",
        "            for z in range(4)\n",
        "        )\n",
        "    for g in all_pred_genes:\n",
        "        avg_in_sample_scores[g] = sum(\n",
        "            fold_to_in_sample_scores[z][g] * fold_to_prop[z] \n",
        "            for z in range(4)\n",
        "        )\n",
        "    \n",
        "    return (\n",
        "        out_of_sample_corrs,\n",
        "        out_of_sample_scores,\n",
        "        avg_in_sample_corrs,\n",
        "        avg_in_sample_scores,\n",
        "        common_out_of_sample,\n",
        "        common_in_sample\n",
        "    )\n",
        "\n",
        "def calculate_program_metrics(\n",
        "    pred_in_sample, ground_truth_in_sample,\n",
        "    pred_out_of_sample, ground_truth_out_of_sample,\n",
        "    common_in_sample, common_out_of_sample,\n",
        "    all_programs\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate program-level metrics.\n",
        "    \n",
        "    Args:\n",
        "    - pred_in_sample: Dict of predicted in-sample data\n",
        "    - ground_truth_in_sample: Dict of ground truth in-sample data\n",
        "    - pred_out_of_sample: Predicted out-of-sample data\n",
        "    - ground_truth_out_of_sample: Ground truth out-of-sample data\n",
        "    - common_in_sample: Common genes for in-sample data\n",
        "    - common_out_of_sample: Common genes for out-of-sample data\n",
        "    - all_programs: Dict of gene programs\n",
        "    \n",
        "    Returns:\n",
        "    - Tuple: Program-level metrics\n",
        "    \"\"\"\n",
        "    # Calculate fold proportions\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_in_sample[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_in_sample[z].shape[0] / total_cells\n",
        "    \n",
        "    # Calculate in-sample program metrics\n",
        "    gene_set_to_avg_in_sample_program_corrs = {}\n",
        "    gene_set_to_avg_in_sample_program_scores = {}\n",
        "    \n",
        "    for program_name, program_genes in all_programs.items():\n",
        "        program_genes = [g.lower() for g in program_genes]\n",
        "        program_genes = np.intersect1d(program_genes, common_in_sample)\n",
        "        \n",
        "        if len(program_genes) < 5:\n",
        "            continue\n",
        "        \n",
        "        fold_to_program_corrs = {}\n",
        "        fold_to_program_scores = {}\n",
        "        \n",
        "        for fold in range(4):\n",
        "            # Calculate program expression\n",
        "            true_program = ground_truth_in_sample[fold][:, program_genes].X.mean(axis=1)\n",
        "            pred_program = pred_in_sample[fold][:, program_genes].X.mean(axis=1)\n",
        "            \n",
        "            # Calculate correlation and score\n",
        "            fold_to_program_corrs[fold] = np.corrcoef(true_program, pred_program)[0, 1]\n",
        "            fold_to_program_scores[fold] = pred_program.std()\n",
        "        \n",
        "        # Calculate weighted averages\n",
        "        gene_set_to_avg_in_sample_program_corrs[program_name] = sum(\n",
        "            fold_to_program_corrs[z] * fold_to_prop[z] \n",
        "            for z in range(4)\n",
        "        )\n",
        "        gene_set_to_avg_in_sample_program_scores[program_name] = sum(\n",
        "            fold_to_program_scores[z] * fold_to_prop[z] \n",
        "            for z in range(4)\n",
        "        )\n",
        "    \n",
        "    # Calculate out-of-sample program metrics\n",
        "    gene_set_to_out_of_sample_program_corrs = {}\n",
        "    gene_set_to_out_of_sample_program_scores = {}\n",
        "    \n",
        "    for program_name, program_genes in all_programs.items():\n",
        "        program_genes = [g.lower() for g in program_genes]\n",
        "        program_genes = np.intersect1d(program_genes, common_out_of_sample)\n",
        "        \n",
        "        if len(program_genes) < 5:\n",
        "            continue\n",
        "        \n",
        "        # Calculate program expression\n",
        "        true_program = ground_truth_out_of_sample[:, program_genes].X.mean(axis=1)\n",
        "        pred_program = pred_out_of_sample[:, program_genes].X.mean(axis=1)\n",
        "        \n",
        "        # Calculate correlation and score\n",
        "        gene_set_to_out_of_sample_program_corrs[program_name] = np.corrcoef(\n",
        "            true_program, pred_program\n",
        "        )[0, 1]\n",
        "        gene_set_to_out_of_sample_program_scores[program_name] = pred_program.std()\n",
        "    \n",
        "    return (\n",
        "        gene_set_to_avg_in_sample_program_corrs,\n",
        "        gene_set_to_avg_in_sample_program_scores,\n",
        "        gene_set_to_out_of_sample_program_corrs,\n",
        "        gene_set_to_out_of_sample_program_scores\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlations and scores\n",
        "def calculate_correlations_and_scores(ground_truth_out_of_sample, pred_out_of_sample, ground_truth_in_sample, pred_in_sample):\n",
        "    # Calculate out-of-sample correlations and scores\n",
        "    common_out_of_sample = np.intersect1d(ground_truth_out_of_sample.var.index, pred_out_of_sample.var.index)\n",
        "    common_in_sample = np.intersect1d(ground_truth_in_sample[0].var.index, pred_in_sample[0].var.index)\n",
        "    all_pred_genes = list(pred_in_sample[0].var.index)\n",
        "    \n",
        "    out_of_sample_corrs = {}\n",
        "    out_of_sample_scores = {}\n",
        "    for g in common_out_of_sample:\n",
        "        t = np.array(ground_truth_out_of_sample[::,g].X.squeeze())\n",
        "        p = np.array(pred_out_of_sample[::,g].X.squeeze())\n",
        "        out_of_sample_corrs[g] = np.corrcoef(t, p)[0, 1]\n",
        "        out_of_sample_scores[g] = p.std()\n",
        "    out_of_sample_scores = dict(zip(all_pred_genes, pred_out_of_sample.X.std(axis=0)))\n",
        "    \n",
        "    # Calculate in-sample correlations and scores\n",
        "    fold_to_in_sample_corrs = {}\n",
        "    fold_to_in_sample_scores = {}\n",
        "    for z in range(4):\n",
        "        in_sample_corrs = {}\n",
        "        in_sample_scores = {}\n",
        "        for g in common_in_sample:\n",
        "            t = np.array(ground_truth_in_sample[z][::,g].X.squeeze())\n",
        "            p = np.array(pred_in_sample[z][::,g].X.squeeze())\n",
        "            in_sample_corrs[g] = np.corrcoef(t, p)[0, 1]\n",
        "            in_sample_scores[g] = p.std()\n",
        "        in_sample_scores = dict(zip(all_pred_genes, pred_in_sample[z].X.std(axis=0)))\n",
        "        fold_to_in_sample_corrs[z] = in_sample_corrs\n",
        "        fold_to_in_sample_scores[z] = in_sample_scores\n",
        "    \n",
        "    # Calculate fold proportions and averages\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_in_sample[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_in_sample[z].shape[0] / total_cells\n",
        "    \n",
        "    avg_in_sample_corrs = {}\n",
        "    avg_in_sample_scores = {}\n",
        "    for g in common_in_sample:\n",
        "        avg_in_sample_corrs[g] = sum(fold_to_in_sample_corrs[z][g] * fold_to_prop[z] for z in range(4))\n",
        "        avg_in_sample_scores[g] = sum(fold_to_in_sample_scores[z][g] * fold_to_prop[z] for z in range(4))\n",
        "    for g in all_pred_genes:\n",
        "        avg_in_sample_scores[g] = sum(fold_to_in_sample_scores[z][g] * fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    return (out_of_sample_corrs, out_of_sample_scores, avg_in_sample_corrs, avg_in_sample_scores,\n",
        "            common_out_of_sample, common_in_sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process programs\n",
        "def load_programs():\n",
        "    # Load hallmark programs\n",
        "    with open('/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/final_new_schaf_start_jan2324/hallmark_programs.json') as f:\n",
        "        hallmark_programs = json.load(f)\n",
        "    \n",
        "    # Load cancer programs\n",
        "    with open('/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/final_new_schaf_start_jan2324/cancer_programs.json') as f:\n",
        "        cancer_programs = json.load(f)\n",
        "    \n",
        "    # Process programs\n",
        "    hallmark_programs = {k: v['geneSymbols'] for k, v in hallmark_programs.items()}\n",
        "    cancer_programs = {k: v['geneSymbols'] for k, v in cancer_programs.items()}\n",
        "    \n",
        "    # Convert gene symbols to lowercase\n",
        "    hallmark_programs = {k: [g.lower() for g in v] for k, v in hallmark_programs.items()}\n",
        "    cancer_programs = {k: [g.lower() for g in v] for k, v in cancer_programs.items()}\n",
        "    \n",
        "    # Combine programs\n",
        "    all_programs = hallmark_programs.copy()\n",
        "    all_programs.update(cancer_programs)\n",
        "    \n",
        "    return all_programs\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Create output directory\n",
        "    os.makedirs('final_figures_schaf_revision_pngs', exist_ok=True)\n",
        "    \n",
        "    # Load data\n",
        "    print(\"Loading Visium data...\")\n",
        "    visium_adata = load_visium_data()\n",
        "    \n",
        "    print(\"Loading ground truth and prediction data...\")\n",
        "    ground_truth_out_of_sample, pred_out_of_sample, ground_truth_in_sample, pred_in_sample = load_ground_truth_and_predictions()\n",
        "    \n",
        "    # Calculate correlations and scores\n",
        "    print(\"Calculating correlations and scores...\")\n",
        "    results = calculate_correlations_and_scores(\n",
        "        ground_truth_out_of_sample, pred_out_of_sample, \n",
        "        ground_truth_in_sample, pred_in_sample\n",
        "    )\n",
        "    out_of_sample_corrs, out_of_sample_scores, avg_in_sample_corrs, avg_in_sample_scores, common_out_of_sample, common_in_sample = results\n",
        "    \n",
        "    # Load programs\n",
        "    print(\"Loading and processing programs...\")\n",
        "    all_programs = load_programs()\n",
        "    \n",
        "    # Calculate program metrics\n",
        "    print(\"Calculating program metrics...\")\n",
        "    program_results = calculate_program_metrics(\n",
        "        pred_in_sample, ground_truth_in_sample,\n",
        "        pred_out_of_sample, ground_truth_out_of_sample,\n",
        "        common_in_sample, common_out_of_sample,\n",
        "        all_programs\n",
        "    )\n",
        "    gene_set_to_avg_in_sample_program_corrs, gene_set_to_avg_in_sample_program_scores, gene_set_to_out_of_sample_program_corrs, gene_set_to_out_of_sample_program_scores = program_results\n",
        "    \n",
        "    # Generate correlation heatmaps\n",
        "    print(\"Generating correlation heatmaps...\")\n",
        "    generate_heatmaps(\n",
        "        pred_in_sample, ground_truth_in_sample,\n",
        "        pred_out_of_sample, ground_truth_out_of_sample,\n",
        "        common_in_sample, common_out_of_sample\n",
        "    )\n",
        "    \n",
        "    # Load and process mouse data\n",
        "    print(\"Loading mouse data...\")\n",
        "    pred_mouse, ground_truth_mouse, pred_mouse_scores = load_mouse_data()\n",
        "    \n",
        "    \n",
        "    # Calculate mouse metrics\n",
        "    print(\"Calculating mouse metrics...\")\n",
        "    mouse_avg_in_sample_corrs, mouse_avg_in_sample_scores, common_mouse = calculate_mouse_metrics(\n",
        "        pred_mouse, ground_truth_mouse, pred_mouse_scores\n",
        "    )\n",
        "    \n",
        "    # Generate mouse heatmaps\n",
        "    print(\"Generating mouse heatmaps...\")\n",
        "    generate_mouse_heatmaps(pred_mouse, ground_truth_mouse, common_mouse)\n",
        "    \n",
        "    # Load cell type labels\n",
        "    print(\"Loading cell type labels...\")\n",
        "    broad_clusters, xenium_in_sample_fold_to_inferred_labels, mouse_fold_to_inferred_labels = load_cell_type_labels()\n",
        "    \n",
        "    # Calculate cell type-specific expression\n",
        "    print(\"Calculating cell type-specific expression...\")\n",
        "    cell_type_results = calculate_cell_type_expression(\n",
        "        pred_in_sample, ground_truth_in_sample,\n",
        "        pred_out_of_sample, ground_truth_out_of_sample,\n",
        "        pred_mouse, ground_truth_mouse,\n",
        "        common_in_sample, common_mouse, common_out_of_sample,\n",
        "        broad_clusters, xenium_in_sample_fold_to_inferred_labels, mouse_fold_to_inferred_labels\n",
        "    )\n",
        "    fold_to_ct_to_mouse_pred_mean, fold_to_ct_to_mouse_tru_mean, fold_to_ct_to_in_sample_pred_mean, fold_to_ct_to_in_sample_tru_mean, ct_to_out_of_sample_pred_mean, ct_to_out_of_sample_tru_mean = cell_type_results\n",
        "    \n",
        "    # Generate cell type expression heatmaps\n",
        "    print(\"Generating cell type expression heatmaps...\")\n",
        "    generate_cell_type_heatmaps(\n",
        "        fold_to_ct_to_mouse_pred_mean, fold_to_ct_to_mouse_tru_mean,\n",
        "        fold_to_ct_to_in_sample_pred_mean, fold_to_ct_to_in_sample_tru_mean,\n",
        "        ct_to_out_of_sample_pred_mean, ct_to_out_of_sample_tru_mean\n",
        "    )\n",
        "    \n",
        "    # Calculate cell type-specific correlations\n",
        "    print(\"Calculating cell type-specific correlations...\")\n",
        "    dataset_to_ct_corrs_info = calculate_cell_type_correlations(\n",
        "        pred_in_sample, ground_truth_in_sample,\n",
        "        pred_out_of_sample, ground_truth_out_of_sample,\n",
        "        pred_mouse, ground_truth_mouse,\n",
        "        common_in_sample, common_mouse, common_out_of_sample,\n",
        "        ground_truth_out_of_sample_for_cts, pred_out_of_sample_for_cts\n",
        "    )\n",
        "    \n",
        "    # Generate cell type correlation plots\n",
        "    print(\"Generating cell type correlation plots...\")\n",
        "    generate_cell_type_correlation_plots(dataset_to_ct_corrs_info)\n",
        "    \n",
        "    # Generate cell type heterogeneity plots\n",
        "    print(\"Generating cell type heterogeneity plots...\")\n",
        "    generate_cell_type_heterogeneity_plots(dataset_to_ct_corrs_info)\n",
        "    \n",
        "    # Calculate cell type metacorrelations\n",
        "    print(\"Calculating cell type metacorrelations...\")\n",
        "    label_to_in_sample_plot_metacorr, label_to_mouse_plot_metacorr, label_to_out_of_sample_plot_metacorr = calculate_cell_type_metacorrelations(\n",
        "        label_to_ground_truth_in_sample, label_to_pred_in_sample,\n",
        "        label_to_ground_truth_mouse, label_to_pred_mouse,\n",
        "        label_to_ground_truth_out_of_sample, label_to_pred_out_of_sample,\n",
        "        common_in_sample, common_mouse, common_out_of_sample\n",
        "    )\n",
        "    \n",
        "    # Generate metacorrelation plots\n",
        "    print(\"Generating metacorrelation plots...\")\n",
        "    generate_metacorrelation_plots(\n",
        "        label_to_in_sample_plot_metacorr,\n",
        "        label_to_mouse_plot_metacorr,\n",
        "        label_to_out_of_sample_plot_metacorr\n",
        "    )\n",
        "    \n",
        "    # Calculate cell type heterogeneity\n",
        "    print(\"Calculating cell type heterogeneity...\")\n",
        "    label_to_avg_in_sample_heteros, label_to_avg_mouse_heteros, label_to_heteros_out_of_sample = calculate_cell_type_heterogeneity(\n",
        "        label_to_ground_truth_in_sample, label_to_pred_in_sample,\n",
        "        label_to_ground_truth_mouse, label_to_pred_mouse,\n",
        "        label_to_ground_truth_out_of_sample, label_to_pred_out_of_sample,\n",
        "        common_in_sample, common_mouse, common_out_of_sample\n",
        "    )\n",
        "    \n",
        "    # Generate histology split figures\n",
        "    print(\"Generating histology split figures...\")\n",
        "    generate_histology_split_figures(\n",
        "        in_sample_fold_to_hist,\n",
        "        mouse_fold_to_hist,\n",
        "        in_sample_hist,\n",
        "        out_of_sample\n",
        "    )\n",
        "    \n",
        "    # Generate correlation histograms\n",
        "    print(\"Generating correlation histograms...\")\n",
        "    generate_correlation_histograms(\n",
        "        mouse_avg_in_sample_corrs,\n",
        "        mouse_train_genes\n",
        "    )\n",
        "    \n",
        "    # Generate HTAPP and MSKCC schematics\n",
        "    print(\"Generating HTAPP and MSKCC schematics...\")\n",
        "    generate_htapp_schematic(the_hists)  # Note: the_hists needs to be defined\n",
        "    generate_mskcc_schematic()\n",
        "    \n",
        "    print(\"All figures have been generated successfully!\")\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate correlation heatmaps\n",
        "def generate_heatmaps(pred_in_sample, ground_truth_in_sample, pred_out_of_sample, ground_truth_out_of_sample,\n",
        "                     common_in_sample, common_out_of_sample):\n",
        "    # Calculate in-sample heatmaps\n",
        "    fold_to_pred_in_sample_heatmap = {}\n",
        "    fold_to_true_in_sample_heatmap = {}\n",
        "    fold_to_metacorr = {}\n",
        "    \n",
        "    for z in range(4):\n",
        "        pred_arr = np.array(pred_in_sample[z][::,common_in_sample].X)\n",
        "        true_arr = np.array(ground_truth_in_sample[z][::,common_in_sample].X.squeeze())\n",
        "        \n",
        "        pred_heatmap = np.corrcoef(pred_arr, rowvar=0)\n",
        "        true_heatmap = np.corrcoef(true_arr, rowvar=0)\n",
        "        \n",
        "        if not z:\n",
        "            hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
        "            labels = hierarchical_cluster.fit_predict(true_arr.T)\n",
        "            c1_inds = [i for i, l in enumerate(labels) if l]\n",
        "            c2_inds = [i for i, l in enumerate(labels) if not l]\n",
        "        \n",
        "        pred_heatmap = pred_heatmap[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "        true_heatmap = true_heatmap[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "        \n",
        "        fold_to_pred_in_sample_heatmap[z] = pred_heatmap\n",
        "        fold_to_true_in_sample_heatmap[z] = true_heatmap\n",
        "        fold_to_metacorr[z] = np.corrcoef(pred_heatmap.reshape(-1), true_heatmap.reshape(-1))[0, 1]\n",
        "    \n",
        "    # Calculate fold proportions\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_in_sample[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_in_sample[z].shape[0] / total_cells\n",
        "    \n",
        "    # Calculate average in-sample heatmaps\n",
        "    avg_pred_in_sample_heatmap = sum([fold_to_pred_in_sample_heatmap[z]*fold_to_prop[z] for z in range(4)])\n",
        "    avg_true_in_sample_heatmap = sum([fold_to_true_in_sample_heatmap[z]*fold_to_prop[z] for z in range(4)])\n",
        "    avg_in_sample_metacorr = sum(fold_to_metacorr[z]*fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    # Calculate out-of-sample heatmaps\n",
        "    pred_arr = np.array(pred_out_of_sample[::,common_out_of_sample].X)\n",
        "    true_arr = np.array(ground_truth_out_of_sample[::, common_out_of_sample].X)\n",
        "    \n",
        "    pred_heatmap_out_of_sample = np.corrcoef(pred_arr, rowvar=0)\n",
        "    true_heatmap_out_of_sample = np.corrcoef(true_arr, rowvar=0)\n",
        "    \n",
        "    hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
        "    labels = hierarchical_cluster.fit_predict(true_arr.T)\n",
        "    c1_inds = [i for i, l in enumerate(labels) if l]\n",
        "    c2_inds = [i for i, l in enumerate(labels) if not l]\n",
        "    \n",
        "    pred_heatmap_out_of_sample = pred_heatmap_out_of_sample[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "    true_heatmap_out_of_sample = true_heatmap_out_of_sample[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "    \n",
        "    out_of_sample_metacorr = np.corrcoef(pred_heatmap_out_of_sample.reshape(-1), true_heatmap_out_of_sample.reshape(-1))[0, 1]\n",
        "    \n",
        "    # Save heatmap figures\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(131)\n",
        "    sns.heatmap(avg_true_in_sample_heatmap, cmap='coolwarm', center=0)\n",
        "    plt.title('True In-Sample Correlation')\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    sns.heatmap(avg_pred_in_sample_heatmap, cmap='coolwarm', center=0)\n",
        "    plt.title(f'Predicted In-Sample Correlation\\nMetacorr: {avg_in_sample_metacorr:.3f}')\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    sns.heatmap(np.abs(avg_true_in_sample_heatmap - avg_pred_in_sample_heatmap), cmap='coolwarm')\n",
        "    plt.title('Absolute Difference')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/in_sample_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(131)\n",
        "    sns.heatmap(true_heatmap_out_of_sample, cmap='coolwarm', center=0)\n",
        "    plt.title('True Out-of-Sample Correlation')\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    sns.heatmap(pred_heatmap_out_of_sample, cmap='coolwarm', center=0)\n",
        "    plt.title(f'Predicted Out-of-Sample Correlation\\nMetacorr: {out_of_sample_metacorr:.3f}')\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    sns.heatmap(np.abs(true_heatmap_out_of_sample - pred_heatmap_out_of_sample), cmap='coolwarm')\n",
        "    plt.title('Absolute Difference')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/out_of_sample_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process mouse data\n",
        "def load_mouse_data():\n",
        "    # Load predictions\n",
        "    pred_mouse = {\n",
        "        z: sc.read_h5ad(f'/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/mouse_inferences/fold_{z}.h5ad')\n",
        "        for z in range(4)\n",
        "    }\n",
        "    \n",
        "    # Load ground truth\n",
        "    ground_truth_mouse = {\n",
        "        z: sc.read_h5ad(f'/mounts/stultzlab03/ccomiter/schaf_for_revision052424/data/xenium_cancer/mouse_folds/fold_{z}_st.h5ad')\n",
        "        for z in range(4)\n",
        "    }\n",
        "    \n",
        "    # Process ground truth data\n",
        "    for z in ground_truth_mouse:\n",
        "        sc.pp.log1p(ground_truth_mouse[z])\n",
        "        ground_truth_mouse[z].X = np.array(ground_truth_mouse[z].X.todense())\n",
        "    \n",
        "    # Load scores\n",
        "    pred_mouse_scores = {\n",
        "        z: np.load(f'mouse_inferences/fold_{z}_scores.npy') for z in range(4)\n",
        "    }\n",
        "    \n",
        "    return pred_mouse, ground_truth_mouse, pred_mouse_scores\n",
        "\n",
        "\n",
        "\n",
        "# Calculate mouse metrics\n",
        "def calculate_mouse_metrics(pred_mouse, ground_truth_mouse, pred_mouse_scores):\n",
        "    common_mouse = np.intersect1d(ground_truth_mouse[0].var.index, pred_mouse[0].var.index)\n",
        "    all_pred_genes_mouse = list(pred_mouse[0].var.index)\n",
        "    \n",
        "    # Calculate correlations and scores\n",
        "    mouse_fold_to_in_sample_corrs = {}\n",
        "    mouse_fold_to_in_sample_scores = {}\n",
        "    for z in range(4):\n",
        "        in_sample_corrs = {}\n",
        "        in_sample_scores = {}\n",
        "        for g in common_mouse:\n",
        "            t = np.array(ground_truth_mouse[z][::,g].X.squeeze())\n",
        "            p = np.array(pred_mouse[z][::,g].X.squeeze())\n",
        "            in_sample_corrs[g] = np.corrcoef(t, p)[0, 1]\n",
        "            in_sample_scores[g] = p.std()\n",
        "        in_sample_scores = dict(zip(all_pred_genes_mouse, pred_mouse_scores[z]))\n",
        "        mouse_fold_to_in_sample_corrs[z] = in_sample_corrs\n",
        "        mouse_fold_to_in_sample_scores[z] = in_sample_scores\n",
        "    \n",
        "    # Calculate fold proportions\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_mouse[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_mouse[z].shape[0] / total_cells\n",
        "    \n",
        "    # Calculate averages\n",
        "    mouse_avg_in_sample_corrs = {}\n",
        "    mouse_avg_in_sample_scores = {}\n",
        "    for g in common_mouse:\n",
        "        mouse_avg_in_sample_corrs[g] = sum(mouse_fold_to_in_sample_corrs[z][g] * fold_to_prop[z] for z in range(4))\n",
        "        mouse_avg_in_sample_scores[g] = sum(mouse_fold_to_in_sample_scores[z][g] * fold_to_prop[z] for z in range(4))\n",
        "    for g in all_pred_genes_mouse:\n",
        "        mouse_avg_in_sample_scores[g] = sum(mouse_fold_to_in_sample_scores[z][g] * fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    return mouse_avg_in_sample_corrs, mouse_avg_in_sample_scores, common_mouse\n",
        "\n",
        "# Generate mouse heatmaps\n",
        "def generate_mouse_heatmaps(pred_mouse, ground_truth_mouse, common_mouse):\n",
        "    fold_to_pred_mouse_heatmap = {}\n",
        "    fold_to_true_mouse_heatmap = {}\n",
        "    fold_to_metacorr = {}\n",
        "    \n",
        "    for z in range(4):\n",
        "        pred_arr = np.array(pred_mouse[z][::,common_mouse].X)\n",
        "        true_arr = np.array(ground_truth_mouse[z][::,common_mouse].X.squeeze())\n",
        "        \n",
        "        pred_heatmap = np.corrcoef(pred_arr, rowvar=0)\n",
        "        true_heatmap = np.corrcoef(true_arr, rowvar=0)\n",
        "        \n",
        "        if not z:\n",
        "            hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
        "            labels = hierarchical_cluster.fit_predict(true_arr.T)\n",
        "            c1_inds = [i for i, l in enumerate(labels) if l]\n",
        "            c2_inds = [i for i, l in enumerate(labels) if not l]\n",
        "        \n",
        "        pred_heatmap = pred_heatmap[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "        true_heatmap = true_heatmap[c1_inds+c2_inds][::,c1_inds+c2_inds]\n",
        "        \n",
        "        fold_to_pred_mouse_heatmap[z] = pred_heatmap\n",
        "        fold_to_true_mouse_heatmap[z] = true_heatmap\n",
        "        fold_to_metacorr[z] = np.corrcoef(pred_heatmap.reshape(-1), true_heatmap.reshape(-1))[0, 1]\n",
        "    \n",
        "    # Calculate fold proportions\n",
        "    fold_to_prop = {}\n",
        "    total_cells = float(sum(ground_truth_mouse[z].shape[0] for z in range(4)))\n",
        "    for z in range(4):\n",
        "        fold_to_prop[z] = ground_truth_mouse[z].shape[0] / total_cells\n",
        "    \n",
        "    # Calculate average heatmaps\n",
        "    avg_pred_mouse_heatmap = sum([fold_to_pred_mouse_heatmap[z]*fold_to_prop[z] for z in range(4)])\n",
        "    avg_true_mouse_heatmap = sum([fold_to_true_mouse_heatmap[z]*fold_to_prop[z] for z in range(4)])\n",
        "    avg_mouse_metacorr = sum(fold_to_metacorr[z]*fold_to_prop[z] for z in range(4))\n",
        "    \n",
        "    # Generate heatmap figure\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(131)\n",
        "    sns.heatmap(avg_true_mouse_heatmap, cmap='coolwarm', center=0)\n",
        "    plt.title('True Mouse Correlation')\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    sns.heatmap(avg_pred_mouse_heatmap, cmap='coolwarm', center=0)\n",
        "    plt.title(f'Predicted Mouse Correlation\\nMetacorr: {avg_mouse_metacorr:.3f}')\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    sns.heatmap(np.abs(avg_true_mouse_heatmap - avg_pred_mouse_heatmap), cmap='coolwarm')\n",
        "    plt.title('Absolute Difference')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/mouse_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process cell type labels\n",
        "def load_cell_type_labels():\n",
        "    # Load broad clusters\n",
        "    broad_clusters = pd.read_csv('/mounts/stultzlab03/ccomiter/htapp_supervise/new_schaf_experiment_scripts/more_data/xenium/analysis/clustering/gene_expression_kmeans_10_clusters/clusters.csv')\n",
        "    broad_clusters = broad_clusters.set_index('Barcode')\n",
        "    \n",
        "    # Load inferred labels\n",
        "    xenium_in_sample_fold_to_inferred_labels = {}\n",
        "    mouse_fold_to_inferred_labels = {}\n",
        "    for f in range(4):\n",
        "        xenium_in_sample_fold_to_inferred_labels[f] = np.load(f'cancer_fold_to_new_labels/{f}.npy')\n",
        "        mouse_fold_to_inferred_labels[f] = np.load(f'mouse_fold_to_new_labels/{f}.npy')\n",
        "    \n",
        "    return broad_clusters, xenium_in_sample_fold_to_inferred_labels, mouse_fold_to_inferred_labels\n",
        "\n",
        "# Calculate cell type-specific expression\n",
        "def calculate_cell_type_expression(pred_in_sample, ground_truth_in_sample, pred_out_of_sample, ground_truth_out_of_sample,\n",
        "                                 pred_mouse, ground_truth_mouse, common_in_sample, common_mouse, common_out_of_sample,\n",
        "                                 broad_clusters, xenium_in_sample_fold_to_inferred_labels, mouse_fold_to_inferred_labels):\n",
        "    # Process out-of-sample data with broad clusters\n",
        "    ground_truth_out_of_sample_for_cts = ground_truth_out_of_sample[broad_clusters.index]\n",
        "    pred_out_of_sample_for_cts = pred_out_of_sample[broad_clusters.index]\n",
        "    ground_truth_out_of_sample_for_cts.obs['broad_clusters'] = np.array(broad_clusters['Cluster'])\n",
        "    \n",
        "    # Get true and predicted labels\n",
        "    ostl = np.array(ground_truth_out_of_sample_for_cts.obs['broad_clusters']).astype(int)\n",
        "    ospl = np.load('cancer_fold_to_new_labels/out_of_sample.npy').astype(int)\n",
        "    \n",
        "    # Process in-sample labels\n",
        "    isftl = {}\n",
        "    isfpl = {}\n",
        "    for i in range(4):\n",
        "        isfpl[i] = np.load(f'cancer_fold_to_new_labels/{i}.npy').astype(int)\n",
        "        isftl[i] = np.array(ground_truth_in_sample[i].obs['broad_clusters']).astype(int)\n",
        "    \n",
        "    # Process mouse labels\n",
        "    mftl = {}\n",
        "    mfpl = {}\n",
        "    for i in range(4):\n",
        "        mfpl[i] = np.load(f'mouse_fold_to_new_labels/{i}.npy').astype(int)\n",
        "        mftl[i] = np.array(ground_truth_mouse[i].obs['broad_clusters']).astype(int)\n",
        "    \n",
        "    # Calculate overall means and standard deviations\n",
        "    num_genes = len(common_in_sample)\n",
        "    all_means = np.zeros(num_genes)\n",
        "    total_samples = 0\n",
        "    for k, v in ground_truth_in_sample.items():\n",
        "        all_means = all_means + v.shape[0]*(ground_truth_in_sample[k][::,common_in_sample].X.mean(axis=0))\n",
        "        total_samples += v.shape[0]\n",
        "    all_means = all_means / total_samples\n",
        "    \n",
        "    all_vars = np.zeros(num_genes)\n",
        "    for k, v in ground_truth_in_sample.items():\n",
        "        n = v.shape[0]\n",
        "        all_vars += v.shape[0]*(ground_truth_in_sample[k][::,common_in_sample].X.var(axis=0) + \n",
        "                               (ground_truth_in_sample[k][::,common_in_sample].X.mean(axis=0) - all_means)**2)\n",
        "    all_vars = all_vars / total_samples\n",
        "    all_stds = (all_vars)**.5\n",
        "    \n",
        "    # Calculate cell type-specific expression\n",
        "    cts = list(range(1, 11, 1))\n",
        "    \n",
        "    # Mouse cell type expression\n",
        "    fold_to_ct_to_mouse_pred = {}\n",
        "    fold_to_ct_to_mouse_tru = {}\n",
        "    fold_to_ct_to_mouse_pred_mean = {}\n",
        "    fold_to_ct_to_mouse_tru_mean = {}\n",
        "    \n",
        "    for z in range(4):\n",
        "        ct_to_mouse_pred = {}\n",
        "        ct_to_mouse_tru = {}\n",
        "        for ct in cts:\n",
        "            ct_to_mouse_pred[ct] = pred_mouse[z][mfpl[z]==ct,common_mouse]\n",
        "            ct_to_mouse_tru[ct] = ground_truth_mouse[z][mftl[z]==ct,common_mouse]\n",
        "        fold_to_ct_to_mouse_pred[z] = ct_to_mouse_pred\n",
        "        fold_to_ct_to_mouse_tru[z] = ct_to_mouse_tru\n",
        "        \n",
        "        fold_to_ct_to_mouse_pred_mean[z] = {}\n",
        "        fold_to_ct_to_mouse_tru_mean[z] = {}\n",
        "        for ct in cts:\n",
        "            fold_to_ct_to_mouse_pred_mean[z][ct] = fold_to_ct_to_mouse_pred[z][ct].X.mean(axis=0)\n",
        "            fold_to_ct_to_mouse_tru_mean[z][ct] = fold_to_ct_to_mouse_tru[z][ct].X.mean(axis=0)\n",
        "    \n",
        "    # In-sample cell type expression\n",
        "    fold_to_ct_to_in_sample_pred = {}\n",
        "    fold_to_ct_to_in_sample_tru = {}\n",
        "    fold_to_ct_to_in_sample_pred_mean = {}\n",
        "    fold_to_ct_to_in_sample_tru_mean = {}\n",
        "    \n",
        "    for z in range(4):\n",
        "        ct_to_in_sample_pred = {}\n",
        "        ct_to_in_sample_tru = {}\n",
        "        for ct in cts:\n",
        "            ct_to_in_sample_pred[ct] = pred_in_sample[z][isfpl[z]==ct,common_in_sample]\n",
        "            ct_to_in_sample_tru[ct] = ground_truth_in_sample[z][isftl[z]==ct,common_in_sample]\n",
        "        fold_to_ct_to_in_sample_pred[z] = ct_to_in_sample_pred\n",
        "        fold_to_ct_to_in_sample_tru[z] = ct_to_in_sample_tru\n",
        "        \n",
        "        fold_to_ct_to_in_sample_pred_mean[z] = {}\n",
        "        fold_to_ct_to_in_sample_tru_mean[z] = {}\n",
        "        for ct in cts:\n",
        "            fold_to_ct_to_in_sample_pred_mean[z][ct] = (all_means+(all_stds*fold_to_ct_to_in_sample_pred[z][ct].X)).mean(axis=0)\n",
        "            fold_to_ct_to_in_sample_tru_mean[z][ct] = (all_means+(all_stds*fold_to_ct_to_in_sample_tru[z][ct].X)).mean(axis=0)\n",
        "    \n",
        "    # Out-of-sample cell type expression\n",
        "    ct_to_out_of_sample_pred = {}\n",
        "    ct_to_out_of_sample_tru = {}\n",
        "    ct_to_out_of_sample_pred_mean = {}\n",
        "    ct_to_out_of_sample_tru_mean = {}\n",
        "    \n",
        "    for ct in cts:\n",
        "        ct_to_out_of_sample_pred[ct] = pred_out_of_sample_for_cts[ospl==ct,common_out_of_sample]\n",
        "        ct_to_out_of_sample_tru[ct] = ground_truth_out_of_sample_for_cts[ostl==ct,common_out_of_sample]\n",
        "        ct_to_out_of_sample_pred_mean[ct] = ct_to_out_of_sample_pred[ct].X.mean(axis=0)\n",
        "        ct_to_out_of_sample_tru_mean[ct] = ct_to_out_of_sample_tru[ct].X.mean(axis=0)\n",
        "    \n",
        "    return (fold_to_ct_to_mouse_pred_mean, fold_to_ct_to_mouse_tru_mean,\n",
        "            fold_to_ct_to_in_sample_pred_mean, fold_to_ct_to_in_sample_tru_mean,\n",
        "            ct_to_out_of_sample_pred_mean, ct_to_out_of_sample_tru_mean)\n",
        "\n",
        "# Generate cell type expression heatmaps\n",
        "def generate_cell_type_heatmaps(fold_to_ct_to_mouse_pred_mean, fold_to_ct_to_mouse_tru_mean,\n",
        "                               fold_to_ct_to_in_sample_pred_mean, fold_to_ct_to_in_sample_tru_mean,\n",
        "                               ct_to_out_of_sample_pred_mean, ct_to_out_of_sample_tru_mean):\n",
        "    cts = list(range(1, 11, 1))\n",
        "    \n",
        "    # Generate mouse heatmap\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    plt.subplot(131)\n",
        "    mouse_true_matrix = np.array([fold_to_ct_to_mouse_tru_mean[0][ct] for ct in cts])\n",
        "    sns.heatmap(mouse_true_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('True Mouse Cell Type Expression')\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    mouse_pred_matrix = np.array([fold_to_ct_to_mouse_pred_mean[0][ct] for ct in cts])\n",
        "    sns.heatmap(mouse_pred_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('Predicted Mouse Cell Type Expression')\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    sns.heatmap(np.abs(mouse_true_matrix - mouse_pred_matrix), cmap='coolwarm')\n",
        "    plt.title('Absolute Difference')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/mouse_cell_type_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # Generate in-sample heatmap\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    plt.subplot(131)\n",
        "    in_sample_true_matrix = np.array([fold_to_ct_to_in_sample_tru_mean[0][ct] for ct in cts])\n",
        "    sns.heatmap(in_sample_true_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('True In-Sample Cell Type Expression')\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    in_sample_pred_matrix = np.array([fold_to_ct_to_in_sample_pred_mean[0][ct] for ct in cts])\n",
        "    sns.heatmap(in_sample_pred_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('Predicted In-Sample Cell Type Expression')\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    sns.heatmap(np.abs(in_sample_true_matrix - in_sample_pred_matrix), cmap='coolwarm')\n",
        "    plt.title('Absolute Difference')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/in_sample_cell_type_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # Generate out-of-sample heatmap\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    plt.subplot(131)\n",
        "    out_sample_true_matrix = np.array([ct_to_out_of_sample_tru_mean[ct] for ct in cts])\n",
        "    sns.heatmap(out_sample_true_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('True Out-of-Sample Cell Type Expression')\n",
        "    \n",
        "    plt.subplot(132)\n",
        "    out_sample_pred_matrix = np.array([ct_to_out_of_sample_pred_mean[ct] for ct in cts])\n",
        "    sns.heatmap(out_sample_pred_matrix, cmap='coolwarm', center=0)\n",
        "    plt.title('Predicted Out-of-Sample Cell Type Expression')\n",
        "    \n",
        "    plt.subplot(133)\n",
        "    sns.heatmap(np.abs(out_sample_true_matrix - out_sample_pred_matrix), cmap='coolwarm')\n",
        "    plt.title('Absolute Difference')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/out_of_sample_cell_type_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate cell type-specific correlations\n",
        "def calculate_cell_type_correlations(pred_in_sample, ground_truth_in_sample, pred_out_of_sample, ground_truth_out_of_sample,\n",
        "                                   pred_mouse, ground_truth_mouse, common_in_sample, common_mouse, common_out_of_sample,\n",
        "                                   ground_truth_out_of_sample_for_cts, pred_out_of_sample_for_cts):\n",
        "    # Helper functions for label remapping\n",
        "    def do_in_sample_change(r):\n",
        "        return np.array([9 if int(x) == 10 else x for x in r])\n",
        "        \n",
        "    def do_new_sample_change(r):\n",
        "        return np.array([7 if int(x) == 8 else 1 if int(x) == 5 else x for x in r])\n",
        "    \n",
        "    # Update labels\n",
        "    for fold in range(4):\n",
        "        ground_truth_in_sample[fold].obs['broad_clusters'] = do_in_sample_change(ground_truth_in_sample[fold].obs['broad_clusters'])\n",
        "        pred_in_sample[fold].obs['broad_clusters'] = do_in_sample_change(pred_in_sample[fold].obs['broad_clusters'])\n",
        "    \n",
        "    ground_truth_out_of_sample_for_cts.obs['broad_clusters'] = do_new_sample_change(ground_truth_out_of_sample_for_cts.obs['broad_clusters'])\n",
        "    \n",
        "    # Get unique labels\n",
        "    labels = []\n",
        "    for fold in range(4):\n",
        "        labels = np.union1d(labels, sorted(set(ground_truth_in_sample[fold].obs['broad_clusters'])))\n",
        "    labels = labels.astype(int)\n",
        "    \n",
        "    # Process in-sample data by cell type\n",
        "    label_to_ground_truth_in_sample = {}\n",
        "    label_to_pred_in_sample = {}\n",
        "    label_to_pred_predicted_in_sample = {}\n",
        "    \n",
        "    for label in labels:\n",
        "        # Ground truth\n",
        "        this_dict = {}\n",
        "        for fold in range(4):\n",
        "            this_dict[fold] = ground_truth_in_sample[fold][ground_truth_in_sample[fold].obs['broad_clusters']==label]\n",
        "        label_to_ground_truth_in_sample[label] = this_dict\n",
        "        \n",
        "        # Predictions based on true labels\n",
        "        this_dict = {}\n",
        "        for fold in range(4):\n",
        "            this_dict[fold] = pred_in_sample[fold][ground_truth_in_sample[fold].obs['broad_clusters']==label]\n",
        "        label_to_pred_in_sample[label] = this_dict\n",
        "        \n",
        "        # Predictions based on predicted labels\n",
        "        this_dict = {}\n",
        "        for fold in range(4):\n",
        "            this_dict[fold] = pred_in_sample[fold][pred_in_sample[fold].obs['broad_clusters']==label]\n",
        "        label_to_pred_predicted_in_sample[label] = this_dict\n",
        "    \n",
        "    # Process mouse data by cell type\n",
        "    mouse_labels = []\n",
        "    for fold in range(4):\n",
        "        mouse_labels = np.union1d(mouse_labels, sorted(set(ground_truth_mouse[fold].obs['broad_clusters'])))\n",
        "    mouse_labels = mouse_labels.astype(int)\n",
        "    \n",
        "    label_to_ground_truth_mouse = {}\n",
        "    label_to_pred_mouse = {}\n",
        "    label_to_pred_predicted_mouse = {}\n",
        "    \n",
        "    for label in mouse_labels:\n",
        "        # Ground truth\n",
        "        this_dict = {}\n",
        "        for fold in range(4):\n",
        "            this_dict[fold] = ground_truth_mouse[fold][ground_truth_mouse[fold].obs['broad_clusters']==label]\n",
        "        label_to_ground_truth_mouse[label] = this_dict\n",
        "        \n",
        "        # Predictions based on true labels\n",
        "        this_dict = {}\n",
        "        for fold in range(4):\n",
        "            this_dict[fold] = pred_mouse[fold][ground_truth_mouse[fold].obs['broad_clusters']==label]\n",
        "        label_to_pred_mouse[label] = this_dict\n",
        "        \n",
        "        # Predictions based on predicted labels\n",
        "        this_dict = {}\n",
        "        for fold in range(4):\n",
        "            this_dict[fold] = pred_mouse[fold][pred_mouse[fold].obs['broad_clusters']==label]\n",
        "        label_to_pred_predicted_mouse[label] = this_dict\n",
        "    \n",
        "    # Calculate correlations for each cell type\n",
        "    label_to_fold_to_in_sample_corrs = {}\n",
        "    label_to_fold_to_mouse_corrs = {}\n",
        "    label_to_avg_in_sample_corrs = {}\n",
        "    label_to_avg_mouse_corrs = {}\n",
        "    \n",
        "    # In-sample correlations\n",
        "    for label in label_to_ground_truth_in_sample:\n",
        "        label_to_fold_to_in_sample_corrs[label] = {}\n",
        "        label_to_avg_in_sample_corrs[label] = {}\n",
        "        \n",
        "        for z in range(4):\n",
        "            in_sample_corrs = {}\n",
        "            for g in common_in_sample:\n",
        "                t = np.array(label_to_ground_truth_in_sample[label][z][::,g].X.squeeze())\n",
        "                p = np.array(label_to_pred_in_sample[label][z][::,g].X.squeeze())\n",
        "                in_sample_corrs[g] = (p.std(), np.nan_to_num(np.corrcoef(t, p)[0, 1]))\n",
        "            label_to_fold_to_in_sample_corrs[label][z] = in_sample_corrs\n",
        "        \n",
        "        # Calculate weighted average across folds\n",
        "        fold_to_prop = {}\n",
        "        total_cells = float(sum(label_to_ground_truth_in_sample[label][z].shape[0] for z in range(4)))\n",
        "        for z in range(4):\n",
        "            fold_to_prop[z] = label_to_ground_truth_in_sample[label][z].shape[0] / total_cells\n",
        "        \n",
        "        for g in common_in_sample:\n",
        "            label_to_avg_in_sample_corrs[label][g] = (\n",
        "                sum(label_to_fold_to_in_sample_corrs[label][z][g][0] * fold_to_prop[z] for z in range(4)),\n",
        "                sum(label_to_fold_to_in_sample_corrs[label][z][g][1] * fold_to_prop[z] for z in range(4))\n",
        "            )\n",
        "    \n",
        "    # Mouse correlations\n",
        "    for label in label_to_ground_truth_mouse:\n",
        "        label_to_fold_to_mouse_corrs[label] = {}\n",
        "        label_to_avg_mouse_corrs[label] = {}\n",
        "        \n",
        "        for z in range(4):\n",
        "            mouse_corrs = {}\n",
        "            for g in common_mouse:\n",
        "                t = np.array(label_to_ground_truth_mouse[label][z][::,g].X.squeeze())\n",
        "                p = np.array(label_to_pred_mouse[label][z][::,g].X.squeeze())\n",
        "                mouse_corrs[g] = (p.std(), np.nan_to_num(np.corrcoef(t, p)[0, 1]))\n",
        "            label_to_fold_to_mouse_corrs[label][z] = mouse_corrs\n",
        "        \n",
        "        # Calculate weighted average across folds\n",
        "        fold_to_prop = {}\n",
        "        total_cells = float(sum(label_to_ground_truth_mouse[label][z].shape[0] for z in range(4)))\n",
        "        for z in range(4):\n",
        "            fold_to_prop[z] = label_to_ground_truth_mouse[label][z].shape[0] / total_cells\n",
        "        \n",
        "        for g in common_mouse:\n",
        "            label_to_avg_mouse_corrs[label][g] = (\n",
        "                sum(label_to_fold_to_mouse_corrs[label][z][g][0] * fold_to_prop[z] for z in range(4)),\n",
        "                sum(label_to_fold_to_mouse_corrs[label][z][g][1] * fold_to_prop[z] for z in range(4))\n",
        "            )\n",
        "    \n",
        "    # Out-of-sample correlations\n",
        "    out_sample_labels = []\n",
        "    for fold in range(4):\n",
        "        out_sample_labels = np.union1d(out_sample_labels, sorted(set(ground_truth_out_of_sample_for_cts.obs['broad_clusters'])))\n",
        "    out_sample_labels = out_sample_labels.astype(int)\n",
        "    \n",
        "    label_to_ground_truth_out_of_sample = {}\n",
        "    label_to_pred_out_of_sample = {}\n",
        "    \n",
        "    for label in out_sample_labels:\n",
        "        label_to_ground_truth_out_of_sample[label] = ground_truth_out_of_sample_for_cts[ground_truth_out_of_sample_for_cts.obs['broad_clusters']==label]\n",
        "        label_to_pred_out_of_sample[label] = pred_out_of_sample_for_cts[ground_truth_out_of_sample_for_cts.obs['broad_clusters']==label]\n",
        "    \n",
        "    # Calculate out-of-sample correlations\n",
        "    label_to_corrs_out_of_sample = {}\n",
        "    for label in label_to_ground_truth_out_of_sample:\n",
        "        label_to_corrs_out_of_sample[label] = {}\n",
        "        for g in common_out_of_sample:\n",
        "            t = np.array(label_to_ground_truth_out_of_sample[label][::,g].X.squeeze())\n",
        "            p = np.array(label_to_pred_out_of_sample[label][::,g].X.squeeze())\n",
        "            label_to_corrs_out_of_sample[label][g] = (p.std(), np.nan_to_num(np.corrcoef(t, p)[0, 1]))\n",
        "    \n",
        "    # Prepare summary statistics\n",
        "    dataset_to_ct_corrs_info = {}\n",
        "    dataset_to_all_info = {\n",
        "        'mouse': label_to_avg_mouse_corrs,\n",
        "        'in_sample': label_to_avg_in_sample_corrs,\n",
        "        'out_of_sample': label_to_corrs_out_of_sample,\n",
        "    }\n",
        "    \n",
        "    for dataset in ['mouse', 'in_sample', 'out_of_sample']:\n",
        "        all_info = dataset_to_all_info[dataset]\n",
        "        ct_corrs_info = {}\n",
        "        for ct, more_stuff in all_info.items():\n",
        "            this_res = {k: v[-1] for k, v in more_stuff.items()}\n",
        "            ct_corrs_info[ct] = this_res\n",
        "        dataset_to_ct_corrs_info[dataset] = ct_corrs_info\n",
        "    \n",
        "    return dataset_to_ct_corrs_info\n",
        "\n",
        "# Generate cell type correlation plots\n",
        "def generate_cell_type_correlation_plots(dataset_to_ct_corrs_info):\n",
        "    datasets = ['mouse', 'in_sample', 'out_of_sample']\n",
        "    fig, axes = plt.subplots(1, len(datasets), figsize=(15, 5))\n",
        "    \n",
        "    for i, dataset in enumerate(datasets):\n",
        "        ct_corrs_info = dataset_to_ct_corrs_info[dataset]\n",
        "        \n",
        "        # Calculate mean correlation for each cell type\n",
        "        ct_means = []\n",
        "        for ct in sorted(ct_corrs_info.keys()):\n",
        "            corrs = list(ct_corrs_info[ct].values())\n",
        "            ct_means.append(np.mean(corrs))\n",
        "        \n",
        "        # Plot boxplot\n",
        "        axes[i].boxplot([list(ct_corrs_info[ct].values()) for ct in sorted(ct_corrs_info.keys())],\n",
        "                       labels=[f'CT{ct}' for ct in sorted(ct_corrs_info.keys())])\n",
        "        axes[i].set_title(f'{dataset.replace(\"_\", \" \").title()} Correlations')\n",
        "        axes[i].set_ylabel('Correlation')\n",
        "        axes[i].set_xlabel('Cell Type')\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/cell_type_correlations.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define cell type names\n",
        "datasets_to_ct_name = {\n",
        "    'in_sample': {\n",
        "        1: 'Invasive Tumor',\n",
        "        2: 'Ductal Carcinoma In Situ',\n",
        "        3: 'Stromal Cells',\n",
        "        4: 'T Cells',\n",
        "        5: 'Macrophages',\n",
        "        6: 'Endothelial Cells',\n",
        "        7: 'Myoepithelial Cells',\n",
        "        8: 'NK Cells',\n",
        "        9: 'B Cells',\n",
        "        10: 'Mature B Cells',\n",
        "    },\n",
        "    'out_of_sample': {\n",
        "        1: 'T Cells',\n",
        "        2: 'Stromal Cells',\n",
        "        3: 'Macrophages',\n",
        "        4: 'Invasive Tumor',\n",
        "        5: 'T CD8 Cells',\n",
        "        6: 'Endothelial Cells',\n",
        "        7: 'Myoepithelial Cells',\n",
        "        8: 'Epithelial Cells',\n",
        "        9: 'Ductal Carcinoma In Situ',\n",
        "        10: 'B Cells',\n",
        "    },\n",
        "    'mouse': {\n",
        "        1: 'Bone/Cartilage',\n",
        "        2: 'Skeletal Muscle',\n",
        "        3: 'Blood Vessel',\n",
        "        4: 'Brain',\n",
        "        5: 'Lung',\n",
        "        6: 'Liver',\n",
        "        7: 'Skin',\n",
        "        8: 'GI Track',\n",
        "        9: 'Lymphatic Vessels',\n",
        "        10: 'Kidney',\n",
        "    },\n",
        "}\n",
        "\n",
        "ds_to_name = {\n",
        "    'in_sample': 'In-Sample Xenium MBC',\n",
        "    'out_of_sample': 'New-Sample Xenium MBC',\n",
        "    'mouse': 'In-Sample Mouse',\n",
        "}\n",
        "\n",
        "# Generate cell type heterogeneity plots\n",
        "def generate_cell_type_heterogeneity_plots(dataset_to_ct_corrs_info):\n",
        "    num_celltypes = 10\n",
        "    fig, axs = plt.subplots(num_celltypes, len(dataset_to_ct_corrs_info), figsize=(18, 24))\n",
        "    \n",
        "    for col, (dataset_name, celltypes) in enumerate(dataset_to_ct_corrs_info.items()):\n",
        "        ct_to_name = datasets_to_ct_name[dataset_name]\n",
        "        for row, (celltype, values) in enumerate(celltypes.items()):\n",
        "            values = list(values.values())\n",
        "            axs[row][col].hist(values, rwidth=.7, bins=np.arange(-.2, 1.01, .1))\n",
        "            mean_value = np.mean(values)\n",
        "            \n",
        "            # Set title\n",
        "            if not row:\n",
        "                axs[row][col].set_title(f'{ds_to_name[dataset_name]}\\n{ct_to_name[celltype]}', pad=15)\n",
        "            else:\n",
        "                axs[row][col].set_title(f'{ct_to_name[celltype]}', pad=15)\n",
        "            \n",
        "            axs[row][col].set_xlabel('Spatial Correlation', size='x-large')\n",
        "            axs[row][col].set_ylabel('Number of Genes', size='x-large')\n",
        "            plt.setp(axs[row][col].spines.values(), linewidth=2)\n",
        "            \n",
        "            # Display mean value\n",
        "            axs[row][col].text(0.95, 0.95, f'Avg. = {mean_value:.2f}',\n",
        "                             transform=axs[row][col].transAxes,\n",
        "                             fontsize=15, ha='right', va='top',\n",
        "                             bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
        "        \n",
        "        # Hide axes for missing cell types\n",
        "        if len(celltypes) < num_celltypes:\n",
        "            axs[-1, col].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/celltype_heterogeneity.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "# Calculate metacorrelations for each cell type\n",
        "def calculate_cell_type_metacorrelations(label_to_ground_truth_in_sample, label_to_pred_in_sample,\n",
        "                                       label_to_ground_truth_mouse, label_to_pred_mouse,\n",
        "                                       label_to_ground_truth_out_of_sample, label_to_pred_out_of_sample,\n",
        "                                       common_in_sample, common_mouse, common_out_of_sample):\n",
        "    # Calculate in-sample metacorrelations\n",
        "    label_to_in_sample_plot_metacorr = {}\n",
        "    for label in label_to_ground_truth_in_sample:\n",
        "        fold_to_metacorr = {}\n",
        "        for z in range(4):\n",
        "            pred_arr = np.array(label_to_pred_in_sample[label][z][::,common_in_sample].X)\n",
        "            true_arr = np.array(label_to_ground_truth_in_sample[label][z][::,common_in_sample].X.squeeze())\n",
        "            pred_heatmap = np.nan_to_num(np.corrcoef(pred_arr, rowvar=0))\n",
        "            true_heatmap = np.nan_to_num(np.corrcoef(true_arr, rowvar=0))\n",
        "            fold_to_metacorr[z] = np.nan_to_num(np.corrcoef(pred_heatmap.reshape(-1), true_heatmap.reshape(-1))[0, 1])\n",
        "        \n",
        "        fold_to_prop = {}\n",
        "        total_cells = float(sum(label_to_ground_truth_in_sample[label][z].shape[0] for z in range(4)))\n",
        "        for z in range(4):\n",
        "            fold_to_prop[z] = label_to_ground_truth_in_sample[label][z].shape[0] / total_cells\n",
        "        avg_in_sample_metacorr = sum(fold_to_metacorr[z]*fold_to_prop[z] for z in range(4))\n",
        "        label_to_in_sample_plot_metacorr[label] = avg_in_sample_metacorr\n",
        "    \n",
        "    # Calculate mouse metacorrelations\n",
        "    label_to_mouse_plot_metacorr = {}\n",
        "    for label in label_to_ground_truth_mouse:\n",
        "        fold_to_metacorr = {}\n",
        "        for z in range(4):\n",
        "            pred_arr = np.array(label_to_pred_mouse[label][z][::,common_mouse].X)\n",
        "            true_arr = np.array(label_to_ground_truth_mouse[label][z][::,common_mouse].X.squeeze())\n",
        "            pred_heatmap = np.nan_to_num(np.corrcoef(pred_arr, rowvar=0))\n",
        "            true_heatmap = np.nan_to_num(np.corrcoef(true_arr, rowvar=0))\n",
        "            fold_to_metacorr[z] = np.nan_to_num(np.corrcoef(pred_heatmap.reshape(-1), true_heatmap.reshape(-1))[0, 1])\n",
        "        \n",
        "        fold_to_prop = {}\n",
        "        total_cells = float(sum(label_to_ground_truth_mouse[label][z].shape[0] for z in range(4)))\n",
        "        for z in range(4):\n",
        "            fold_to_prop[z] = label_to_ground_truth_mouse[label][z].shape[0] / total_cells\n",
        "        avg_mouse_metacorr = sum(fold_to_metacorr[z]*fold_to_prop[z] for z in range(4))\n",
        "        label_to_mouse_plot_metacorr[label] = avg_mouse_metacorr\n",
        "    \n",
        "    # Calculate out-of-sample metacorrelations\n",
        "    label_to_out_of_sample_plot_metacorr = {}\n",
        "    for label in label_to_ground_truth_out_of_sample:\n",
        "        pred_arr = np.array(label_to_pred_out_of_sample[label][::,common_out_of_sample].X)\n",
        "        true_arr = np.array(label_to_ground_truth_out_of_sample[label][::, common_out_of_sample].X)\n",
        "        pred_heatmap_out_of_sample = np.nan_to_num(np.corrcoef(pred_arr, rowvar=0))\n",
        "        true_heatmap_out_of_sample = np.nan_to_num(np.corrcoef(true_arr, rowvar=0))\n",
        "        out_of_sample_metacorr = np.nan_to_num(np.corrcoef(pred_heatmap_out_of_sample.reshape(-1), true_heatmap_out_of_sample.reshape(-1))[0, 1])\n",
        "        label_to_out_of_sample_plot_metacorr[label] = out_of_sample_metacorr\n",
        "    \n",
        "    return label_to_in_sample_plot_metacorr, label_to_mouse_plot_metacorr, label_to_out_of_sample_plot_metacorr\n",
        "\n",
        "# Generate metacorrelation plots\n",
        "def generate_metacorrelation_plots(label_to_in_sample_plot_metacorr, label_to_mouse_plot_metacorr, label_to_out_of_sample_plot_metacorr):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    # Plot in-sample metacorrelations\n",
        "    axes[0].bar(range(len(label_to_in_sample_plot_metacorr)), \n",
        "                [label_to_in_sample_plot_metacorr[i] for i in sorted(label_to_in_sample_plot_metacorr.keys())])\n",
        "    axes[0].set_title('In-Sample Metacorrelations')\n",
        "    axes[0].set_xticks(range(len(label_to_in_sample_plot_metacorr)))\n",
        "    axes[0].set_xticklabels([datasets_to_ct_name['in_sample'][i] for i in sorted(label_to_in_sample_plot_metacorr.keys())],\n",
        "                           rotation=45, ha='right')\n",
        "    axes[0].set_ylabel('Metacorrelation')\n",
        "    \n",
        "    # Plot mouse metacorrelations\n",
        "    axes[1].bar(range(len(label_to_mouse_plot_metacorr)),\n",
        "                [label_to_mouse_plot_metacorr[i] for i in sorted(label_to_mouse_plot_metacorr.keys())])\n",
        "    axes[1].set_title('Mouse Metacorrelations')\n",
        "    axes[1].set_xticks(range(len(label_to_mouse_plot_metacorr)))\n",
        "    axes[1].set_xticklabels([datasets_to_ct_name['mouse'][i] for i in sorted(label_to_mouse_plot_metacorr.keys())],\n",
        "                           rotation=45, ha='right')\n",
        "    axes[1].set_ylabel('Metacorrelation')\n",
        "    \n",
        "    # Plot out-of-sample metacorrelations\n",
        "    axes[2].bar(range(len(label_to_out_of_sample_plot_metacorr)),\n",
        "                [label_to_out_of_sample_plot_metacorr[i] for i in sorted(label_to_out_of_sample_plot_metacorr.keys())])\n",
        "    axes[2].set_title('Out-of-Sample Metacorrelations')\n",
        "    axes[2].set_xticks(range(len(label_to_out_of_sample_plot_metacorr)))\n",
        "    axes[2].set_xticklabels([datasets_to_ct_name['out_of_sample'][i] for i in sorted(label_to_out_of_sample_plot_metacorr.keys())],\n",
        "                           rotation=45, ha='right')\n",
        "    axes[2].set_ylabel('Metacorrelation')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_figures_schaf_revision_pngs/metacorrelations.png', dpi=400, bbox_inches='tight')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate cell type heterogeneity\n",
        "def calculate_cell_type_heterogeneity(label_to_ground_truth_in_sample, label_to_pred_in_sample,\n",
        "                                    label_to_ground_truth_mouse, label_to_pred_mouse,\n",
        "                                    label_to_ground_truth_out_of_sample, label_to_pred_out_of_sample,\n",
        "                                    common_in_sample, common_mouse, common_out_of_sample):\n",
        "    def get_hetero(cells, genes):\n",
        "        res = {}\n",
        "        for g in genes:\n",
        "            try:\n",
        "                to_see = np.array(cells[::,g].X).squeeze()\n",
        "                to_see = (to_see - to_see.max()) / (to_see.max() - to_see.min())\n",
        "                res[g] = np.nan_to_num(to_see.std())\n",
        "            except:\n",
        "                res[g] = 0\n",
        "        return res\n",
        "    \n",
        "    # Calculate in-sample heterogeneity\n",
        "    label_to_fold_to_in_sample_heteros = {}\n",
        "    label_to_avg_in_sample_heteros = {}\n",
        "    \n",
        "    for label in label_to_ground_truth_in_sample:\n",
        "        label_to_fold_to_in_sample_heteros[label] = {}\n",
        "        label_to_avg_in_sample_heteros[label] = {}\n",
        "        \n",
        "        for z in range(4):\n",
        "            label_to_fold_to_in_sample_heteros[label][z] = (\n",
        "                get_hetero(label_to_pred_in_sample[label][z], common_in_sample),\n",
        "                get_hetero(label_to_ground_truth_in_sample[label][z], common_in_sample)\n",
        "            )\n",
        "        \n",
        "        fold_to_prop = {}\n",
        "        total_cells = float(sum(label_to_ground_truth_in_sample[label][z].shape[0] for z in range(4)))\n",
        "        for z in range(4):\n",
        "            fold_to_prop[z] = label_to_ground_truth_in_sample[label][z].shape[0] / total_cells\n",
        "        \n",
        "        for g in common_in_sample:\n",
        "            label_to_avg_in_sample_heteros[label][g] = (\n",
        "                sum(label_to_fold_to_in_sample_heteros[label][z][0][g] * fold_to_prop[z] for z in range(4)),\n",
        "                sum(label_to_fold_to_in_sample_heteros[label][z][1][g] * fold_to_prop[z] for z in range(4))\n",
        "            )\n",
        "    \n",
        "    # Calculate mouse heterogeneity\n",
        "    label_to_fold_to_mouse_heteros = {}\n",
        "    label_to_avg_mouse_heteros = {}\n",
        "    \n",
        "    for label in label_to_ground_truth_mouse:\n",
        "        label_to_fold_to_mouse_heteros[label] = {}\n",
        "        label_to_avg_mouse_heteros[label] = {}\n",
        "        \n",
        "        for z in range(4):\n",
        "            label_to_fold_to_mouse_heteros[label][z] = (\n",
        "                get_hetero(label_to_pred_mouse[label][z], common_mouse),\n",
        "                get_hetero(label_to_ground_truth_mouse[label][z], common_mouse)\n",
        "            )\n",
        "        \n",
        "        fold_to_prop = {}\n",
        "        total_cells = float(sum(label_to_ground_truth_mouse[label][z].shape[0] for z in range(4)))\n",
        "        for z in range(4):\n",
        "            fold_to_prop[z] = label_to_ground_truth_mouse[label][z].shape[0] / total_cells\n",
        "        \n",
        "        for g in common_mouse:\n",
        "            label_to_avg_mouse_heteros[label][g] = (\n",
        "                sum(label_to_fold_to_mouse_heteros[label][z][0][g] * fold_to_prop[z] for z in range(4)),\n",
        "                sum(label_to_fold_to_mouse_heteros[label][z][1][g] * fold_to_prop[z] for z in range(4))\n",
        "            )\n",
        "    \n",
        "    # Calculate out-of-sample heterogeneity\n",
        "    label_to_heteros_out_of_sample = {}\n",
        "    for label in label_to_ground_truth_out_of_sample:\n",
        "        label_to_heteros_out_of_sample[label] = (\n",
        "            get_hetero(label_to_pred_out_of_sample[label], common_out_of_sample),\n",
        "            get_hetero(label_to_ground_truth_out_of_sample[label], common_out_of_sample)\n",
        "        )\n",
        "    \n",
        "    return label_to_avg_in_sample_heteros, label_to_avg_mouse_heteros, label_to_heteros_out_of_sample\n",
        "\n",
        "# Generate histology split figures\n",
        "def generate_histology_split_figures(in_sample_fold_to_hist, mouse_fold_to_hist, in_sample_hist, out_of_sample):\n",
        "    # In-sample histology split\n",
        "    fig_name = 'in_sample_hist_split'\n",
        "    f, axs = plt.subplots(2, 2, figsize=(20, 14))\n",
        "    \n",
        "    fold_to_row_col = {\n",
        "        0: (1, 0),\n",
        "        1: (0, 1),\n",
        "        2: (0, 0),\n",
        "        3: (1, 1),\n",
        "    }\n",
        "    \n",
        "    f.suptitle(\"In-Sample Xenium MBC\", fontsize=\"xx-large\")\n",
        "    for fold in range(4):\n",
        "        row, col = fold_to_row_col[fold]\n",
        "        axs[row, col].axis('off')\n",
        "        if row == 0 and col == 1:\n",
        "            to_show = in_sample_fold_to_hist[fold][::,::,0]\n",
        "            axs[row, col].imshow(to_show.max() - to_show, cmap=mpl.cm.Blues, aspect='auto')\n",
        "            axs[row, col].set_title(f'Evaluation', size='xx-large', c='blue')\n",
        "        else:\n",
        "            to_show = in_sample_fold_to_hist[fold][::,::,0]\n",
        "            axs[row, col].imshow(to_show.max() - to_show, cmap=mpl.cm.Reds, aspect='auto')\n",
        "            axs[row, col].set_title(f'Training', size='xx-large', c='red')\n",
        "        axs[row, col].invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{fig_name}.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "    \n",
        "    # Mouse histology split\n",
        "    fig_name = 'mouse_hist_split'\n",
        "    f, axs = plt.subplots(2, 2, figsize=(20, 12.5))\n",
        "    \n",
        "    fold_to_row_col = {\n",
        "        0: (0, 0),\n",
        "        1: (1, 1),\n",
        "        2: (1, 0),\n",
        "        3: (0, 1),\n",
        "    }\n",
        "    \n",
        "    f.suptitle(\"In-Sample Mouse\", fontsize=\"xx-large\")\n",
        "    for fold in range(4):\n",
        "        row, col = fold_to_row_col[fold]\n",
        "        axs[row, col].axis('off')\n",
        "        if row == 0 and col == 1:\n",
        "            to_show = mouse_fold_to_hist[fold][::,::,0]\n",
        "            axs[row, col].imshow(to_show.max() - to_show, cmap=mpl.cm.Blues, aspect='auto')\n",
        "            axs[row, col].set_title(f'Evaluation', size='xx-large', c='blue')\n",
        "        else:\n",
        "            to_show = mouse_fold_to_hist[fold][::,::,0]\n",
        "            axs[row, col].imshow(to_show.max() - to_show, cmap=mpl.cm.Reds, aspect='auto')\n",
        "            axs[row, col].set_title(f'Training', size='xx-large', c='red')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{fig_name}.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "    \n",
        "    # New sample histology split\n",
        "    fig_name = 'new_sample_hist_split_train'\n",
        "    f, axs = plt.subplots(1, 1, figsize=(20, 14))\n",
        "    f.suptitle(\"New-Sample Xenium MBC\", fontsize=\"xx-large\")\n",
        "    axs.axis('off')\n",
        "    \n",
        "    to_show = in_sample_hist[::-1][::,::,0]\n",
        "    axs.imshow(to_show.max() - to_show, cmap=mpl.cm.Reds)\n",
        "    axs.set_title(f'Training', size='xx-large', c='red')\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{fig_name}.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "    \n",
        "    fig_name = 'new_sample_hist_split_eval'\n",
        "    f, axs = plt.subplots(1, 1, figsize=(13, 20))\n",
        "    axs.axis('off')\n",
        "    \n",
        "    to_show = out_of_sample[::-1][::,::,0]\n",
        "    axs.imshow(to_show.max() - to_show, cmap=mpl.cm.Blues)\n",
        "    axs.set_title(f'Evaluation', size='xx-large', c='blue')\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{fig_name}.png', dpi=400, transparent=True)\n",
        "    plt.close()\n",
        "\n",
        "# Generate correlation histograms\n",
        "def generate_correlation_histograms(mouse_avg_in_sample_corrs, mouse_train_genes):\n",
        "    fig_name = 'all_gene_corrs'\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(13.5, 5.4))\n",
        "    \n",
        "    # Mouse correlations\n",
        "    ax[0].hist(\n",
        "        [\n",
        "            [mouse_avg_in_sample_corrs[g] for g in mouse_avg_in_sample_corrs if g in mouse_train_genes],\n",
        "            [mouse_avg_in_sample_corrs[g] for g in mouse_avg_in_sample_corrs if g not in mouse_train_genes],\n",
        "        ],\n",
        "        color=['orange', 'blue'],\n",
        "        label=[f'Training Genes Avg. = {.445}', f'Hold-Out Genes Avg. = {.377}'],\n",
        "        alpha=.5, rwidth=.7, bins=np.arange(0, 1.01, .1)\n",
        "    )\n",
        "    ax[0].set_title('In-Sample Mouse', loc='left')\n",
        "    ax[0].set_ylabel('Number of Genes')\n",
        "    ax[0].set_xlabel('Spatial Correlation Coefficient')\n",
        "    ax[0].set_xticks(ticks=np.arange(0, 1.01, .2), labels=np.round(np.arange(0, 1.01, .2), 2))\n",
        "    ax[0].set_yticks(ticks=np.arange(0, 51, 10), labels=np.arange(0, 51, 10))\n",
        "    ax[0].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'final_figures_schaf_revision_pngs/{fig_name}.png', dpi=400, transparent=True)\n",
        "    plt.close()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
